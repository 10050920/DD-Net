{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 21 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joint\n",
    "        self.joint_ind = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "        self.key_ind = np.array([1,3,7,11,14,18])\n",
    "        self.feat_d = 252\n",
    "        self.filters = 64\n",
    "        self.clc_num=49\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def get_CG_double(p_0,p_1,C):\n",
    "    M_0 = []\n",
    "    M_1 = []\n",
    "\n",
    "    for f in range(C.frame_l):\n",
    "        #correlation graph \n",
    "        d_m_0 = cdist(p_0[f][C.key_ind,:],p_0[f],'euclidean').flatten()\n",
    "        d_m_1 = cdist(p_1[f][C.key_ind,:],p_1[f],'euclidean').flatten()\n",
    "        d_m_01 = cdist(p_0[f][C.key_ind,:],p_1[f],'euclidean').flatten()\n",
    "        d_m_10 = cdist(p_1[f][C.key_ind,:],p_0[f],'euclidean').flatten() \n",
    "        \n",
    "        M_0.append(np.concatenate([d_m_0,d_m_01]))\n",
    "        M_1.append(np.concatenate([d_m_1,d_m_10]))\n",
    " \n",
    "    M_0 = np.stack(M_0)\n",
    "    M_1 = np.stack(M_1)   \n",
    "    return M_0,M_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,:1,...],x[:,:-1,...])\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    #P_diff_slow = Lambda(lambda x: tf.norm(x,axis=-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    #P_diff_fast = Lambda(lambda x: tf.norm(x,axis=-1))(P_diff_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "\n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=21,joint_d=3,feat_d=210,filters=128):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "\n",
    "    x = c1D(M,filters*2,1)\n",
    "    x = SpatialDropout1D(0.05)(x)\n",
    "    x = c1D(x,filters,3)\n",
    "    x = block(x,filters)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = SpatialDropout1D(0.05)(x)\n",
    "\n",
    "    x_d_slow = c1D(diff_slow,filters*2,1)\n",
    "    x_d_slow = SpatialDropout1D(0.05)(x_d_slow)\n",
    "    x_d_slow = block(x_d_slow,filters)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    x_d_slow = SpatialDropout1D(0.05)(x_d_slow)\n",
    "        \n",
    "    x_d_fast = c1D(diff_fast,filters*2,1)\n",
    "    x_d_fast = SpatialDropout1D(0.05)(x_d_fast)\n",
    "    x_d_fast = block(x_d_fast,filters) \n",
    "    x_d_fast = SpatialDropout1D(0.05)(x_d_fast)\n",
    "    \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "    return Model(inputs=[M,P],outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_AR_double(C):\n",
    "    M_0 = Input(name='M_0', shape=(C.frame_l,C.feat_d))  \n",
    "    M_1 = Input(name='M_1', shape=(C.frame_l,C.feat_d))\n",
    "    P_0 = Input(name='P_0', shape=(C.frame_l,C.joint_n,C.joint_d)) \n",
    "    P_1 = Input(name='P_1', shape=(C.frame_l,C.joint_n,C.joint_d)) \n",
    "    \n",
    "    FM = build_FM(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.filters)\n",
    "    \n",
    "    x_0 = FM([M_0,P_0])\n",
    "    x_1 = FM([M_1,P_1])\n",
    "    \n",
    "    x = maximum([x_0,x_1])\n",
    "    \n",
    "    x = GlobalMaxPool1D()(x)\n",
    "\n",
    "    x = d1D(x,256)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = d1D(x,256)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(C.clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M_0,M_1,P_0,P_1],outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_double = build_AR_double(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M_0 (InputLayer)                (None, 32, 252)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P_0 (InputLayer)                (None, 32, 21, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "M_1 (InputLayer)                (None, 32, 252)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P_1 (InputLayer)                (None, 32, 21, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 512)       1779200     M_0[0][0]                        \n",
      "                                                                 P_0[0][0]                        \n",
      "                                                                 M_1[0][0]                        \n",
      "                                                                 P_1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "maximum_1 (Maximum)             (None, 4, 512)       0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           maximum_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131072      global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 256)          0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65536       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 256)          0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           2827        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,980,683\n",
      "Trainable params: 1,974,411\n",
      "Non-trainable params: 6,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AR_double.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_double.load_weights('weights/weight_xv_1D_double_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/mnt/nasbi/homes/fan/projects/action/skeleton/data/NTU/'\n",
    "xlist = ['xview_train.pkl','xview_val.pkl','xsub_train.pkl','xsub_val.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross view train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xv_train = pd.read_pickle(data_path+xlist[0],compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 67/37646 [00:00<00:57, 653.92it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 37646/37646 [01:03<00:00, 589.13it/s]\n"
     ]
    }
   ],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "X_2 = []\n",
    "X_3 = []\n",
    "Y = []\n",
    "num_xv_train = len(xv_train['label'])\n",
    "for i in tqdm(range(num_xv_train)): \n",
    "    p_0 = xv_train['poses'][i][0][:,C.joint_ind,:]\n",
    "    p_1 = xv_train['poses'][i][1][:,C.joint_ind,:]\n",
    "\n",
    "    if np.all(p_0==0) or np.all(p_1==0): \n",
    "        continue\n",
    "           \n",
    "    p_0 = zoom(p_0,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p_1 = zoom(p_1,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    \n",
    "    try:\n",
    "        label = np.zeros(11)\n",
    "        label[xv_train['label'][i]-50] = 1   \n",
    "    except:\n",
    "        continue\n",
    "        #print(xv_train['name'][i])\n",
    "\n",
    "    M_0,M_1 = get_CG_double(p_0,p_1,C)\n",
    "\n",
    "    X_0.append(M_0)\n",
    "    X_1.append(M_1)\n",
    "    X_2.append(p_0)\n",
    "    X_3.append(p_1)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  \n",
    "X_1 = np.stack(X_1) \n",
    "X_2 = np.stack(X_2)  \n",
    "X_3 = np.stack(X_3) \n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#note: the acc may suddently jump from 20% to 90%, but it may takes hundreds of epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6970 samples, validate on 3475 samples\n",
      "Stochastic weight averaging selected for last 20 epochs.\n",
      "Epoch 1/100\n",
      "6970/6970 [==============================] - 15s 2ms/step - loss: 0.0865 - acc: 0.9735 - val_loss: 0.1957 - val_acc: 0.9378\n",
      "Epoch 2/100\n",
      "6970/6970 [==============================] - 2s 256us/step - loss: 0.0879 - acc: 0.9747 - val_loss: 0.1957 - val_acc: 0.9387\n",
      "Epoch 3/100\n",
      "6970/6970 [==============================] - 2s 268us/step - loss: 0.0852 - acc: 0.9727 - val_loss: 0.1938 - val_acc: 0.9399\n",
      "Epoch 4/100\n",
      "6970/6970 [==============================] - 2s 252us/step - loss: 0.0854 - acc: 0.9733 - val_loss: 0.1982 - val_acc: 0.9387\n",
      "Epoch 5/100\n",
      "6970/6970 [==============================] - 2s 247us/step - loss: 0.0793 - acc: 0.9772 - val_loss: 0.1882 - val_acc: 0.9424\n",
      "Epoch 6/100\n",
      "6970/6970 [==============================] - 2s 264us/step - loss: 0.0862 - acc: 0.9726 - val_loss: 0.1929 - val_acc: 0.9399\n",
      "Epoch 7/100\n",
      "6970/6970 [==============================] - 2s 260us/step - loss: 0.0867 - acc: 0.9732 - val_loss: 0.1946 - val_acc: 0.9407\n",
      "Epoch 8/100\n",
      "6970/6970 [==============================] - 2s 259us/step - loss: 0.0832 - acc: 0.9750 - val_loss: 0.1903 - val_acc: 0.9404\n",
      "Epoch 9/100\n",
      "6970/6970 [==============================] - 2s 256us/step - loss: 0.0766 - acc: 0.9785 - val_loss: 0.1978 - val_acc: 0.9390\n",
      "Epoch 10/100\n",
      "6970/6970 [==============================] - 2s 243us/step - loss: 0.0863 - acc: 0.9737 - val_loss: 0.2019 - val_acc: 0.9364\n",
      "Epoch 11/100\n",
      "6970/6970 [==============================] - 2s 242us/step - loss: 0.0839 - acc: 0.9732 - val_loss: 0.1875 - val_acc: 0.9404\n",
      "Epoch 12/100\n",
      "6970/6970 [==============================] - 2s 256us/step - loss: 0.0827 - acc: 0.9752 - val_loss: 0.1891 - val_acc: 0.9399\n",
      "Epoch 13/100\n",
      "6970/6970 [==============================] - 2s 244us/step - loss: 0.0843 - acc: 0.9753 - val_loss: 0.2072 - val_acc: 0.9387\n",
      "Epoch 14/100\n",
      "6970/6970 [==============================] - 2s 236us/step - loss: 0.0805 - acc: 0.9762 - val_loss: 0.1924 - val_acc: 0.9401\n",
      "Epoch 15/100\n",
      "6970/6970 [==============================] - 2s 265us/step - loss: 0.0800 - acc: 0.9753 - val_loss: 0.1879 - val_acc: 0.9413\n",
      "Epoch 16/100\n",
      "6970/6970 [==============================] - 2s 251us/step - loss: 0.0789 - acc: 0.9796 - val_loss: 0.2046 - val_acc: 0.9373\n",
      "Epoch 17/100\n",
      "6970/6970 [==============================] - 2s 253us/step - loss: 0.0855 - acc: 0.9752 - val_loss: 0.1993 - val_acc: 0.9384\n",
      "Epoch 18/100\n",
      "6970/6970 [==============================] - 2s 275us/step - loss: 0.0853 - acc: 0.9732 - val_loss: 0.1955 - val_acc: 0.9399\n",
      "Epoch 19/100\n",
      "6970/6970 [==============================] - 2s 259us/step - loss: 0.0865 - acc: 0.9742 - val_loss: 0.1979 - val_acc: 0.9396\n",
      "Epoch 20/100\n",
      "6970/6970 [==============================] - 2s 249us/step - loss: 0.0891 - acc: 0.9717 - val_loss: 0.1912 - val_acc: 0.9407\n",
      "Epoch 21/100\n",
      "6970/6970 [==============================] - 2s 271us/step - loss: 0.0835 - acc: 0.9745 - val_loss: 0.1900 - val_acc: 0.9399\n",
      "Epoch 22/100\n",
      "6970/6970 [==============================] - 2s 262us/step - loss: 0.0888 - acc: 0.9733 - val_loss: 0.1950 - val_acc: 0.9396\n",
      "Epoch 23/100\n",
      "6970/6970 [==============================] - 2s 276us/step - loss: 0.0811 - acc: 0.9769 - val_loss: 0.1980 - val_acc: 0.9404\n",
      "Epoch 24/100\n",
      "6970/6970 [==============================] - 2s 267us/step - loss: 0.0812 - acc: 0.9747 - val_loss: 0.1987 - val_acc: 0.9378\n",
      "Epoch 25/100\n",
      "6970/6970 [==============================] - 2s 276us/step - loss: 0.0793 - acc: 0.9779 - val_loss: 0.1949 - val_acc: 0.9401\n",
      "Epoch 26/100\n",
      "6970/6970 [==============================] - 2s 272us/step - loss: 0.0793 - acc: 0.9763 - val_loss: 0.1984 - val_acc: 0.9396\n",
      "Epoch 27/100\n",
      "6970/6970 [==============================] - 2s 264us/step - loss: 0.0830 - acc: 0.9773 - val_loss: 0.1986 - val_acc: 0.9399\n",
      "Epoch 28/100\n",
      "6970/6970 [==============================] - 2s 251us/step - loss: 0.0816 - acc: 0.9759 - val_loss: 0.2065 - val_acc: 0.9367\n",
      "Epoch 29/100\n",
      "6970/6970 [==============================] - 2s 276us/step - loss: 0.0770 - acc: 0.9769 - val_loss: 0.1981 - val_acc: 0.9390\n",
      "Epoch 30/100\n",
      "6970/6970 [==============================] - 2s 253us/step - loss: 0.0817 - acc: 0.9762 - val_loss: 0.1975 - val_acc: 0.9401\n",
      "Epoch 31/100\n",
      "6970/6970 [==============================] - 2s 267us/step - loss: 0.0771 - acc: 0.9782 - val_loss: 0.1888 - val_acc: 0.9422\n",
      "Epoch 32/100\n",
      "6970/6970 [==============================] - 2s 262us/step - loss: 0.0818 - acc: 0.9768 - val_loss: 0.1891 - val_acc: 0.9419\n",
      "Epoch 33/100\n",
      "6970/6970 [==============================] - 2s 262us/step - loss: 0.0821 - acc: 0.9765 - val_loss: 0.1941 - val_acc: 0.9416\n",
      "Epoch 34/100\n",
      "6970/6970 [==============================] - 2s 278us/step - loss: 0.0853 - acc: 0.9737 - val_loss: 0.1900 - val_acc: 0.9404\n",
      "Epoch 35/100\n",
      "6970/6970 [==============================] - 2s 279us/step - loss: 0.0855 - acc: 0.9737 - val_loss: 0.1975 - val_acc: 0.9396\n",
      "Epoch 36/100\n",
      "6970/6970 [==============================] - 2s 268us/step - loss: 0.0859 - acc: 0.9723 - val_loss: 0.1962 - val_acc: 0.9393\n",
      "Epoch 37/100\n",
      "6970/6970 [==============================] - 2s 266us/step - loss: 0.0783 - acc: 0.9756 - val_loss: 0.1975 - val_acc: 0.9401\n",
      "Epoch 38/100\n",
      "6970/6970 [==============================] - 2s 266us/step - loss: 0.0841 - acc: 0.9745 - val_loss: 0.2039 - val_acc: 0.9381\n",
      "Epoch 39/100\n",
      "6970/6970 [==============================] - 2s 279us/step - loss: 0.0828 - acc: 0.9745 - val_loss: 0.1993 - val_acc: 0.9390\n",
      "Epoch 40/100\n",
      "6970/6970 [==============================] - 2s 282us/step - loss: 0.0785 - acc: 0.9765 - val_loss: 0.1983 - val_acc: 0.9381\n",
      "Epoch 41/100\n",
      "6970/6970 [==============================] - 2s 250us/step - loss: 0.0850 - acc: 0.9727 - val_loss: 0.1908 - val_acc: 0.9410\n",
      "Epoch 42/100\n",
      "6970/6970 [==============================] - 2s 247us/step - loss: 0.0799 - acc: 0.9760 - val_loss: 0.1901 - val_acc: 0.9404\n",
      "Epoch 43/100\n",
      "6970/6970 [==============================] - 2s 259us/step - loss: 0.0872 - acc: 0.9740 - val_loss: 0.2088 - val_acc: 0.9367\n",
      "Epoch 44/100\n",
      "6970/6970 [==============================] - 2s 255us/step - loss: 0.0829 - acc: 0.9740 - val_loss: 0.1972 - val_acc: 0.9370\n",
      "Epoch 45/100\n",
      "6970/6970 [==============================] - 2s 246us/step - loss: 0.0800 - acc: 0.9756 - val_loss: 0.1979 - val_acc: 0.9387\n",
      "Epoch 46/100\n",
      "6970/6970 [==============================] - 2s 273us/step - loss: 0.0795 - acc: 0.9762 - val_loss: 0.2011 - val_acc: 0.9381\n",
      "Epoch 47/100\n",
      "6970/6970 [==============================] - 2s 260us/step - loss: 0.0810 - acc: 0.9749 - val_loss: 0.2088 - val_acc: 0.9381\n",
      "Epoch 48/100\n",
      "6970/6970 [==============================] - 2s 248us/step - loss: 0.0784 - acc: 0.9759 - val_loss: 0.1957 - val_acc: 0.9393\n",
      "Epoch 49/100\n",
      "6970/6970 [==============================] - 2s 249us/step - loss: 0.0767 - acc: 0.9758 - val_loss: 0.1963 - val_acc: 0.9381\n",
      "Epoch 50/100\n",
      "6970/6970 [==============================] - 2s 270us/step - loss: 0.0815 - acc: 0.9742 - val_loss: 0.2053 - val_acc: 0.9364\n",
      "Epoch 51/100\n",
      "6970/6970 [==============================] - 2s 270us/step - loss: 0.0797 - acc: 0.9760 - val_loss: 0.2032 - val_acc: 0.9387\n",
      "Epoch 52/100\n",
      "6970/6970 [==============================] - 2s 249us/step - loss: 0.0805 - acc: 0.9766 - val_loss: 0.1976 - val_acc: 0.9396\n",
      "Epoch 53/100\n",
      "6970/6970 [==============================] - 2s 259us/step - loss: 0.0799 - acc: 0.9756 - val_loss: 0.1978 - val_acc: 0.9387\n",
      "Epoch 54/100\n",
      "6970/6970 [==============================] - 2s 258us/step - loss: 0.0839 - acc: 0.9742 - val_loss: 0.1910 - val_acc: 0.9404\n",
      "Epoch 55/100\n",
      "6970/6970 [==============================] - 2s 279us/step - loss: 0.0823 - acc: 0.9755 - val_loss: 0.2001 - val_acc: 0.9381\n",
      "Epoch 56/100\n",
      "6970/6970 [==============================] - 2s 276us/step - loss: 0.0799 - acc: 0.9760 - val_loss: 0.1959 - val_acc: 0.9401\n",
      "Epoch 57/100\n",
      "6970/6970 [==============================] - 2s 280us/step - loss: 0.0831 - acc: 0.9756 - val_loss: 0.2069 - val_acc: 0.9358\n",
      "Epoch 58/100\n",
      "6970/6970 [==============================] - 2s 265us/step - loss: 0.0757 - acc: 0.9773 - val_loss: 0.1955 - val_acc: 0.9393\n",
      "Epoch 59/100\n",
      "6970/6970 [==============================] - 2s 274us/step - loss: 0.0850 - acc: 0.9737 - val_loss: 0.1919 - val_acc: 0.9390\n",
      "Epoch 60/100\n",
      "6970/6970 [==============================] - 2s 279us/step - loss: 0.0818 - acc: 0.9756 - val_loss: 0.1941 - val_acc: 0.9401\n",
      "Epoch 61/100\n",
      "6970/6970 [==============================] - 2s 280us/step - loss: 0.0832 - acc: 0.9753 - val_loss: 0.1986 - val_acc: 0.9387\n",
      "Epoch 62/100\n",
      "6970/6970 [==============================] - 2s 271us/step - loss: 0.0824 - acc: 0.9747 - val_loss: 0.1998 - val_acc: 0.9384\n",
      "Epoch 63/100\n",
      "6970/6970 [==============================] - 2s 249us/step - loss: 0.0778 - acc: 0.9768 - val_loss: 0.2007 - val_acc: 0.9387\n",
      "Epoch 64/100\n",
      "6970/6970 [==============================] - 2s 254us/step - loss: 0.0743 - acc: 0.9779 - val_loss: 0.2089 - val_acc: 0.9364\n",
      "Epoch 65/100\n",
      "6970/6970 [==============================] - 2s 261us/step - loss: 0.0867 - acc: 0.9743 - val_loss: 0.1991 - val_acc: 0.9396\n",
      "Epoch 66/100\n",
      "6970/6970 [==============================] - 2s 252us/step - loss: 0.0780 - acc: 0.9765 - val_loss: 0.2018 - val_acc: 0.9393\n",
      "Epoch 67/100\n",
      "6970/6970 [==============================] - 2s 265us/step - loss: 0.0798 - acc: 0.9780 - val_loss: 0.1924 - val_acc: 0.9413\n",
      "Epoch 68/100\n",
      "6970/6970 [==============================] - 2s 243us/step - loss: 0.0829 - acc: 0.9745 - val_loss: 0.2019 - val_acc: 0.9404\n",
      "Epoch 69/100\n",
      "6970/6970 [==============================] - 2s 251us/step - loss: 0.0774 - acc: 0.9791 - val_loss: 0.2081 - val_acc: 0.9361\n",
      "Epoch 70/100\n",
      "6970/6970 [==============================] - 2s 275us/step - loss: 0.0789 - acc: 0.9772 - val_loss: 0.1993 - val_acc: 0.9401\n",
      "Epoch 71/100\n",
      "6970/6970 [==============================] - 2s 266us/step - loss: 0.0812 - acc: 0.9753 - val_loss: 0.2085 - val_acc: 0.9396\n",
      "Epoch 72/100\n",
      "6970/6970 [==============================] - 2s 242us/step - loss: 0.0732 - acc: 0.9783 - val_loss: 0.1925 - val_acc: 0.9396\n",
      "Epoch 73/100\n",
      "6970/6970 [==============================] - 2s 264us/step - loss: 0.0781 - acc: 0.9778 - val_loss: 0.1960 - val_acc: 0.9396\n",
      "Epoch 74/100\n",
      "6970/6970 [==============================] - 2s 253us/step - loss: 0.0801 - acc: 0.9756 - val_loss: 0.1998 - val_acc: 0.9387\n",
      "Epoch 75/100\n",
      "6970/6970 [==============================] - 2s 248us/step - loss: 0.0796 - acc: 0.9736 - val_loss: 0.1983 - val_acc: 0.9381\n",
      "Epoch 76/100\n",
      "6970/6970 [==============================] - 2s 261us/step - loss: 0.0813 - acc: 0.9750 - val_loss: 0.1922 - val_acc: 0.9410\n",
      "Epoch 77/100\n",
      "6970/6970 [==============================] - 2s 274us/step - loss: 0.0746 - acc: 0.9788 - val_loss: 0.2000 - val_acc: 0.9367\n",
      "Epoch 78/100\n",
      "6970/6970 [==============================] - 2s 243us/step - loss: 0.0750 - acc: 0.9770 - val_loss: 0.1943 - val_acc: 0.9396\n",
      "Epoch 79/100\n",
      "6970/6970 [==============================] - 2s 276us/step - loss: 0.0767 - acc: 0.9760 - val_loss: 0.2146 - val_acc: 0.9384\n",
      "Epoch 80/100\n",
      "6970/6970 [==============================] - 2s 275us/step - loss: 0.0799 - acc: 0.9769 - val_loss: 0.1951 - val_acc: 0.9404\n",
      "Epoch 81/100\n",
      "6970/6970 [==============================] - 2s 275us/step - loss: 0.0827 - acc: 0.9756 - val_loss: 0.2031 - val_acc: 0.9378\n",
      "Epoch 82/100\n",
      "6970/6970 [==============================] - 2s 263us/step - loss: 0.0781 - acc: 0.9758 - val_loss: 0.2022 - val_acc: 0.9387\n",
      "Epoch 83/100\n",
      "6970/6970 [==============================] - 2s 268us/step - loss: 0.0782 - acc: 0.9753 - val_loss: 0.2131 - val_acc: 0.9358\n",
      "Epoch 84/100\n",
      "6970/6970 [==============================] - 2s 272us/step - loss: 0.0769 - acc: 0.9755 - val_loss: 0.2062 - val_acc: 0.9381\n",
      "Epoch 85/100\n",
      "6970/6970 [==============================] - 2s 272us/step - loss: 0.0830 - acc: 0.9746 - val_loss: 0.1967 - val_acc: 0.9393\n",
      "Epoch 86/100\n",
      "6970/6970 [==============================] - 2s 258us/step - loss: 0.0818 - acc: 0.9739 - val_loss: 0.2073 - val_acc: 0.9378\n",
      "Epoch 87/100\n",
      "6970/6970 [==============================] - 2s 276us/step - loss: 0.0779 - acc: 0.9765 - val_loss: 0.2008 - val_acc: 0.9401\n",
      "Epoch 88/100\n",
      "6970/6970 [==============================] - 2s 267us/step - loss: 0.0791 - acc: 0.9742 - val_loss: 0.2147 - val_acc: 0.9367\n",
      "Epoch 89/100\n",
      "6970/6970 [==============================] - 2s 264us/step - loss: 0.0847 - acc: 0.9730 - val_loss: 0.1968 - val_acc: 0.9401\n",
      "Epoch 90/100\n",
      "6970/6970 [==============================] - 2s 263us/step - loss: 0.0783 - acc: 0.9763 - val_loss: 0.2065 - val_acc: 0.9381\n",
      "Epoch 91/100\n",
      "6970/6970 [==============================] - 2s 274us/step - loss: 0.0848 - acc: 0.9740 - val_loss: 0.1980 - val_acc: 0.9410\n",
      "Epoch 92/100\n",
      "6970/6970 [==============================] - 2s 271us/step - loss: 0.0785 - acc: 0.9768 - val_loss: 0.2008 - val_acc: 0.9387\n",
      "Epoch 93/100\n",
      "6970/6970 [==============================] - 2s 252us/step - loss: 0.0806 - acc: 0.9740 - val_loss: 0.1977 - val_acc: 0.9399\n",
      "Epoch 94/100\n",
      "6970/6970 [==============================] - 2s 272us/step - loss: 0.0719 - acc: 0.9796 - val_loss: 0.1999 - val_acc: 0.9387\n",
      "Epoch 95/100\n",
      "6970/6970 [==============================] - 2s 259us/step - loss: 0.0770 - acc: 0.9765 - val_loss: 0.2012 - val_acc: 0.9376\n",
      "Epoch 96/100\n",
      "6970/6970 [==============================] - 2s 260us/step - loss: 0.0718 - acc: 0.9791 - val_loss: 0.2008 - val_acc: 0.9381\n",
      "Epoch 97/100\n",
      "6970/6970 [==============================] - 2s 251us/step - loss: 0.0828 - acc: 0.9737 - val_loss: 0.1988 - val_acc: 0.9378\n",
      "Epoch 98/100\n",
      "6970/6970 [==============================] - 2s 265us/step - loss: 0.0774 - acc: 0.9766 - val_loss: 0.2004 - val_acc: 0.9384\n",
      "Epoch 99/100\n",
      "6970/6970 [==============================] - 2s 258us/step - loss: 0.0717 - acc: 0.9785 - val_loss: 0.2097 - val_acc: 0.9384\n",
      "Epoch 100/100\n",
      "6970/6970 [==============================] - 2s 263us/step - loss: 0.0757 - acc: 0.9772 - val_loss: 0.2023 - val_acc: 0.9396\n",
      "Final model parameters set to stochastic weight average.\n",
      "Final stochastic averaged weights saved to file.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "lr = 1e-4\n",
    "AR_double.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = AR_double.fit([X_0,X_1,X_2,X_3],Y,\n",
    "        batch_size=2048,\n",
    "        epochs=500,\n",
    "        verbose=True,\n",
    "        shuffle=True,\n",
    "        callbacks=[lrScheduler],\n",
    "        validation_data=([X_TEST_0,X_TEST_1,X_TEST_2,X_TEST_3],Y_TEST)      \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AR_double.save_weights('weights/weight_xv_1D_double.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('xv_double.json', 'w') as f:\n",
    "    json.dump(str(history.history), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC3MYhmEYhi/MYRiGYRi++H8w4DvSTyER8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9c7793860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_frame_double(p_0,p_1,C):\n",
    "    full_l = p_0.shape[0] # full length\n",
    "    if random.uniform(0,1)<0.5: # aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.85,1)*full_l)\n",
    "        s = random.randint(0, full_l-int(valid_l))\n",
    "        e = s+valid_l # sample end point\n",
    "        p_0 = p_0[int(s):int(e),:,:]\n",
    "        p_1 = p_1[int(s):int(e),:,:]     \n",
    "    else: # without aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.9,1)*full_l)\n",
    "        index = np.sort(np.random.choice(range(0,full_l),int(valid_l),replace=False))\n",
    "        p_0 = p_0[index,:,:]\n",
    "        p_1 = p_1[index,:,:]\n",
    "    p_0 = zoom(p_0,C.frame_l,C.joint_n,C.joint_d)\n",
    "    p_1 = zoom(p_1,C.frame_l,C.joint_n,C.joint_d)\n",
    "    return p_0,p_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "AR_double.compile(loss=\"categorical_crossentropy\",optimizer=Adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 11s 2ms/step - loss: 0.0743 - acc: 0.9746 - val_loss: 0.1978 - val_acc: 0.9445\n",
      "epoch1\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 228us/step - loss: 0.0753 - acc: 0.9763 - val_loss: 0.1804 - val_acc: 0.9453\n",
      "epoch2\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 231us/step - loss: 0.0674 - acc: 0.9786 - val_loss: 0.1773 - val_acc: 0.9462\n",
      "epoch3\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 228us/step - loss: 0.0665 - acc: 0.9789 - val_loss: 0.1741 - val_acc: 0.9456\n",
      "epoch4\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 228us/step - loss: 0.0692 - acc: 0.9776 - val_loss: 0.1825 - val_acc: 0.9459\n",
      "epoch5\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 234us/step - loss: 0.0741 - acc: 0.9765 - val_loss: 0.1791 - val_acc: 0.9459\n",
      "epoch6\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 221us/step - loss: 0.0724 - acc: 0.9763 - val_loss: 0.1917 - val_acc: 0.9462\n",
      "epoch7\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 228us/step - loss: 0.0661 - acc: 0.9798 - val_loss: 0.2030 - val_acc: 0.9424\n",
      "epoch8\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 219us/step - loss: 0.0626 - acc: 0.9799 - val_loss: 0.2088 - val_acc: 0.9407\n",
      "epoch9\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 233us/step - loss: 0.0655 - acc: 0.9789 - val_loss: 0.2008 - val_acc: 0.9433\n",
      "epoch10\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 234us/step - loss: 0.0646 - acc: 0.9796 - val_loss: 0.1941 - val_acc: 0.9439\n",
      "epoch11\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 222us/step - loss: 0.0701 - acc: 0.9765 - val_loss: 0.1986 - val_acc: 0.9422\n",
      "epoch12\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 229us/step - loss: 0.0638 - acc: 0.9786 - val_loss: 0.1943 - val_acc: 0.9447\n",
      "epoch13\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 239us/step - loss: 0.0678 - acc: 0.9788 - val_loss: 0.1914 - val_acc: 0.9422\n",
      "epoch14\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 218us/step - loss: 0.0690 - acc: 0.9780 - val_loss: 0.2002 - val_acc: 0.9430\n",
      "epoch15\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 223us/step - loss: 0.0724 - acc: 0.9776 - val_loss: 0.2104 - val_acc: 0.9416\n",
      "epoch16\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 238us/step - loss: 0.0659 - acc: 0.9793 - val_loss: 0.1949 - val_acc: 0.9436\n",
      "epoch17\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 226us/step - loss: 0.0708 - acc: 0.9768 - val_loss: 0.1952 - val_acc: 0.9430\n",
      "epoch18\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 223us/step - loss: 0.0672 - acc: 0.9798 - val_loss: 0.1943 - val_acc: 0.9436\n",
      "epoch19\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 238us/step - loss: 0.0652 - acc: 0.9791 - val_loss: 0.1893 - val_acc: 0.9445\n",
      "epoch20\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 239us/step - loss: 0.0583 - acc: 0.9834 - val_loss: 0.1922 - val_acc: 0.9430\n",
      "epoch21\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 233us/step - loss: 0.0713 - acc: 0.9778 - val_loss: 0.2066 - val_acc: 0.9413\n",
      "epoch22\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 225us/step - loss: 0.0715 - acc: 0.9786 - val_loss: 0.2048 - val_acc: 0.9410\n",
      "epoch23\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 229us/step - loss: 0.0648 - acc: 0.9803 - val_loss: 0.1945 - val_acc: 0.9427\n",
      "epoch24\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 238us/step - loss: 0.0688 - acc: 0.9791 - val_loss: 0.2024 - val_acc: 0.9442\n",
      "epoch25\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 233us/step - loss: 0.0636 - acc: 0.9808 - val_loss: 0.2154 - val_acc: 0.9422\n",
      "epoch26\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 233us/step - loss: 0.0700 - acc: 0.9768 - val_loss: 0.2199 - val_acc: 0.9387\n",
      "epoch27\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 239us/step - loss: 0.0686 - acc: 0.9769 - val_loss: 0.2072 - val_acc: 0.9422\n",
      "epoch28\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 233us/step - loss: 0.0668 - acc: 0.9789 - val_loss: 0.2230 - val_acc: 0.9364\n",
      "epoch29\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 240us/step - loss: 0.0636 - acc: 0.9798 - val_loss: 0.1890 - val_acc: 0.9447\n",
      "epoch30\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 221us/step - loss: 0.0622 - acc: 0.9819 - val_loss: 0.1944 - val_acc: 0.9459\n",
      "epoch31\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 237us/step - loss: 0.0636 - acc: 0.9803 - val_loss: 0.1876 - val_acc: 0.9447\n",
      "epoch32\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 226us/step - loss: 0.0650 - acc: 0.9798 - val_loss: 0.1806 - val_acc: 0.9473\n",
      "epoch33\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 228us/step - loss: 0.0675 - acc: 0.9806 - val_loss: 0.1884 - val_acc: 0.9445\n",
      "epoch34\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 230us/step - loss: 0.0582 - acc: 0.9819 - val_loss: 0.1802 - val_acc: 0.9447\n",
      "epoch35\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 223us/step - loss: 0.0604 - acc: 0.9795 - val_loss: 0.1837 - val_acc: 0.9456\n",
      "epoch36\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 233us/step - loss: 0.0673 - acc: 0.9795 - val_loss: 0.1876 - val_acc: 0.9445\n",
      "epoch37\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 242us/step - loss: 0.0658 - acc: 0.9806 - val_loss: 0.1895 - val_acc: 0.9442\n",
      "epoch38\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 243us/step - loss: 0.0590 - acc: 0.9821 - val_loss: 0.1922 - val_acc: 0.9436\n",
      "epoch39\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 242us/step - loss: 0.0638 - acc: 0.9808 - val_loss: 0.1965 - val_acc: 0.9445\n",
      "epoch40\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 234us/step - loss: 0.0647 - acc: 0.9789 - val_loss: 0.1737 - val_acc: 0.9476\n",
      "epoch41\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 224us/step - loss: 0.0573 - acc: 0.9811 - val_loss: 0.1770 - val_acc: 0.9473\n",
      "epoch42\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 232us/step - loss: 0.0576 - acc: 0.9805 - val_loss: 0.1722 - val_acc: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch43\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 229us/step - loss: 0.0621 - acc: 0.9802 - val_loss: 0.1782 - val_acc: 0.9479\n",
      "epoch44\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 243us/step - loss: 0.0615 - acc: 0.9816 - val_loss: 0.1774 - val_acc: 0.9462\n",
      "epoch45\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 230us/step - loss: 0.0597 - acc: 0.9795 - val_loss: 0.1833 - val_acc: 0.9465\n",
      "epoch46\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 229us/step - loss: 0.0663 - acc: 0.9796 - val_loss: 0.1858 - val_acc: 0.9473\n",
      "epoch47\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 219us/step - loss: 0.0590 - acc: 0.9812 - val_loss: 0.1786 - val_acc: 0.9473\n",
      "epoch48\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 231us/step - loss: 0.0633 - acc: 0.9803 - val_loss: 0.1799 - val_acc: 0.9465\n",
      "epoch49\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 229us/step - loss: 0.0616 - acc: 0.9809 - val_loss: 0.1926 - val_acc: 0.9450\n",
      "epoch50\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 234us/step - loss: 0.0649 - acc: 0.9796 - val_loss: 0.1871 - val_acc: 0.9459\n",
      "epoch51\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 238us/step - loss: 0.0584 - acc: 0.9832 - val_loss: 0.1820 - val_acc: 0.9462\n",
      "epoch52\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 219us/step - loss: 0.0636 - acc: 0.9813 - val_loss: 0.1775 - val_acc: 0.9476\n",
      "epoch53\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 231us/step - loss: 0.0624 - acc: 0.9791 - val_loss: 0.1924 - val_acc: 0.9436\n",
      "epoch54\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 241us/step - loss: 0.0599 - acc: 0.9812 - val_loss: 0.1870 - val_acc: 0.9462\n",
      "epoch55\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 229us/step - loss: 0.0650 - acc: 0.9792 - val_loss: 0.1857 - val_acc: 0.9439\n",
      "epoch56\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 234us/step - loss: 0.0604 - acc: 0.9795 - val_loss: 0.1988 - val_acc: 0.9424\n",
      "epoch57\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 244us/step - loss: 0.0660 - acc: 0.9803 - val_loss: 0.2024 - val_acc: 0.9416\n",
      "epoch58\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 217us/step - loss: 0.0556 - acc: 0.9841 - val_loss: 0.1946 - val_acc: 0.9430\n",
      "epoch59\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 216us/step - loss: 0.0553 - acc: 0.9839 - val_loss: 0.2142 - val_acc: 0.9396\n",
      "epoch60\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 226us/step - loss: 0.0586 - acc: 0.9834 - val_loss: 0.2221 - val_acc: 0.9393\n",
      "epoch61\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 229us/step - loss: 0.0630 - acc: 0.9801 - val_loss: 0.1943 - val_acc: 0.9450\n",
      "epoch62\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 1s 215us/step - loss: 0.0600 - acc: 0.9809 - val_loss: 0.1929 - val_acc: 0.9439\n",
      "epoch63\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 219us/step - loss: 0.0601 - acc: 0.9815 - val_loss: 0.1948 - val_acc: 0.9447\n",
      "epoch64\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 227us/step - loss: 0.0623 - acc: 0.9793 - val_loss: 0.1935 - val_acc: 0.9445\n",
      "epoch65\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 233us/step - loss: 0.0561 - acc: 0.9826 - val_loss: 0.1888 - val_acc: 0.9436\n",
      "epoch66\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 235us/step - loss: 0.0661 - acc: 0.9791 - val_loss: 0.1872 - val_acc: 0.9430\n",
      "epoch67\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 230us/step - loss: 0.0646 - acc: 0.9788 - val_loss: 0.1900 - val_acc: 0.9442\n",
      "epoch68\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 228us/step - loss: 0.0575 - acc: 0.9818 - val_loss: 0.1790 - val_acc: 0.9465\n",
      "epoch69\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 237us/step - loss: 0.0632 - acc: 0.9803 - val_loss: 0.1815 - val_acc: 0.9459\n",
      "epoch70\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 231us/step - loss: 0.0568 - acc: 0.9824 - val_loss: 0.1754 - val_acc: 0.9468\n",
      "epoch71\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 233us/step - loss: 0.0580 - acc: 0.9822 - val_loss: 0.1759 - val_acc: 0.9465\n",
      "epoch72\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 221us/step - loss: 0.0526 - acc: 0.9841 - val_loss: 0.1838 - val_acc: 0.9450\n",
      "epoch73\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 221us/step - loss: 0.0560 - acc: 0.9846 - val_loss: 0.1766 - val_acc: 0.9468\n",
      "epoch74\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 221us/step - loss: 0.0566 - acc: 0.9824 - val_loss: 0.1810 - val_acc: 0.9462\n",
      "epoch75\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 233us/step - loss: 0.0567 - acc: 0.9835 - val_loss: 0.1736 - val_acc: 0.9465\n",
      "epoch76\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 235us/step - loss: 0.0587 - acc: 0.9822 - val_loss: 0.1777 - val_acc: 0.9476\n",
      "epoch77\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 224us/step - loss: 0.0538 - acc: 0.9825 - val_loss: 0.1845 - val_acc: 0.9462\n",
      "epoch78\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 1s 214us/step - loss: 0.0611 - acc: 0.9812 - val_loss: 0.1801 - val_acc: 0.9465\n",
      "epoch79\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 230us/step - loss: 0.0592 - acc: 0.9812 - val_loss: 0.1728 - val_acc: 0.9485\n",
      "epoch80\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 237us/step - loss: 0.0579 - acc: 0.9812 - val_loss: 0.1743 - val_acc: 0.9482\n",
      "epoch81\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 231us/step - loss: 0.0565 - acc: 0.9824 - val_loss: 0.1755 - val_acc: 0.9479\n",
      "epoch82\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 238us/step - loss: 0.0586 - acc: 0.9811 - val_loss: 0.1797 - val_acc: 0.9473\n",
      "epoch83\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 240us/step - loss: 0.0598 - acc: 0.9798 - val_loss: 0.1964 - val_acc: 0.9442\n",
      "epoch84\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 228us/step - loss: 0.0635 - acc: 0.9809 - val_loss: 0.1829 - val_acc: 0.9479\n",
      "epoch85\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 220us/step - loss: 0.0524 - acc: 0.9842 - val_loss: 0.1831 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch86\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 240us/step - loss: 0.0610 - acc: 0.9788 - val_loss: 0.1926 - val_acc: 0.9462\n",
      "epoch87\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 232us/step - loss: 0.0541 - acc: 0.9822 - val_loss: 0.1843 - val_acc: 0.9502\n",
      "epoch88\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 237us/step - loss: 0.0603 - acc: 0.9799 - val_loss: 0.1833 - val_acc: 0.9479\n",
      "epoch89\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 1s 212us/step - loss: 0.0615 - acc: 0.9789 - val_loss: 0.1908 - val_acc: 0.9468\n",
      "epoch90\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 228us/step - loss: 0.0559 - acc: 0.9824 - val_loss: 0.1755 - val_acc: 0.9482\n",
      "epoch91\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 238us/step - loss: 0.0565 - acc: 0.9826 - val_loss: 0.1903 - val_acc: 0.9456\n",
      "epoch92\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 226us/step - loss: 0.0525 - acc: 0.9835 - val_loss: 0.1873 - val_acc: 0.9465\n",
      "epoch93\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 223us/step - loss: 0.0588 - acc: 0.9816 - val_loss: 0.1968 - val_acc: 0.9442\n",
      "epoch94\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 230us/step - loss: 0.0560 - acc: 0.9832 - val_loss: 0.1828 - val_acc: 0.9462\n",
      "epoch95\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 222us/step - loss: 0.0503 - acc: 0.9854 - val_loss: 0.1813 - val_acc: 0.9450\n",
      "epoch96\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 228us/step - loss: 0.0572 - acc: 0.9812 - val_loss: 0.1918 - val_acc: 0.9439\n",
      "epoch97\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 1s 211us/step - loss: 0.0610 - acc: 0.9825 - val_loss: 0.1861 - val_acc: 0.9445\n",
      "epoch98\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 230us/step - loss: 0.0581 - acc: 0.9813 - val_loss: 0.1844 - val_acc: 0.9459\n",
      "epoch99\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 218us/step - loss: 0.0535 - acc: 0.9819 - val_loss: 0.1829 - val_acc: 0.9450\n",
      "epoch100\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 229us/step - loss: 0.0567 - acc: 0.9835 - val_loss: 0.1750 - val_acc: 0.9485\n",
      "epoch101\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 2s 219us/step - loss: 0.0577 - acc: 0.9834 - val_loss: 0.1778 - val_acc: 0.9482\n",
      "epoch102\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 5s 649us/step - loss: 0.0529 - acc: 0.9845 - val_loss: 0.1817 - val_acc: 0.9471\n",
      "epoch103\n",
      "Train on 6970 samples, validate on 3475 samples\n",
      "Epoch 1/1\n",
      "6970/6970 [==============================] - 5s 651us/step - loss: 0.0493 - acc: 0.9852 - val_loss: 0.1761 - val_acc: 0.9485\n",
      "epoch104\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    X_2 = []\n",
    "    X_3 = []\n",
    "    Y = []\n",
    "\n",
    "    #for i in tqdm(range(len(xv_train['label']))): \n",
    "    for i in range(len(xv_train['label'])):\n",
    "        p_0 = xv_train['poses'][i][0][:,C.joint_ind,:]\n",
    "        p_1 = xv_train['poses'][i][1][:,C.joint_ind,:]\n",
    "        \n",
    "        if np.all(p_0==0) or np.all(p_1==0): \n",
    "            continue\n",
    "\n",
    "        p_0,p_1 = sampling_frame_double(p_0,p_1,C)\n",
    "           \n",
    "        try:\n",
    "            label = np.zeros(11)\n",
    "            label[xv_train['label'][i]-50] = 1   \n",
    "        except:\n",
    "            continue\n",
    "            #print(xv_train['name'][i])\n",
    "\n",
    "        M_0,M_1 = get_CG_double(p_0,p_1,C)\n",
    "        \n",
    "        #rotation\n",
    "        x_angle = np.random.uniform(-0.02,0.02)\n",
    "        y_angle = np.random.uniform(-np.pi/3,np.pi/3)\n",
    "        z_angle = np.random.uniform(-0.02,0.02)\n",
    "        R = euler2mat(x_angle, y_angle, z_angle, 'sxyz')\n",
    "        p_0,p_1 = rotaion_two(p_0,p_1,R)\n",
    "\n",
    "        X_0.append(M_0)\n",
    "        X_1.append(M_1)\n",
    "        X_2.append(p_0)\n",
    "        X_3.append(p_1)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    X_2 = np.stack(X_2)  \n",
    "    X_3 = np.stack(X_3) \n",
    "    Y = np.stack(Y)\n",
    "\n",
    "    AR_double.fit([X_0,X_1,X_2,X_3],Y,\n",
    "            batch_size=2048,\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_TEST_0,X_TEST_1,X_TEST_2,X_TEST_3],Y_TEST)\n",
    "            )\n",
    "\n",
    "    del X_0\n",
    "    del X_1\n",
    "    del X_2\n",
    "    del X_3\n",
    "    del Y\n",
    "    gc.collect()\n",
    "\n",
    "    if e%10==0:\n",
    "        AR_double.save_weights('weights/weight_xv_1D_double_aug.h5')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AR_double.save_weights('weights/weight_xv_1D_double_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross view validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xv_val = pd.read_pickle(data_path+xlist[1],compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 43/18932 [00:00<00:46, 404.91it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 18932/18932 [00:29<00:00, 639.82it/s]\n"
     ]
    }
   ],
   "source": [
    "X_TEST_0 = []\n",
    "X_TEST_1 = []\n",
    "X_TEST_2 = []\n",
    "X_TEST_3 = []\n",
    "Y_TEST = []\n",
    "num_xv_val = len(xv_val['label'])\n",
    "for i in tqdm(range(num_xv_val)): \n",
    "    p_0 = xv_val['poses'][i][0]#[:,C.joint_ind,:]\n",
    "    p_1 = xv_val['poses'][i][1]#[:,C.joint_ind,:]\n",
    "\n",
    "    if np.all(p_0==0) or np.all(p_1==0): \n",
    "        continue\n",
    "           \n",
    "    p_0 = zoom(p_0,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p_1 = zoom(p_1,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    \n",
    "    try:\n",
    "        label = np.zeros(11)\n",
    "        label[xv_val['label'][i]-50] = 1   \n",
    "    except:\n",
    "        continue\n",
    "        #print(xv_val['name'][i])\n",
    "\n",
    "    M_0,M_1 = get_CG_double(p_0,p_1,C)\n",
    "\n",
    "    X_TEST_0.append(M_0)\n",
    "    X_TEST_1.append(M_1)\n",
    "    X_TEST_2.append(p_0)\n",
    "    X_TEST_3.append(p_1)\n",
    "    Y_TEST.append(label)\n",
    "\n",
    "X_TEST_0 = np.stack(X_TEST_0)  \n",
    "X_TEST_1 = np.stack(X_TEST_1) \n",
    "X_TEST_2 = np.stack(X_TEST_2)  \n",
    "X_TEST_3 = np.stack(X_TEST_3) \n",
    "Y_TEST = np.stack(Y_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9470503597122302\n"
     ]
    }
   ],
   "source": [
    "Y_pred = AR_double.predict([X_TEST_0,X_TEST_1,X_TEST_2,X_TEST_3])\n",
    "acc = 1-np.count_nonzero(np.argmax(Y_pred,axis=1)-np.argmax(Y_TEST,axis=1))/Y_TEST.shape[0]\n",
    "print('Accuracy:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3022"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.20984879763434"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3022*94.71+14901*90.5)/(14901+3022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
