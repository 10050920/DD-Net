{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 21 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joint\n",
    "        self.joint_ind = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "        self.key_ind = np.array([1,3,7,11,14,18])\n",
    "        self.feat_d = 252\n",
    "        self.filters = 64\n",
    "        self.clc_num=49\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def get_CG_double(p_0,p_1,C):\n",
    "    M_0 = []\n",
    "    M_1 = []\n",
    "\n",
    "    for f in range(C.frame_l):\n",
    "        #correlation graph \n",
    "        d_m_0 = cdist(p_0[f][C.key_ind,:],p_0[f],'euclidean').flatten()\n",
    "        d_m_1 = cdist(p_1[f][C.key_ind,:],p_1[f],'euclidean').flatten()\n",
    "        d_m_01 = cdist(p_0[f][C.key_ind,:],p_1[f],'euclidean').flatten()\n",
    "        d_m_10 = cdist(p_1[f][C.key_ind,:],p_0[f],'euclidean').flatten() \n",
    "        \n",
    "        M_0.append(np.concatenate([d_m_0,d_m_01]))\n",
    "        M_1.append(np.concatenate([d_m_1,d_m_10]))\n",
    " \n",
    "    M_0 = np.stack(M_0)\n",
    "    M_1 = np.stack(M_1)   \n",
    "    return M_0,M_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,:1,...],x[:,:-1,...])\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    #P_diff_slow = Lambda(lambda x: tf.norm(x,axis=-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    #P_diff_fast = Lambda(lambda x: tf.norm(x,axis=-1))(P_diff_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "\n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=21,joint_d=3,feat_d=210,filters=128):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "\n",
    "    x = c1D(M,filters*2,1)\n",
    "    x = SpatialDropout1D(0.05)(x)\n",
    "    x = c1D(x,filters,3)\n",
    "    x = block(x,filters)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = SpatialDropout1D(0.05)(x)\n",
    "\n",
    "    x_d_slow = c1D(diff_slow,filters*2,1)\n",
    "    x_d_slow = SpatialDropout1D(0.05)(x_d_slow)\n",
    "    x_d_slow = block(x_d_slow,filters)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    x_d_slow = SpatialDropout1D(0.05)(x_d_slow)\n",
    "        \n",
    "    x_d_fast = c1D(diff_fast,filters*2,1)\n",
    "    x_d_fast = SpatialDropout1D(0.05)(x_d_fast)\n",
    "    x_d_fast = block(x_d_fast,filters) \n",
    "    x_d_fast = SpatialDropout1D(0.05)(x_d_fast)\n",
    "    \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "    return Model(inputs=[M,P],outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_AR_double(C):\n",
    "    M_0 = Input(name='M_0', shape=(C.frame_l,C.feat_d))  \n",
    "    M_1 = Input(name='M_1', shape=(C.frame_l,C.feat_d))\n",
    "    P_0 = Input(name='P_0', shape=(C.frame_l,C.joint_n,C.joint_d)) \n",
    "    P_1 = Input(name='P_1', shape=(C.frame_l,C.joint_n,C.joint_d)) \n",
    "    \n",
    "    FM = build_FM(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.filters)\n",
    "    \n",
    "    x_0 = FM([M_0,P_0])\n",
    "    x_1 = FM([M_1,P_1])\n",
    "    \n",
    "    x = maximum([x_0,x_1])\n",
    "    \n",
    "    x = GlobalMaxPool1D()(x)\n",
    "\n",
    "    x = d1D(x,256)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = d1D(x,256)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(C.clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M_0,M_1,P_0,P_1],outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_double = build_AR_double(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M_0 (InputLayer)                (None, 32, 252)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P_0 (InputLayer)                (None, 32, 21, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "M_1 (InputLayer)                (None, 32, 252)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P_1 (InputLayer)                (None, 32, 21, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 512)       1779200     M_0[0][0]                        \n",
      "                                                                 P_0[0][0]                        \n",
      "                                                                 M_1[0][0]                        \n",
      "                                                                 P_1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "maximum_1 (Maximum)             (None, 4, 512)       0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           maximum_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131072      global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 256)          0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65536       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 256)          0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           2827        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,980,683\n",
      "Trainable params: 1,974,411\n",
      "Non-trainable params: 6,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AR_double.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AR_double.load_weights('weights/weight_xo_1D_double_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/mnt/nasbi/homes/fan/projects/action/skeleton/data/NTU/'\n",
    "xlist = ['xview_train.pkl','xview_val.pkl','xsub_train.pkl','xsub_val.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross view train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xobj_train = pd.read_pickle(data_path+xlist[2],compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/40091 [00:00<01:21, 489.70it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 40091/40091 [01:06<00:00, 601.68it/s]\n"
     ]
    }
   ],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "X_2 = []\n",
    "X_3 = []\n",
    "Y = []\n",
    "num_xobj_train = len(xobj_train['label'])\n",
    "for i in tqdm(range(num_xobj_train)): \n",
    "    p_0 = xobj_train['poses'][i][0][:,C.joint_ind,:]\n",
    "    p_1 = xobj_train['poses'][i][1][:,C.joint_ind,:]\n",
    "\n",
    "    if np.all(p_0==0) or np.all(p_1==0): \n",
    "        continue\n",
    "           \n",
    "    p_0 = zoom(p_0,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p_1 = zoom(p_1,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    \n",
    "    try:\n",
    "        label = np.zeros(11)\n",
    "        label[xobj_train['label'][i]-50] = 1   \n",
    "    except:\n",
    "        continue\n",
    "        #print(xobj_train['name'][i])\n",
    "\n",
    "    M_0,M_1 = get_CG_double(p_0,p_1,C)\n",
    "\n",
    "    X_0.append(M_0)\n",
    "    X_1.append(M_1)\n",
    "    X_2.append(p_0)\n",
    "    X_3.append(p_1)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  \n",
    "X_1 = np.stack(X_1) \n",
    "X_2 = np.stack(X_2)  \n",
    "X_3 = np.stack(X_3) \n",
    "Y = np.stack(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/300\n",
      "7423/7423 [==============================] - 9s 1ms/step - loss: 0.0540 - acc: 0.9841 - val_loss: 0.2971 - val_acc: 0.9136\n",
      "Epoch 2/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0540 - acc: 0.9838 - val_loss: 0.2981 - val_acc: 0.9136\n",
      "Epoch 3/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0481 - acc: 0.9869 - val_loss: 0.3001 - val_acc: 0.9166\n",
      "Epoch 4/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0516 - acc: 0.9848 - val_loss: 0.2963 - val_acc: 0.9153\n",
      "Epoch 5/300\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0478 - acc: 0.9860 - val_loss: 0.3004 - val_acc: 0.9153\n",
      "Epoch 6/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0515 - acc: 0.9852 - val_loss: 0.2952 - val_acc: 0.9153\n",
      "Epoch 7/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0542 - acc: 0.9855 - val_loss: 0.2984 - val_acc: 0.9140\n",
      "Epoch 8/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0531 - acc: 0.9856 - val_loss: 0.2901 - val_acc: 0.9163\n",
      "Epoch 9/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0501 - acc: 0.9859 - val_loss: 0.3009 - val_acc: 0.9146\n",
      "Epoch 10/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0508 - acc: 0.9876 - val_loss: 0.2913 - val_acc: 0.9159\n",
      "Epoch 11/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0479 - acc: 0.9877 - val_loss: 0.2908 - val_acc: 0.9163\n",
      "Epoch 12/300\n",
      "7423/7423 [==============================] - 1s 197us/step - loss: 0.0503 - acc: 0.9859 - val_loss: 0.2937 - val_acc: 0.9156\n",
      "Epoch 13/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0510 - acc: 0.9848 - val_loss: 0.2896 - val_acc: 0.9163\n",
      "Epoch 14/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0485 - acc: 0.9859 - val_loss: 0.2967 - val_acc: 0.9143\n",
      "Epoch 15/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0538 - acc: 0.9841 - val_loss: 0.2934 - val_acc: 0.9150\n",
      "Epoch 16/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0489 - acc: 0.9868 - val_loss: 0.3014 - val_acc: 0.9136\n",
      "Epoch 17/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0510 - acc: 0.9859 - val_loss: 0.2946 - val_acc: 0.9146\n",
      "Epoch 18/300\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0474 - acc: 0.9873 - val_loss: 0.3000 - val_acc: 0.9143\n",
      "Epoch 19/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0537 - acc: 0.9850 - val_loss: 0.2984 - val_acc: 0.9159\n",
      "Epoch 20/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0509 - acc: 0.9850 - val_loss: 0.2949 - val_acc: 0.9153\n",
      "Epoch 21/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0531 - acc: 0.9833 - val_loss: 0.2977 - val_acc: 0.9153\n",
      "Epoch 22/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0442 - acc: 0.9876 - val_loss: 0.2996 - val_acc: 0.9143\n",
      "Epoch 23/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0507 - acc: 0.9841 - val_loss: 0.2941 - val_acc: 0.9176\n",
      "Epoch 24/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0495 - acc: 0.9868 - val_loss: 0.2981 - val_acc: 0.9133\n",
      "Epoch 25/300\n",
      "7423/7423 [==============================] - 1s 196us/step - loss: 0.0441 - acc: 0.9877 - val_loss: 0.2916 - val_acc: 0.9173\n",
      "Epoch 26/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0478 - acc: 0.9873 - val_loss: 0.2988 - val_acc: 0.9136\n",
      "Epoch 27/300\n",
      "7423/7423 [==============================] - 1s 199us/step - loss: 0.0506 - acc: 0.9856 - val_loss: 0.2988 - val_acc: 0.9156\n",
      "Epoch 28/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0502 - acc: 0.9864 - val_loss: 0.3045 - val_acc: 0.9140\n",
      "Epoch 29/300\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0475 - acc: 0.9864 - val_loss: 0.2989 - val_acc: 0.9166\n",
      "Epoch 30/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0490 - acc: 0.9869 - val_loss: 0.3008 - val_acc: 0.9153\n",
      "Epoch 31/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0453 - acc: 0.9876 - val_loss: 0.2959 - val_acc: 0.9173\n",
      "Epoch 32/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0487 - acc: 0.9865 - val_loss: 0.2967 - val_acc: 0.9143\n",
      "Epoch 33/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0463 - acc: 0.9883 - val_loss: 0.2921 - val_acc: 0.9173\n",
      "Epoch 34/300\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0483 - acc: 0.9863 - val_loss: 0.2999 - val_acc: 0.9153\n",
      "Epoch 35/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0481 - acc: 0.9875 - val_loss: 0.2958 - val_acc: 0.9166\n",
      "Epoch 36/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0475 - acc: 0.9856 - val_loss: 0.2946 - val_acc: 0.9179\n",
      "Epoch 37/300\n",
      "7423/7423 [==============================] - 1s 197us/step - loss: 0.0483 - acc: 0.9859 - val_loss: 0.2978 - val_acc: 0.9169\n",
      "Epoch 38/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0474 - acc: 0.9860 - val_loss: 0.2971 - val_acc: 0.9150\n",
      "Epoch 39/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0481 - acc: 0.9876 - val_loss: 0.2938 - val_acc: 0.9179\n",
      "Epoch 40/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0479 - acc: 0.9868 - val_loss: 0.2963 - val_acc: 0.9156\n",
      "Epoch 41/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0478 - acc: 0.9865 - val_loss: 0.2980 - val_acc: 0.9150\n",
      "Epoch 42/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0452 - acc: 0.9873 - val_loss: 0.3017 - val_acc: 0.9143\n",
      "Epoch 43/300\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0503 - acc: 0.9855 - val_loss: 0.2988 - val_acc: 0.9163\n",
      "Epoch 44/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0448 - acc: 0.9892 - val_loss: 0.3026 - val_acc: 0.9140\n",
      "Epoch 45/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0446 - acc: 0.9876 - val_loss: 0.2946 - val_acc: 0.9176\n",
      "Epoch 46/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0493 - acc: 0.9849 - val_loss: 0.2991 - val_acc: 0.9146\n",
      "Epoch 47/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0440 - acc: 0.9883 - val_loss: 0.2904 - val_acc: 0.9166\n",
      "Epoch 48/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0478 - acc: 0.9871 - val_loss: 0.2997 - val_acc: 0.9166\n",
      "Epoch 49/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0495 - acc: 0.9849 - val_loss: 0.2875 - val_acc: 0.9186\n",
      "Epoch 50/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0495 - acc: 0.9856 - val_loss: 0.2888 - val_acc: 0.9179\n",
      "Epoch 51/300\n",
      "7423/7423 [==============================] - 1s 200us/step - loss: 0.0467 - acc: 0.9861 - val_loss: 0.2944 - val_acc: 0.9146\n",
      "Epoch 52/300\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0514 - acc: 0.9850 - val_loss: 0.2928 - val_acc: 0.9166\n",
      "Epoch 53/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0469 - acc: 0.9872 - val_loss: 0.2891 - val_acc: 0.9186\n",
      "Epoch 54/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0436 - acc: 0.9891 - val_loss: 0.2982 - val_acc: 0.9156\n",
      "Epoch 55/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0487 - acc: 0.9841 - val_loss: 0.2938 - val_acc: 0.9153\n",
      "Epoch 56/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0474 - acc: 0.9857 - val_loss: 0.2975 - val_acc: 0.9153\n",
      "Epoch 57/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0461 - acc: 0.9885 - val_loss: 0.2941 - val_acc: 0.9156\n",
      "Epoch 58/300\n",
      "7423/7423 [==============================] - 2s 202us/step - loss: 0.0458 - acc: 0.9875 - val_loss: 0.2998 - val_acc: 0.9169\n",
      "Epoch 59/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0472 - acc: 0.9863 - val_loss: 0.3017 - val_acc: 0.9153\n",
      "Epoch 60/300\n",
      "7423/7423 [==============================] - 2s 202us/step - loss: 0.0468 - acc: 0.9861 - val_loss: 0.3027 - val_acc: 0.9156\n",
      "Epoch 61/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0465 - acc: 0.9876 - val_loss: 0.3001 - val_acc: 0.9143\n",
      "Epoch 62/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0458 - acc: 0.9876 - val_loss: 0.3025 - val_acc: 0.9169\n",
      "Epoch 63/300\n",
      "7423/7423 [==============================] - 1s 197us/step - loss: 0.0486 - acc: 0.9864 - val_loss: 0.2958 - val_acc: 0.9179\n",
      "Epoch 64/300\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0440 - acc: 0.9887 - val_loss: 0.3013 - val_acc: 0.9133\n",
      "Epoch 65/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0460 - acc: 0.9876 - val_loss: 0.2983 - val_acc: 0.9156\n",
      "Epoch 66/300\n",
      "7423/7423 [==============================] - 1s 199us/step - loss: 0.0450 - acc: 0.9876 - val_loss: 0.2951 - val_acc: 0.9173\n",
      "Epoch 67/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0451 - acc: 0.9863 - val_loss: 0.2995 - val_acc: 0.9146\n",
      "Epoch 68/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0436 - acc: 0.9888 - val_loss: 0.2988 - val_acc: 0.9159\n",
      "Epoch 69/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0491 - acc: 0.9873 - val_loss: 0.3029 - val_acc: 0.9150\n",
      "Epoch 70/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0484 - acc: 0.9869 - val_loss: 0.2998 - val_acc: 0.9143\n",
      "Epoch 71/300\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0420 - acc: 0.9900 - val_loss: 0.3037 - val_acc: 0.9140\n",
      "Epoch 72/300\n",
      "7423/7423 [==============================] - 1s 200us/step - loss: 0.0460 - acc: 0.9868 - val_loss: 0.2992 - val_acc: 0.9176\n",
      "Epoch 73/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0465 - acc: 0.9877 - val_loss: 0.3003 - val_acc: 0.9153\n",
      "Epoch 74/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0504 - acc: 0.9857 - val_loss: 0.3008 - val_acc: 0.9153\n",
      "Epoch 75/300\n",
      "7423/7423 [==============================] - 1s 196us/step - loss: 0.0442 - acc: 0.9884 - val_loss: 0.2909 - val_acc: 0.9183\n",
      "Epoch 76/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0475 - acc: 0.9861 - val_loss: 0.2970 - val_acc: 0.9150\n",
      "Epoch 77/300\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0495 - acc: 0.9860 - val_loss: 0.2965 - val_acc: 0.9159\n",
      "Epoch 78/300\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0419 - acc: 0.9888 - val_loss: 0.2997 - val_acc: 0.9153\n",
      "Epoch 79/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0451 - acc: 0.9871 - val_loss: 0.2978 - val_acc: 0.9163\n",
      "Epoch 80/300\n",
      "7423/7423 [==============================] - 1s 200us/step - loss: 0.0442 - acc: 0.9853 - val_loss: 0.2926 - val_acc: 0.9173\n",
      "Epoch 81/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0425 - acc: 0.9887 - val_loss: 0.2992 - val_acc: 0.9146\n",
      "Epoch 82/300\n",
      "7423/7423 [==============================] - 1s 202us/step - loss: 0.0496 - acc: 0.9863 - val_loss: 0.2954 - val_acc: 0.9146\n",
      "Epoch 83/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0453 - acc: 0.9877 - val_loss: 0.3010 - val_acc: 0.9159\n",
      "Epoch 84/300\n",
      "7423/7423 [==============================] - 1s 200us/step - loss: 0.0466 - acc: 0.9863 - val_loss: 0.2967 - val_acc: 0.9159\n",
      "Epoch 85/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0459 - acc: 0.9876 - val_loss: 0.2938 - val_acc: 0.9169\n",
      "Epoch 86/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0401 - acc: 0.9899 - val_loss: 0.2967 - val_acc: 0.9176\n",
      "Epoch 87/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0450 - acc: 0.9880 - val_loss: 0.2940 - val_acc: 0.9179\n",
      "Epoch 88/300\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0450 - acc: 0.9881 - val_loss: 0.2980 - val_acc: 0.9176\n",
      "Epoch 89/300\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0463 - acc: 0.9873 - val_loss: 0.2939 - val_acc: 0.9179\n",
      "Epoch 90/300\n",
      "7423/7423 [==============================] - 2s 202us/step - loss: 0.0377 - acc: 0.9910 - val_loss: 0.2981 - val_acc: 0.9176\n",
      "Epoch 91/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0442 - acc: 0.9877 - val_loss: 0.2953 - val_acc: 0.9169\n",
      "Epoch 92/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0444 - acc: 0.9880 - val_loss: 0.2945 - val_acc: 0.9186\n",
      "Epoch 93/300\n",
      "7423/7423 [==============================] - 1s 196us/step - loss: 0.0475 - acc: 0.9868 - val_loss: 0.2944 - val_acc: 0.9163\n",
      "Epoch 94/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0461 - acc: 0.9880 - val_loss: 0.2985 - val_acc: 0.9146\n",
      "Epoch 95/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0493 - acc: 0.9857 - val_loss: 0.2946 - val_acc: 0.9163\n",
      "Epoch 96/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0487 - acc: 0.9863 - val_loss: 0.2951 - val_acc: 0.9166\n",
      "Epoch 97/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0475 - acc: 0.9856 - val_loss: 0.3026 - val_acc: 0.9143\n",
      "Epoch 98/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0418 - acc: 0.9888 - val_loss: 0.3005 - val_acc: 0.9150\n",
      "Epoch 99/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0417 - acc: 0.9877 - val_loss: 0.3064 - val_acc: 0.9133\n",
      "Epoch 100/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0445 - acc: 0.9885 - val_loss: 0.3054 - val_acc: 0.9136\n",
      "Epoch 101/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0435 - acc: 0.9879 - val_loss: 0.3041 - val_acc: 0.9143\n",
      "Epoch 102/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0462 - acc: 0.9871 - val_loss: 0.3062 - val_acc: 0.9130\n",
      "Epoch 103/300\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0435 - acc: 0.9883 - val_loss: 0.3016 - val_acc: 0.9176\n",
      "Epoch 104/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0462 - acc: 0.9867 - val_loss: 0.3012 - val_acc: 0.9156\n",
      "Epoch 105/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0414 - acc: 0.9896 - val_loss: 0.2943 - val_acc: 0.9166\n",
      "Epoch 106/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0406 - acc: 0.9906 - val_loss: 0.2975 - val_acc: 0.9179\n",
      "Epoch 107/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0426 - acc: 0.9881 - val_loss: 0.3014 - val_acc: 0.9133\n",
      "Epoch 108/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0426 - acc: 0.9877 - val_loss: 0.3022 - val_acc: 0.9133\n",
      "Epoch 109/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0441 - acc: 0.9894 - val_loss: 0.2930 - val_acc: 0.9163\n",
      "Epoch 110/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0441 - acc: 0.9881 - val_loss: 0.3010 - val_acc: 0.9150\n",
      "Epoch 111/300\n",
      "7423/7423 [==============================] - 1s 199us/step - loss: 0.0496 - acc: 0.9844 - val_loss: 0.2955 - val_acc: 0.9153\n",
      "Epoch 112/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0399 - acc: 0.9914 - val_loss: 0.3004 - val_acc: 0.9143\n",
      "Epoch 113/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0506 - acc: 0.9864 - val_loss: 0.3030 - val_acc: 0.9146\n",
      "Epoch 114/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0395 - acc: 0.9904 - val_loss: 0.2970 - val_acc: 0.9153\n",
      "Epoch 115/300\n",
      "7423/7423 [==============================] - 1s 199us/step - loss: 0.0470 - acc: 0.9863 - val_loss: 0.3003 - val_acc: 0.9153\n",
      "Epoch 116/300\n",
      "7423/7423 [==============================] - 1s 197us/step - loss: 0.0498 - acc: 0.9846 - val_loss: 0.2984 - val_acc: 0.9150\n",
      "Epoch 117/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0435 - acc: 0.9891 - val_loss: 0.2997 - val_acc: 0.9163\n",
      "Epoch 118/300\n",
      "7423/7423 [==============================] - 2s 202us/step - loss: 0.0435 - acc: 0.9879 - val_loss: 0.3058 - val_acc: 0.9126\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0466 - acc: 0.9875 - val_loss: 0.3035 - val_acc: 0.9136\n",
      "Epoch 120/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0463 - acc: 0.9881 - val_loss: 0.2953 - val_acc: 0.9189\n",
      "Epoch 121/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0448 - acc: 0.9885 - val_loss: 0.3031 - val_acc: 0.9136\n",
      "Epoch 122/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0405 - acc: 0.9900 - val_loss: 0.3026 - val_acc: 0.9153\n",
      "Epoch 123/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0432 - acc: 0.9883 - val_loss: 0.3044 - val_acc: 0.9140\n",
      "Epoch 124/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0422 - acc: 0.9887 - val_loss: 0.3002 - val_acc: 0.9159\n",
      "Epoch 125/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0437 - acc: 0.9883 - val_loss: 0.2957 - val_acc: 0.9176\n",
      "Epoch 126/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0458 - acc: 0.9867 - val_loss: 0.3022 - val_acc: 0.9173\n",
      "Epoch 127/300\n",
      "7423/7423 [==============================] - 1s 199us/step - loss: 0.0440 - acc: 0.9883 - val_loss: 0.2991 - val_acc: 0.9153\n",
      "Epoch 128/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0445 - acc: 0.9885 - val_loss: 0.3017 - val_acc: 0.9136\n",
      "Epoch 129/300\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0423 - acc: 0.9890 - val_loss: 0.3056 - val_acc: 0.9140\n",
      "Epoch 130/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0421 - acc: 0.9890 - val_loss: 0.3035 - val_acc: 0.9143\n",
      "Epoch 131/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0403 - acc: 0.9892 - val_loss: 0.3060 - val_acc: 0.9133\n",
      "Epoch 132/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0464 - acc: 0.9872 - val_loss: 0.3082 - val_acc: 0.9130\n",
      "Epoch 133/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0403 - acc: 0.9899 - val_loss: 0.3061 - val_acc: 0.9150\n",
      "Epoch 134/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0432 - acc: 0.9871 - val_loss: 0.3055 - val_acc: 0.9146\n",
      "Epoch 135/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0468 - acc: 0.9865 - val_loss: 0.3042 - val_acc: 0.9120\n",
      "Epoch 136/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0455 - acc: 0.9879 - val_loss: 0.3037 - val_acc: 0.9143\n",
      "Epoch 137/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0468 - acc: 0.9880 - val_loss: 0.3019 - val_acc: 0.9179\n",
      "Epoch 138/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0464 - acc: 0.9877 - val_loss: 0.3051 - val_acc: 0.9156\n",
      "Epoch 139/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0426 - acc: 0.9891 - val_loss: 0.2988 - val_acc: 0.9169\n",
      "Epoch 140/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0450 - acc: 0.9875 - val_loss: 0.3038 - val_acc: 0.9159\n",
      "Epoch 141/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0446 - acc: 0.9877 - val_loss: 0.3026 - val_acc: 0.9163\n",
      "Epoch 142/300\n",
      "7423/7423 [==============================] - 1s 194us/step - loss: 0.0455 - acc: 0.9868 - val_loss: 0.3023 - val_acc: 0.9163\n",
      "Epoch 143/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0426 - acc: 0.9880 - val_loss: 0.2972 - val_acc: 0.9153\n",
      "Epoch 144/300\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0388 - acc: 0.9890 - val_loss: 0.3004 - val_acc: 0.9159\n",
      "Epoch 145/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0443 - acc: 0.9887 - val_loss: 0.2983 - val_acc: 0.9156\n",
      "Epoch 146/300\n",
      "7423/7423 [==============================] - 1s 198us/step - loss: 0.0430 - acc: 0.9872 - val_loss: 0.3007 - val_acc: 0.9146\n",
      "Epoch 147/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0463 - acc: 0.9879 - val_loss: 0.2947 - val_acc: 0.9173\n",
      "Epoch 148/300\n",
      "7423/7423 [==============================] - 1s 200us/step - loss: 0.0441 - acc: 0.9880 - val_loss: 0.2992 - val_acc: 0.9179\n",
      "Epoch 149/300\n",
      "7423/7423 [==============================] - 1s 197us/step - loss: 0.0438 - acc: 0.9879 - val_loss: 0.3012 - val_acc: 0.9159\n",
      "Epoch 150/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0442 - acc: 0.9879 - val_loss: 0.3024 - val_acc: 0.9146\n",
      "Epoch 151/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0396 - acc: 0.9900 - val_loss: 0.3015 - val_acc: 0.9143\n",
      "Epoch 152/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0441 - acc: 0.9872 - val_loss: 0.3078 - val_acc: 0.9136\n",
      "Epoch 153/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0438 - acc: 0.9879 - val_loss: 0.3021 - val_acc: 0.9150\n",
      "Epoch 154/300\n",
      "7423/7423 [==============================] - 2s 202us/step - loss: 0.0409 - acc: 0.9895 - val_loss: 0.3046 - val_acc: 0.9143\n",
      "Epoch 155/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0438 - acc: 0.9884 - val_loss: 0.3055 - val_acc: 0.9150\n",
      "Epoch 156/300\n",
      "7423/7423 [==============================] - 1s 196us/step - loss: 0.0450 - acc: 0.9861 - val_loss: 0.2972 - val_acc: 0.9146\n",
      "Epoch 157/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0405 - acc: 0.9899 - val_loss: 0.2971 - val_acc: 0.9173\n",
      "Epoch 158/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0416 - acc: 0.9883 - val_loss: 0.2994 - val_acc: 0.9166\n",
      "Epoch 159/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0424 - acc: 0.9892 - val_loss: 0.2945 - val_acc: 0.9153\n",
      "Epoch 160/300\n",
      "7423/7423 [==============================] - 1s 193us/step - loss: 0.0477 - acc: 0.9853 - val_loss: 0.3055 - val_acc: 0.9166\n",
      "Epoch 161/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0448 - acc: 0.9880 - val_loss: 0.3037 - val_acc: 0.9126\n",
      "Epoch 162/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0468 - acc: 0.9871 - val_loss: 0.2962 - val_acc: 0.9143\n",
      "Epoch 163/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0403 - acc: 0.9899 - val_loss: 0.2968 - val_acc: 0.9173\n",
      "Epoch 164/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0411 - acc: 0.9885 - val_loss: 0.2959 - val_acc: 0.9183\n",
      "Epoch 165/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0410 - acc: 0.9880 - val_loss: 0.2973 - val_acc: 0.9140\n",
      "Epoch 166/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0426 - acc: 0.9883 - val_loss: 0.3015 - val_acc: 0.9150\n",
      "Epoch 167/300\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0465 - acc: 0.9879 - val_loss: 0.3021 - val_acc: 0.9186\n",
      "Epoch 168/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0409 - acc: 0.9898 - val_loss: 0.3033 - val_acc: 0.9156\n",
      "Epoch 169/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0452 - acc: 0.9865 - val_loss: 0.3053 - val_acc: 0.9143\n",
      "Epoch 170/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0456 - acc: 0.9880 - val_loss: 0.3024 - val_acc: 0.9189\n",
      "Epoch 171/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0416 - acc: 0.9891 - val_loss: 0.3043 - val_acc: 0.9150\n",
      "Epoch 172/300\n",
      "7423/7423 [==============================] - 2s 202us/step - loss: 0.0430 - acc: 0.9881 - val_loss: 0.3024 - val_acc: 0.9143\n",
      "Epoch 173/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0432 - acc: 0.9883 - val_loss: 0.3067 - val_acc: 0.9150\n",
      "Epoch 174/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0425 - acc: 0.9885 - val_loss: 0.2981 - val_acc: 0.9156\n",
      "Epoch 175/300\n",
      "7423/7423 [==============================] - 1s 196us/step - loss: 0.0427 - acc: 0.9894 - val_loss: 0.2977 - val_acc: 0.9163\n",
      "Epoch 176/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0444 - acc: 0.9869 - val_loss: 0.2951 - val_acc: 0.9176\n",
      "Epoch 177/300\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0382 - acc: 0.9904 - val_loss: 0.3023 - val_acc: 0.9130\n",
      "Epoch 178/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0444 - acc: 0.9877 - val_loss: 0.3016 - val_acc: 0.9166\n",
      "Epoch 179/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0414 - acc: 0.9895 - val_loss: 0.2945 - val_acc: 0.9183\n",
      "Epoch 180/300\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0413 - acc: 0.9891 - val_loss: 0.3028 - val_acc: 0.9146\n",
      "Epoch 181/300\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0417 - acc: 0.9891 - val_loss: 0.2991 - val_acc: 0.9179\n",
      "Epoch 182/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0438 - acc: 0.9871 - val_loss: 0.3025 - val_acc: 0.9159\n",
      "Epoch 183/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0419 - acc: 0.9894 - val_loss: 0.3028 - val_acc: 0.9153\n",
      "Epoch 184/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0448 - acc: 0.9885 - val_loss: 0.3024 - val_acc: 0.9169\n",
      "Epoch 185/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0412 - acc: 0.9892 - val_loss: 0.3000 - val_acc: 0.9186\n",
      "Epoch 186/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0434 - acc: 0.9877 - val_loss: 0.3077 - val_acc: 0.9163\n",
      "Epoch 187/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0388 - acc: 0.9895 - val_loss: 0.3034 - val_acc: 0.9176\n",
      "Epoch 188/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0396 - acc: 0.9888 - val_loss: 0.3086 - val_acc: 0.9146\n",
      "Epoch 189/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0371 - acc: 0.9902 - val_loss: 0.3088 - val_acc: 0.9163\n",
      "Epoch 190/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0414 - acc: 0.9871 - val_loss: 0.3013 - val_acc: 0.9169\n",
      "Epoch 191/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0441 - acc: 0.9890 - val_loss: 0.3050 - val_acc: 0.9159\n",
      "Epoch 192/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0425 - acc: 0.9887 - val_loss: 0.3044 - val_acc: 0.9146\n",
      "Epoch 193/300\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0414 - acc: 0.9894 - val_loss: 0.3034 - val_acc: 0.9166\n",
      "Epoch 194/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0436 - acc: 0.9883 - val_loss: 0.2982 - val_acc: 0.9163\n",
      "Epoch 195/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0427 - acc: 0.9892 - val_loss: 0.3080 - val_acc: 0.9150\n",
      "Epoch 196/300\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0385 - acc: 0.9906 - val_loss: 0.3064 - val_acc: 0.9166\n",
      "Epoch 197/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0422 - acc: 0.9894 - val_loss: 0.3038 - val_acc: 0.9163\n",
      "Epoch 198/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0418 - acc: 0.9884 - val_loss: 0.3112 - val_acc: 0.9169\n",
      "Epoch 199/300\n",
      "7423/7423 [==============================] - 1s 198us/step - loss: 0.0453 - acc: 0.9865 - val_loss: 0.2989 - val_acc: 0.9173\n",
      "Epoch 200/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0462 - acc: 0.9872 - val_loss: 0.3073 - val_acc: 0.9156\n",
      "Epoch 201/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0395 - acc: 0.9892 - val_loss: 0.2963 - val_acc: 0.9203\n",
      "Epoch 202/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0377 - acc: 0.9895 - val_loss: 0.3097 - val_acc: 0.9140\n",
      "Epoch 203/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0390 - acc: 0.9898 - val_loss: 0.3074 - val_acc: 0.9156\n",
      "Epoch 204/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0404 - acc: 0.9888 - val_loss: 0.3097 - val_acc: 0.9133\n",
      "Epoch 205/300\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0379 - acc: 0.9900 - val_loss: 0.3036 - val_acc: 0.9173\n",
      "Epoch 206/300\n",
      "7423/7423 [==============================] - 1s 198us/step - loss: 0.0423 - acc: 0.9877 - val_loss: 0.3096 - val_acc: 0.9176\n",
      "Epoch 207/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0368 - acc: 0.9907 - val_loss: 0.3002 - val_acc: 0.9183\n",
      "Epoch 208/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0399 - acc: 0.9899 - val_loss: 0.3089 - val_acc: 0.9150\n",
      "Epoch 209/300\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0371 - acc: 0.9907 - val_loss: 0.3033 - val_acc: 0.9159\n",
      "Epoch 210/300\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0422 - acc: 0.9879 - val_loss: 0.3114 - val_acc: 0.9163\n",
      "Epoch 211/300\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0435 - acc: 0.9879 - val_loss: 0.3094 - val_acc: 0.9136\n",
      "Epoch 212/300\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0421 - acc: 0.9881 - val_loss: 0.3007 - val_acc: 0.9173\n",
      "Epoch 213/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0412 - acc: 0.9891 - val_loss: 0.3081 - val_acc: 0.9156\n",
      "Epoch 214/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0399 - acc: 0.9900 - val_loss: 0.3074 - val_acc: 0.9153\n",
      "Epoch 215/300\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0429 - acc: 0.9881 - val_loss: 0.3011 - val_acc: 0.9156\n",
      "Epoch 216/300\n",
      "7423/7423 [==============================] - 1s 200us/step - loss: 0.0387 - acc: 0.9895 - val_loss: 0.3120 - val_acc: 0.9163\n",
      "Epoch 217/300\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0438 - acc: 0.9885 - val_loss: 0.3052 - val_acc: 0.9146\n",
      "Epoch 218/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0404 - acc: 0.9896 - val_loss: 0.3012 - val_acc: 0.9183\n",
      "Epoch 219/300\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0445 - acc: 0.9875 - val_loss: 0.3063 - val_acc: 0.9153\n",
      "Epoch 220/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0431 - acc: 0.9888 - val_loss: 0.3081 - val_acc: 0.9143\n",
      "Epoch 221/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0434 - acc: 0.9880 - val_loss: 0.3064 - val_acc: 0.9166\n",
      "Epoch 222/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0428 - acc: 0.9871 - val_loss: 0.3042 - val_acc: 0.9150\n",
      "Epoch 223/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0363 - acc: 0.9902 - val_loss: 0.3112 - val_acc: 0.9150\n",
      "Epoch 224/300\n",
      "7423/7423 [==============================] - 1s 202us/step - loss: 0.0394 - acc: 0.9902 - val_loss: 0.3103 - val_acc: 0.9163\n",
      "Epoch 225/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0419 - acc: 0.9892 - val_loss: 0.3095 - val_acc: 0.9153\n",
      "Epoch 226/300\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0404 - acc: 0.9898 - val_loss: 0.3103 - val_acc: 0.9153\n",
      "Epoch 227/300\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0392 - acc: 0.9903 - val_loss: 0.3109 - val_acc: 0.9150\n",
      "Epoch 228/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0364 - acc: 0.9908 - val_loss: 0.3080 - val_acc: 0.9140\n",
      "Epoch 229/300\n",
      "7423/7423 [==============================] - 1s 197us/step - loss: 0.0422 - acc: 0.9884 - val_loss: 0.3043 - val_acc: 0.9169\n",
      "Epoch 230/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0392 - acc: 0.9896 - val_loss: 0.2983 - val_acc: 0.9189\n",
      "Epoch 231/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0390 - acc: 0.9899 - val_loss: 0.3038 - val_acc: 0.9179\n",
      "Epoch 232/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0365 - acc: 0.9911 - val_loss: 0.3063 - val_acc: 0.9150\n",
      "Epoch 233/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0359 - acc: 0.9916 - val_loss: 0.3107 - val_acc: 0.9153\n",
      "Epoch 234/300\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0398 - acc: 0.9890 - val_loss: 0.3073 - val_acc: 0.9163\n",
      "Epoch 235/300\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0397 - acc: 0.9900 - val_loss: 0.3041 - val_acc: 0.9169\n",
      "Epoch 236/300\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0370 - acc: 0.9891 - val_loss: 0.3082 - val_acc: 0.9166\n",
      "Epoch 237/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0421 - acc: 0.9881 - val_loss: 0.3035 - val_acc: 0.9173\n",
      "Epoch 238/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0425 - acc: 0.9891 - val_loss: 0.3105 - val_acc: 0.9163\n",
      "Epoch 239/300\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0397 - acc: 0.9887 - val_loss: 0.3091 - val_acc: 0.9146\n",
      "Epoch 240/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0393 - acc: 0.9904 - val_loss: 0.3024 - val_acc: 0.9193\n",
      "Epoch 241/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0403 - acc: 0.9884 - val_loss: 0.3090 - val_acc: 0.9140\n",
      "Epoch 242/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0431 - acc: 0.9876 - val_loss: 0.3051 - val_acc: 0.9153\n",
      "Epoch 243/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0411 - acc: 0.9890 - val_loss: 0.3052 - val_acc: 0.9159\n",
      "Epoch 244/300\n",
      "7423/7423 [==============================] - 1s 198us/step - loss: 0.0389 - acc: 0.9902 - val_loss: 0.2974 - val_acc: 0.9183\n",
      "Epoch 245/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0442 - acc: 0.9869 - val_loss: 0.3084 - val_acc: 0.9136\n",
      "Epoch 246/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0415 - acc: 0.9887 - val_loss: 0.3103 - val_acc: 0.9156\n",
      "Epoch 247/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0382 - acc: 0.9890 - val_loss: 0.3040 - val_acc: 0.9156\n",
      "Epoch 248/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0388 - acc: 0.9894 - val_loss: 0.3066 - val_acc: 0.9146\n",
      "Epoch 249/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0395 - acc: 0.9891 - val_loss: 0.3080 - val_acc: 0.9156\n",
      "Epoch 250/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0412 - acc: 0.9883 - val_loss: 0.3114 - val_acc: 0.9133\n",
      "Epoch 251/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0435 - acc: 0.9892 - val_loss: 0.3052 - val_acc: 0.9173\n",
      "Epoch 252/300\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0391 - acc: 0.9906 - val_loss: 0.3042 - val_acc: 0.9153\n",
      "Epoch 253/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0428 - acc: 0.9887 - val_loss: 0.3108 - val_acc: 0.9150\n",
      "Epoch 254/300\n",
      "7423/7423 [==============================] - 1s 202us/step - loss: 0.0414 - acc: 0.9884 - val_loss: 0.3090 - val_acc: 0.9123\n",
      "Epoch 255/300\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0375 - acc: 0.9898 - val_loss: 0.3041 - val_acc: 0.9183\n",
      "Epoch 256/300\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0379 - acc: 0.9892 - val_loss: 0.3140 - val_acc: 0.9146\n",
      "Epoch 257/300\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0399 - acc: 0.9891 - val_loss: 0.3139 - val_acc: 0.9150\n",
      "Epoch 258/300\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0421 - acc: 0.9891 - val_loss: 0.3073 - val_acc: 0.9140\n",
      "Epoch 259/300\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0409 - acc: 0.9879 - val_loss: 0.3081 - val_acc: 0.9143\n",
      "Epoch 260/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0381 - acc: 0.9898 - val_loss: 0.3163 - val_acc: 0.9140\n",
      "Epoch 261/300\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0386 - acc: 0.9903 - val_loss: 0.3071 - val_acc: 0.9186\n",
      "Epoch 262/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0386 - acc: 0.9903 - val_loss: 0.2986 - val_acc: 0.9176\n",
      "Epoch 263/300\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0386 - acc: 0.9896 - val_loss: 0.3124 - val_acc: 0.9143\n",
      "Epoch 264/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0359 - acc: 0.9910 - val_loss: 0.3032 - val_acc: 0.9143\n",
      "Epoch 265/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0403 - acc: 0.9885 - val_loss: 0.3097 - val_acc: 0.9130\n",
      "Epoch 266/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0396 - acc: 0.9906 - val_loss: 0.3025 - val_acc: 0.9183\n",
      "Epoch 267/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0390 - acc: 0.9883 - val_loss: 0.3095 - val_acc: 0.9150\n",
      "Epoch 268/300\n",
      "7423/7423 [==============================] - 2s 210us/step - loss: 0.0371 - acc: 0.9907 - val_loss: 0.3038 - val_acc: 0.9130\n",
      "Epoch 269/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0386 - acc: 0.9894 - val_loss: 0.3096 - val_acc: 0.9146\n",
      "Epoch 270/300\n",
      "7423/7423 [==============================] - 1s 202us/step - loss: 0.0396 - acc: 0.9876 - val_loss: 0.3044 - val_acc: 0.9136\n",
      "Epoch 271/300\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0381 - acc: 0.9887 - val_loss: 0.3046 - val_acc: 0.9153\n",
      "Epoch 272/300\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0419 - acc: 0.9876 - val_loss: 0.3066 - val_acc: 0.9113\n",
      "Epoch 273/300\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0383 - acc: 0.9899 - val_loss: 0.3094 - val_acc: 0.9140\n",
      "Epoch 274/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0339 - acc: 0.9926 - val_loss: 0.3025 - val_acc: 0.9156\n",
      "Epoch 275/300\n",
      "7423/7423 [==============================] - 1s 199us/step - loss: 0.0400 - acc: 0.9881 - val_loss: 0.3068 - val_acc: 0.9140\n",
      "Epoch 276/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0364 - acc: 0.9910 - val_loss: 0.2989 - val_acc: 0.9169\n",
      "Epoch 277/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0414 - acc: 0.9879 - val_loss: 0.3031 - val_acc: 0.9146\n",
      "Epoch 278/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0350 - acc: 0.9911 - val_loss: 0.3039 - val_acc: 0.9159\n",
      "Epoch 279/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0365 - acc: 0.9895 - val_loss: 0.2986 - val_acc: 0.9199\n",
      "Epoch 280/300\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0384 - acc: 0.9896 - val_loss: 0.3130 - val_acc: 0.9153\n",
      "Epoch 281/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0371 - acc: 0.9888 - val_loss: 0.3052 - val_acc: 0.9150\n",
      "Epoch 282/300\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0410 - acc: 0.9877 - val_loss: 0.3109 - val_acc: 0.9133\n",
      "Epoch 283/300\n",
      "7423/7423 [==============================] - 1s 198us/step - loss: 0.0431 - acc: 0.9880 - val_loss: 0.3087 - val_acc: 0.9130\n",
      "Epoch 284/300\n",
      "7423/7423 [==============================] - 1s 198us/step - loss: 0.0411 - acc: 0.9896 - val_loss: 0.3109 - val_acc: 0.9146\n",
      "Epoch 285/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0358 - acc: 0.9910 - val_loss: 0.3088 - val_acc: 0.9150\n",
      "Epoch 286/300\n",
      "7423/7423 [==============================] - 1s 200us/step - loss: 0.0367 - acc: 0.9899 - val_loss: 0.3132 - val_acc: 0.9136\n",
      "Epoch 287/300\n",
      "7423/7423 [==============================] - 2s 202us/step - loss: 0.0375 - acc: 0.9894 - val_loss: 0.3097 - val_acc: 0.9123\n",
      "Epoch 288/300\n",
      "7423/7423 [==============================] - 1s 201us/step - loss: 0.0384 - acc: 0.9883 - val_loss: 0.3127 - val_acc: 0.9146\n",
      "Epoch 289/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0383 - acc: 0.9890 - val_loss: 0.3083 - val_acc: 0.9159\n",
      "Epoch 290/300\n",
      "7423/7423 [==============================] - 2s 202us/step - loss: 0.0376 - acc: 0.9894 - val_loss: 0.3055 - val_acc: 0.9140\n",
      "Epoch 291/300\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0356 - acc: 0.9903 - val_loss: 0.3072 - val_acc: 0.9136\n",
      "Epoch 292/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0400 - acc: 0.9883 - val_loss: 0.3073 - val_acc: 0.9146\n",
      "Epoch 293/300\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0378 - acc: 0.9892 - val_loss: 0.3066 - val_acc: 0.9159\n",
      "Epoch 294/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0395 - acc: 0.9891 - val_loss: 0.3152 - val_acc: 0.9133\n",
      "Epoch 295/300\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0364 - acc: 0.9910 - val_loss: 0.3085 - val_acc: 0.9143\n",
      "Epoch 296/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0379 - acc: 0.9903 - val_loss: 0.3043 - val_acc: 0.9196\n",
      "Epoch 297/300\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0404 - acc: 0.9891 - val_loss: 0.3115 - val_acc: 0.9179\n",
      "Epoch 298/300\n",
      "7423/7423 [==============================] - 2s 205us/step - loss: 0.0388 - acc: 0.9896 - val_loss: 0.3091 - val_acc: 0.9179\n",
      "Epoch 299/300\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0360 - acc: 0.9899 - val_loss: 0.3088 - val_acc: 0.9179\n",
      "Epoch 300/300\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0359 - acc: 0.9892 - val_loss: 0.3173 - val_acc: 0.9140\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "lr = 1e-5\n",
    "AR_double.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = AR_double.fit([X_0,X_1,X_2,X_3],Y,\n",
    "        batch_size=2048,\n",
    "        epochs=300,\n",
    "        verbose=True,\n",
    "        shuffle=True,\n",
    "        callbacks=[lrScheduler],\n",
    "        validation_data=([X_TEST_0,X_TEST_1,X_TEST_2,X_TEST_3],Y_TEST)      \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AR_double.save_weights('weights/weight_xo_1D_double.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('xo_double.json', 'w') as f:\n",
    "    json.dump(str(history.history), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_frame_double(p_0,p_1,C):\n",
    "    full_l = p_0.shape[0] # full length\n",
    "    if random.uniform(0,1)<0.5: # aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.9,1)*full_l)\n",
    "        s = random.randint(0, full_l-int(valid_l))\n",
    "        e = s+valid_l # sample end point\n",
    "        p_0 = p_0[int(s):int(e),:,:]\n",
    "        p_1 = p_1[int(s):int(e),:,:]     \n",
    "    else: # without aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.9,1)*full_l)\n",
    "        index = np.sort(np.random.choice(range(0,full_l),int(valid_l),replace=False))\n",
    "        p_0 = p_0[index,:,:]\n",
    "        p_1 = p_1[index,:,:]\n",
    "    p_0 = zoom(p_0,C.frame_l,C.joint_n,C.joint_d)\n",
    "    p_1 = zoom(p_1,C.frame_l,C.joint_n,C.joint_d)\n",
    "    return p_0,p_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "AR_double.compile(loss=\"categorical_crossentropy\",optimizer=Adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 10s 1ms/step - loss: 0.0437 - acc: 0.9867 - val_loss: 0.3356 - val_acc: 0.9150\n",
      "epoch1\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 230us/step - loss: 0.0460 - acc: 0.9853 - val_loss: 0.3299 - val_acc: 0.9169\n",
      "epoch2\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0476 - acc: 0.9865 - val_loss: 0.3218 - val_acc: 0.9176\n",
      "epoch3\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0464 - acc: 0.9853 - val_loss: 0.3266 - val_acc: 0.9169\n",
      "epoch4\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0496 - acc: 0.9846 - val_loss: 0.3332 - val_acc: 0.9136\n",
      "epoch5\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0486 - acc: 0.9872 - val_loss: 0.3229 - val_acc: 0.9183\n",
      "epoch6\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 232us/step - loss: 0.0457 - acc: 0.9852 - val_loss: 0.3189 - val_acc: 0.9166\n",
      "epoch7\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0420 - acc: 0.9872 - val_loss: 0.3234 - val_acc: 0.9163\n",
      "epoch8\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0461 - acc: 0.9845 - val_loss: 0.3102 - val_acc: 0.9179\n",
      "epoch9\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 228us/step - loss: 0.0524 - acc: 0.9837 - val_loss: 0.3169 - val_acc: 0.9176\n",
      "epoch10\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 224us/step - loss: 0.0469 - acc: 0.9855 - val_loss: 0.3214 - val_acc: 0.9196\n",
      "epoch11\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 233us/step - loss: 0.0495 - acc: 0.9840 - val_loss: 0.3204 - val_acc: 0.9199\n",
      "epoch12\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0420 - acc: 0.9884 - val_loss: 0.3235 - val_acc: 0.9209\n",
      "epoch13\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 231us/step - loss: 0.0422 - acc: 0.9852 - val_loss: 0.3266 - val_acc: 0.9189\n",
      "epoch14\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 228us/step - loss: 0.0493 - acc: 0.9855 - val_loss: 0.3365 - val_acc: 0.9186\n",
      "epoch15\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 228us/step - loss: 0.0409 - acc: 0.9879 - val_loss: 0.3340 - val_acc: 0.9179\n",
      "epoch16\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0453 - acc: 0.9859 - val_loss: 0.3365 - val_acc: 0.9163\n",
      "epoch17\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0401 - acc: 0.9879 - val_loss: 0.3422 - val_acc: 0.9173\n",
      "epoch18\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0413 - acc: 0.9879 - val_loss: 0.3287 - val_acc: 0.9156\n",
      "epoch19\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0458 - acc: 0.9859 - val_loss: 0.3231 - val_acc: 0.9159\n",
      "epoch20\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 229us/step - loss: 0.0476 - acc: 0.9857 - val_loss: 0.3253 - val_acc: 0.9189\n",
      "epoch21\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0465 - acc: 0.9860 - val_loss: 0.3165 - val_acc: 0.9189\n",
      "epoch22\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0464 - acc: 0.9861 - val_loss: 0.3288 - val_acc: 0.9163\n",
      "epoch23\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0431 - acc: 0.9867 - val_loss: 0.3261 - val_acc: 0.9159\n",
      "epoch24\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0425 - acc: 0.9871 - val_loss: 0.3305 - val_acc: 0.9176\n",
      "epoch25\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0440 - acc: 0.9861 - val_loss: 0.3315 - val_acc: 0.9150\n",
      "epoch26\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0428 - acc: 0.9876 - val_loss: 0.3208 - val_acc: 0.9156\n",
      "epoch27\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0437 - acc: 0.9852 - val_loss: 0.3130 - val_acc: 0.9199\n",
      "epoch28\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0371 - acc: 0.9883 - val_loss: 0.3179 - val_acc: 0.9186\n",
      "epoch29\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 227us/step - loss: 0.0385 - acc: 0.9887 - val_loss: 0.3148 - val_acc: 0.9169\n",
      "epoch30\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 231us/step - loss: 0.0444 - acc: 0.9875 - val_loss: 0.3138 - val_acc: 0.9179\n",
      "epoch31\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0414 - acc: 0.9879 - val_loss: 0.3180 - val_acc: 0.9183\n",
      "epoch32\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0411 - acc: 0.9881 - val_loss: 0.3159 - val_acc: 0.9146\n",
      "epoch33\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 228us/step - loss: 0.0383 - acc: 0.9884 - val_loss: 0.3194 - val_acc: 0.9150\n",
      "epoch34\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0392 - acc: 0.9892 - val_loss: 0.3136 - val_acc: 0.9169\n",
      "epoch35\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 232us/step - loss: 0.0366 - acc: 0.9895 - val_loss: 0.3217 - val_acc: 0.9130\n",
      "epoch36\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0433 - acc: 0.9871 - val_loss: 0.3252 - val_acc: 0.9133\n",
      "epoch37\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0409 - acc: 0.9884 - val_loss: 0.3168 - val_acc: 0.9169\n",
      "epoch38\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0441 - acc: 0.9856 - val_loss: 0.3276 - val_acc: 0.9166\n",
      "epoch39\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 230us/step - loss: 0.0376 - acc: 0.9873 - val_loss: 0.3273 - val_acc: 0.9156\n",
      "epoch40\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 240us/step - loss: 0.0389 - acc: 0.9879 - val_loss: 0.3249 - val_acc: 0.9179\n",
      "epoch41\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 251us/step - loss: 0.0391 - acc: 0.9873 - val_loss: 0.3202 - val_acc: 0.9186\n",
      "epoch42\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 252us/step - loss: 0.0420 - acc: 0.9873 - val_loss: 0.3180 - val_acc: 0.9169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch43\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 257us/step - loss: 0.0410 - acc: 0.9872 - val_loss: 0.3176 - val_acc: 0.9183\n",
      "epoch44\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 240us/step - loss: 0.0447 - acc: 0.9861 - val_loss: 0.3140 - val_acc: 0.9193\n",
      "epoch45\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 240us/step - loss: 0.0399 - acc: 0.9871 - val_loss: 0.3110 - val_acc: 0.9199\n",
      "epoch46\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 241us/step - loss: 0.0409 - acc: 0.9869 - val_loss: 0.3110 - val_acc: 0.9203\n",
      "epoch47\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 251us/step - loss: 0.0356 - acc: 0.9891 - val_loss: 0.3024 - val_acc: 0.9169\n",
      "epoch48\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 233us/step - loss: 0.0385 - acc: 0.9892 - val_loss: 0.3030 - val_acc: 0.9229\n",
      "epoch49\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 227us/step - loss: 0.0373 - acc: 0.9898 - val_loss: 0.3095 - val_acc: 0.9189\n",
      "epoch50\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0430 - acc: 0.9868 - val_loss: 0.3149 - val_acc: 0.9186\n",
      "epoch51\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0389 - acc: 0.9875 - val_loss: 0.3210 - val_acc: 0.9193\n",
      "epoch52\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0435 - acc: 0.9880 - val_loss: 0.3230 - val_acc: 0.9193\n",
      "epoch53\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0406 - acc: 0.9877 - val_loss: 0.3161 - val_acc: 0.9199\n",
      "epoch54\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0444 - acc: 0.9863 - val_loss: 0.3148 - val_acc: 0.9189\n",
      "epoch55\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0411 - acc: 0.9876 - val_loss: 0.3190 - val_acc: 0.9212\n",
      "epoch56\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0382 - acc: 0.9881 - val_loss: 0.3263 - val_acc: 0.9183\n",
      "epoch57\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0434 - acc: 0.9868 - val_loss: 0.3140 - val_acc: 0.9186\n",
      "epoch58\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0421 - acc: 0.9875 - val_loss: 0.3155 - val_acc: 0.9206\n",
      "epoch59\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 203us/step - loss: 0.0416 - acc: 0.9871 - val_loss: 0.3141 - val_acc: 0.9153\n",
      "epoch60\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0402 - acc: 0.9873 - val_loss: 0.3200 - val_acc: 0.9196\n",
      "epoch61\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 226us/step - loss: 0.0377 - acc: 0.9880 - val_loss: 0.3191 - val_acc: 0.9196\n",
      "epoch62\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0372 - acc: 0.9885 - val_loss: 0.3263 - val_acc: 0.9179\n",
      "epoch63\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0381 - acc: 0.9883 - val_loss: 0.3214 - val_acc: 0.9212\n",
      "epoch64\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 224us/step - loss: 0.0406 - acc: 0.9872 - val_loss: 0.3270 - val_acc: 0.9203\n",
      "epoch65\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 232us/step - loss: 0.0372 - acc: 0.9880 - val_loss: 0.3189 - val_acc: 0.9189\n",
      "epoch66\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 228us/step - loss: 0.0435 - acc: 0.9877 - val_loss: 0.3064 - val_acc: 0.9212\n",
      "epoch67\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0362 - acc: 0.9888 - val_loss: 0.3049 - val_acc: 0.9189\n",
      "epoch68\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 223us/step - loss: 0.0373 - acc: 0.9890 - val_loss: 0.3146 - val_acc: 0.9193\n",
      "epoch69\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0420 - acc: 0.9859 - val_loss: 0.3147 - val_acc: 0.9199\n",
      "epoch70\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0444 - acc: 0.9872 - val_loss: 0.3082 - val_acc: 0.9169\n",
      "epoch71\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0401 - acc: 0.9888 - val_loss: 0.3193 - val_acc: 0.9156\n",
      "epoch72\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 230us/step - loss: 0.0399 - acc: 0.9879 - val_loss: 0.3160 - val_acc: 0.9199\n",
      "epoch73\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0440 - acc: 0.9855 - val_loss: 0.3217 - val_acc: 0.9150\n",
      "epoch74\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0379 - acc: 0.9896 - val_loss: 0.3195 - val_acc: 0.9183\n",
      "epoch75\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0363 - acc: 0.9876 - val_loss: 0.3250 - val_acc: 0.9159\n",
      "epoch76\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0361 - acc: 0.9899 - val_loss: 0.3320 - val_acc: 0.9193\n",
      "epoch77\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0401 - acc: 0.9872 - val_loss: 0.3305 - val_acc: 0.9183\n",
      "epoch78\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0418 - acc: 0.9876 - val_loss: 0.3284 - val_acc: 0.9193\n",
      "epoch79\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0376 - acc: 0.9883 - val_loss: 0.3284 - val_acc: 0.9193\n",
      "epoch80\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0382 - acc: 0.9884 - val_loss: 0.3310 - val_acc: 0.9189\n",
      "epoch81\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0410 - acc: 0.9885 - val_loss: 0.3349 - val_acc: 0.9166\n",
      "epoch82\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0342 - acc: 0.9904 - val_loss: 0.3248 - val_acc: 0.9196\n",
      "epoch83\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0374 - acc: 0.9887 - val_loss: 0.3321 - val_acc: 0.9156\n",
      "epoch84\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 224us/step - loss: 0.0382 - acc: 0.9884 - val_loss: 0.3411 - val_acc: 0.9179\n",
      "epoch85\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0377 - acc: 0.9876 - val_loss: 0.3283 - val_acc: 0.9176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch86\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0376 - acc: 0.9875 - val_loss: 0.3227 - val_acc: 0.9179\n",
      "epoch87\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 227us/step - loss: 0.0381 - acc: 0.9884 - val_loss: 0.3212 - val_acc: 0.9199\n",
      "epoch88\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 223us/step - loss: 0.0388 - acc: 0.9895 - val_loss: 0.3275 - val_acc: 0.9173\n",
      "epoch89\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 232us/step - loss: 0.0397 - acc: 0.9880 - val_loss: 0.3225 - val_acc: 0.9193\n",
      "epoch90\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0366 - acc: 0.9887 - val_loss: 0.3370 - val_acc: 0.9176\n",
      "epoch91\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0420 - acc: 0.9876 - val_loss: 0.3334 - val_acc: 0.9179\n",
      "epoch92\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 223us/step - loss: 0.0334 - acc: 0.9904 - val_loss: 0.3360 - val_acc: 0.9176\n",
      "epoch93\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0386 - acc: 0.9888 - val_loss: 0.3323 - val_acc: 0.9176\n",
      "epoch94\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0333 - acc: 0.9903 - val_loss: 0.3369 - val_acc: 0.9169\n",
      "epoch95\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0355 - acc: 0.9892 - val_loss: 0.3477 - val_acc: 0.9183\n",
      "epoch96\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 229us/step - loss: 0.0386 - acc: 0.9888 - val_loss: 0.3467 - val_acc: 0.9166\n",
      "epoch97\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 224us/step - loss: 0.0383 - acc: 0.9888 - val_loss: 0.3526 - val_acc: 0.9176\n",
      "epoch98\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0336 - acc: 0.9890 - val_loss: 0.3664 - val_acc: 0.9163\n",
      "epoch99\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0382 - acc: 0.9896 - val_loss: 0.3447 - val_acc: 0.9159\n",
      "epoch100\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0373 - acc: 0.9867 - val_loss: 0.3498 - val_acc: 0.9166\n",
      "epoch101\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0391 - acc: 0.9881 - val_loss: 0.3330 - val_acc: 0.9183\n",
      "epoch102\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 227us/step - loss: 0.0363 - acc: 0.9895 - val_loss: 0.3336 - val_acc: 0.9159\n",
      "epoch103\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0295 - acc: 0.9904 - val_loss: 0.3277 - val_acc: 0.9179\n",
      "epoch104\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0335 - acc: 0.9902 - val_loss: 0.3426 - val_acc: 0.9169\n",
      "epoch105\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0295 - acc: 0.9912 - val_loss: 0.3356 - val_acc: 0.9206\n",
      "epoch106\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0388 - acc: 0.9880 - val_loss: 0.3440 - val_acc: 0.9173\n",
      "epoch107\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0363 - acc: 0.9873 - val_loss: 0.3508 - val_acc: 0.9199\n",
      "epoch108\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 223us/step - loss: 0.0341 - acc: 0.9911 - val_loss: 0.3436 - val_acc: 0.9193\n",
      "epoch109\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0312 - acc: 0.9904 - val_loss: 0.3327 - val_acc: 0.9196\n",
      "epoch110\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 216us/step - loss: 0.0348 - acc: 0.9898 - val_loss: 0.3381 - val_acc: 0.9206\n",
      "epoch111\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0382 - acc: 0.9899 - val_loss: 0.3356 - val_acc: 0.9166\n",
      "epoch112\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0305 - acc: 0.9911 - val_loss: 0.3256 - val_acc: 0.9196\n",
      "epoch113\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 213us/step - loss: 0.0324 - acc: 0.9899 - val_loss: 0.3178 - val_acc: 0.9196\n",
      "epoch114\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0351 - acc: 0.9883 - val_loss: 0.3402 - val_acc: 0.9206\n",
      "epoch115\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0368 - acc: 0.9891 - val_loss: 0.3375 - val_acc: 0.9189\n",
      "epoch116\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0326 - acc: 0.9890 - val_loss: 0.3379 - val_acc: 0.9189\n",
      "epoch117\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0357 - acc: 0.9891 - val_loss: 0.3517 - val_acc: 0.9183\n",
      "epoch118\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 211us/step - loss: 0.0345 - acc: 0.9892 - val_loss: 0.3430 - val_acc: 0.9179\n",
      "epoch119\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0364 - acc: 0.9892 - val_loss: 0.3326 - val_acc: 0.9156\n",
      "epoch120\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 227us/step - loss: 0.0418 - acc: 0.9876 - val_loss: 0.3501 - val_acc: 0.9163\n",
      "epoch121\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0321 - acc: 0.9898 - val_loss: 0.3347 - val_acc: 0.9159\n",
      "epoch122\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 206us/step - loss: 0.0332 - acc: 0.9895 - val_loss: 0.3282 - val_acc: 0.9183\n",
      "epoch123\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 223us/step - loss: 0.0362 - acc: 0.9906 - val_loss: 0.3358 - val_acc: 0.9176\n",
      "epoch124\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0310 - acc: 0.9902 - val_loss: 0.3551 - val_acc: 0.9159\n",
      "epoch125\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 223us/step - loss: 0.0372 - acc: 0.9877 - val_loss: 0.3317 - val_acc: 0.9169\n",
      "epoch126\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0347 - acc: 0.9894 - val_loss: 0.3335 - val_acc: 0.9169\n",
      "epoch127\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0333 - acc: 0.9902 - val_loss: 0.3294 - val_acc: 0.9189\n",
      "epoch128\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0311 - acc: 0.9900 - val_loss: 0.3343 - val_acc: 0.9163\n",
      "epoch129\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 209us/step - loss: 0.0402 - acc: 0.9868 - val_loss: 0.3364 - val_acc: 0.9173\n",
      "epoch130\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 214us/step - loss: 0.0312 - acc: 0.9912 - val_loss: 0.3413 - val_acc: 0.9163\n",
      "epoch131\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0374 - acc: 0.9872 - val_loss: 0.3360 - val_acc: 0.9173\n",
      "epoch132\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0315 - acc: 0.9898 - val_loss: 0.3183 - val_acc: 0.9183\n",
      "epoch133\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 212us/step - loss: 0.0322 - acc: 0.9904 - val_loss: 0.3332 - val_acc: 0.9176\n",
      "epoch134\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 221us/step - loss: 0.0304 - acc: 0.9906 - val_loss: 0.3384 - val_acc: 0.9193\n",
      "epoch135\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 215us/step - loss: 0.0342 - acc: 0.9891 - val_loss: 0.3242 - val_acc: 0.9193\n",
      "epoch136\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0285 - acc: 0.9919 - val_loss: 0.3221 - val_acc: 0.9196\n",
      "epoch137\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0324 - acc: 0.9903 - val_loss: 0.3319 - val_acc: 0.9173\n",
      "epoch138\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 222us/step - loss: 0.0331 - acc: 0.9907 - val_loss: 0.3317 - val_acc: 0.9166\n",
      "epoch139\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 229us/step - loss: 0.0307 - acc: 0.9911 - val_loss: 0.3292 - val_acc: 0.9199\n",
      "epoch140\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0360 - acc: 0.9883 - val_loss: 0.3315 - val_acc: 0.9203\n",
      "epoch141\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 204us/step - loss: 0.0345 - acc: 0.9883 - val_loss: 0.3266 - val_acc: 0.9186\n",
      "epoch142\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 218us/step - loss: 0.0253 - acc: 0.9927 - val_loss: 0.3395 - val_acc: 0.9169\n",
      "epoch143\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 208us/step - loss: 0.0330 - acc: 0.9888 - val_loss: 0.3460 - val_acc: 0.9150\n",
      "epoch144\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 219us/step - loss: 0.0331 - acc: 0.9900 - val_loss: 0.3422 - val_acc: 0.9159\n",
      "epoch145\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 229us/step - loss: 0.0337 - acc: 0.9890 - val_loss: 0.3285 - val_acc: 0.9179\n",
      "epoch146\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 207us/step - loss: 0.0330 - acc: 0.9907 - val_loss: 0.3368 - val_acc: 0.9176\n",
      "epoch147\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 220us/step - loss: 0.0322 - acc: 0.9907 - val_loss: 0.3370 - val_acc: 0.9199\n",
      "epoch148\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 217us/step - loss: 0.0347 - acc: 0.9894 - val_loss: 0.3271 - val_acc: 0.9203\n",
      "epoch149\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 2s 225us/step - loss: 0.0334 - acc: 0.9899 - val_loss: 0.3180 - val_acc: 0.9236\n",
      "epoch150\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 5s 732us/step - loss: 0.0310 - acc: 0.9911 - val_loss: 0.3213 - val_acc: 0.9222\n",
      "epoch151\n",
      "Train on 7423 samples, validate on 3022 samples\n",
      "Epoch 1/1\n",
      "7423/7423 [==============================] - 4s 526us/step - loss: 0.0343 - acc: 0.9912 - val_loss: 0.3261 - val_acc: 0.9206\n",
      "epoch152\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    X_2 = []\n",
    "    X_3 = []\n",
    "    Y = []\n",
    "\n",
    "    #for i in tqdm(range(len(xobj_train['label']))): \n",
    "    for i in range(len(xobj_train['label'])):\n",
    "        p_0 = xobj_train['poses'][i][0][:,C.joint_ind,:]\n",
    "        p_1 = xobj_train['poses'][i][1][:,C.joint_ind,:]\n",
    "        \n",
    "        if np.all(p_0==0) or np.all(p_1==0): \n",
    "            continue\n",
    "\n",
    "        p_0,p_1 = sampling_frame_double(p_0,p_1,C)\n",
    "           \n",
    "        try:\n",
    "            label = np.zeros(11)\n",
    "            label[xobj_train['label'][i]-50] = 1   \n",
    "        except:\n",
    "            continue\n",
    "            #print(xobj_train['name'][i])\n",
    "\n",
    "        M_0,M_1 = get_CG_double(p_0,p_1,C)\n",
    "                \n",
    "        #rotation\n",
    "        x_angle = np.random.uniform(-0.02,0.02)\n",
    "        y_angle = np.random.uniform(-np.pi/3,np.pi/3)\n",
    "        z_angle = np.random.uniform(-0.02,0.02)\n",
    "        R = euler2mat(x_angle, y_angle, z_angle, 'sxyz')\n",
    "        p_0,p_1 = rotaion_two(p_0,p_1,R)\n",
    "        \n",
    "        X_0.append(M_0)\n",
    "        X_1.append(M_1)\n",
    "        X_2.append(p_0)\n",
    "        X_3.append(p_1)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    X_2 = np.stack(X_2)  \n",
    "    X_3 = np.stack(X_3) \n",
    "    Y = np.stack(Y)\n",
    "\n",
    "    AR_double.fit([X_0,X_1,X_2,X_3],Y,\n",
    "            batch_size=2048,\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_TEST_0,X_TEST_1,X_TEST_2,X_TEST_3],Y_TEST)\n",
    "            )\n",
    "\n",
    "    del X_0\n",
    "    del X_1\n",
    "    del X_2\n",
    "    del X_3\n",
    "    del Y\n",
    "    gc.collect()\n",
    "\n",
    "    if e%10==0:\n",
    "        AR_double.save_weights('weights/weight_xo_1D_double_aug.h5')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AR_double.save_weights('weights/weight_xo_1D_double_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross view validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xobj_val = pd.read_pickle(data_path+xlist[3],compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16487 [00:00<?, ?it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 16487/16487 [00:27<00:00, 625.48it/s]\n"
     ]
    }
   ],
   "source": [
    "X_TEST_0 = []\n",
    "X_TEST_1 = []\n",
    "X_TEST_2 = []\n",
    "X_TEST_3 = []\n",
    "Y_TEST = []\n",
    "num_xobj_val = len(xobj_val['label'])\n",
    "for i in tqdm(range(num_xobj_val)): \n",
    "    p_0 = xobj_val['poses'][i][0][:,C.joint_ind,:]\n",
    "    p_1 = xobj_val['poses'][i][1][:,C.joint_ind,:]\n",
    "\n",
    "    if np.all(p_0==0) or np.all(p_1==0): \n",
    "        continue\n",
    "           \n",
    "    p_0 = zoom(p_0,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p_1 = zoom(p_1,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    \n",
    "    try:\n",
    "        label = np.zeros(11)\n",
    "        label[xobj_val['label'][i]-50] = 1   \n",
    "    except:\n",
    "        continue\n",
    "        #print(xobj_val['name'][i])\n",
    "\n",
    "    M_0,M_1 = get_CG_double(p_0,p_1,C)\n",
    "\n",
    "    X_TEST_0.append(M_0)\n",
    "    X_TEST_1.append(M_1)\n",
    "    X_TEST_2.append(p_0)\n",
    "    X_TEST_3.append(p_1)\n",
    "    Y_TEST.append(label)\n",
    "\n",
    "X_TEST_0 = np.stack(X_TEST_0)  \n",
    "X_TEST_1 = np.stack(X_TEST_1) \n",
    "X_TEST_2 = np.stack(X_TEST_2)  \n",
    "X_TEST_3 = np.stack(X_TEST_3) \n",
    "Y_TEST = np.stack(Y_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9192587690271343\n"
     ]
    }
   ],
   "source": [
    "Y_pred = AR_double.predict([X_TEST_0,X_TEST_1,X_TEST_2,X_TEST_3])\n",
    "acc = 1-np.count_nonzero(np.argmax(Y_pred,axis=1)-np.argmax(Y_TEST,axis=1))/Y_TEST.shape[0]\n",
    "print('Accuracy:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3022"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.68610165708866"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3022*92.3+14901*82)/(14901+3022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9301786896095301"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9334877564526803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAAD8CAYAAABn/so+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAUUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f358007ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Y_TEST[200:240,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAAD8CAYAAABn/so+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nASHBASHBASHBASHBASHBASHBAf8HWBRjwVwnagEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3468d036a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Y_pred[200:240,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):  \n",
    "    p_0 = xobj_val['poses'][i][0]\n",
    "    p_1 = xobj_val['poses'][i][1]\n",
    "    \n",
    "    p_0 = zoom(p_0,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p_1 = zoom(p_1,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(p_1!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(p_0!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_0,M_1,M_01,M_10 = get_distance_matrix(p_0,p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_train_a(p_0,p_1,C,rate=0.8):\n",
    "    #two person with aligment\n",
    "    full_l = p_0.shape[0] # full length\n",
    "    valid_l = np.round(np.random.uniform(rate,1)*full_l) #valid lenghth for training\n",
    "    s = random.randint(0, full_l-int(valid_l))\n",
    "    e = s+valid_l # sample end point\n",
    "    p_0 = p_0[int(s):int(e),:,:]\n",
    "    p_1 = p_1[int(s):int(e),:,:]\n",
    "    p_0 = zoom(p_0,C.frame_l,C.joint_n,C.joint_d)\n",
    "    p_1 = zoom(p_1,C.frame_l,C.joint_n,C.joint_d)\n",
    "    return p_0,p_1\n",
    "\n",
    "def sampling_train_b(p,C,rate=0.9):\n",
    "    #two person without aligment\n",
    "    full_l = p.shape[0] # full length\n",
    "    valid_l = np.round(np.random.uniform(rate,1)*full_l)\n",
    "    index = np.sort(np.random.choice(range(0,full_l),int(valid_l),replace=False)) \n",
    "    p = p[index,:,:]\n",
    "    p = zoom(p,C.frame_l,C.joint_n,C.joint_d)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iu = np.triu_indices(25,0) # the index of up triangle of matrix\n",
    "f=0\n",
    "#distance max \n",
    "d_m_0 = cdist(p_0[f],p_0[f],'euclidean')\n",
    "len(d_m_0[iu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad7a567f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(d_m_0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(p_0[0])\n",
    "def poses_diff(x):\n",
    "    x = tf.convert_to_tensor(x, np.float32)\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,:1,...],x[:,:-1,...])\n",
    "    #x = ZeroPadding2D(padding=((0, 1),(0,0)))(x)\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "plt.imshow(K.eval(poses_diff(p_0))[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
