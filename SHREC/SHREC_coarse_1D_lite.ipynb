{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 12 # the number of joints\n",
    "        self.joint_n = 22 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joints\n",
    "        self.clc_coarse = 14 # the number of coarse class\n",
    "        self.clc_fine = 28 # the number of fine-grained class\n",
    "        self.feat_d = 231\n",
    "        self.filters = 16\n",
    "        self.data_dir = '/mnt/nasbi/homes/fan/projects/action/skeleton/data/SHREC/'\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,1:,...],x[:,:-1,...])\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "    \n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=22,joint_d=2,feat_d=231,filters=16):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "    \n",
    "    x = c1D(M,filters*2,1)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,3)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,1)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x_d_slow = c1D(diff_slow,filters*2,1)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,3)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,1)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "        \n",
    "    x_d_fast = c1D(diff_fast,filters*2,1)\n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,3) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,1) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "   \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    return Model(inputs=[M,P],outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_DD_Net(frame_l=32,joint_n=22,joint_d=3,feat_d=231,clc_num=14,filters=16):\n",
    "    M = Input(name='M', shape=(frame_l,feat_d))  \n",
    "    P = Input(name='P', shape=(frame_l,joint_n,joint_d)) \n",
    "    \n",
    "    FM = build_FM(frame_l,joint_n,joint_d,feat_d,filters)\n",
    "    \n",
    "    x = FM([M,P])\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M,P],outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DD_Net = build_DD_Net(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.clc_coarse,C.filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M (InputLayer)                  (None, 32, 231)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P (InputLayer)                  (None, 32, 22, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 128)       119392      M[0][0]                          \n",
      "                                                                 P[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16384       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 128)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16384       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 128)          0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 14)           1806        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 154,990\n",
      "Trainable params: 153,198\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DD_Net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train.pkl\", \"rb\"))\n",
    "Test = pickle.load(open(C.data_dir+\"test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(Train['pose']))): \n",
    "    p = np.copy(Train['pose'][i]).reshape([-1,22,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Train['coarse_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_0.append(M)\n",
    "    X_1.append(p)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  \n",
    "X_1 = np.stack(X_1) \n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_0 = []\n",
    "X_test_1 = []\n",
    "Y_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    p = np.copy(Test['pose'][i]).reshape([-1,22,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Test['coarse_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_test_0.append(M)\n",
    "    X_test_1.append(p)\n",
    "    Y_test.append(label)\n",
    "\n",
    "X_test_0 = np.stack(X_test_0) \n",
    "X_test_1 = np.stack(X_test_1)  \n",
    "Y_test = np.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/400\n",
      "1960/1960 [==============================] - 10s 5ms/step - loss: 3.1060 - acc: 0.0704 - val_loss: 2.5793 - val_acc: 0.0881\n",
      "Epoch 2/400\n",
      "1960/1960 [==============================] - 1s 342us/step - loss: 2.9880 - acc: 0.0740 - val_loss: 2.5032 - val_acc: 0.1345\n",
      "Epoch 3/400\n",
      "1960/1960 [==============================] - 1s 582us/step - loss: 2.8850 - acc: 0.0985 - val_loss: 2.4300 - val_acc: 0.1869\n",
      "Epoch 4/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 2.8036 - acc: 0.1224 - val_loss: 2.3587 - val_acc: 0.2250\n",
      "Epoch 5/400\n",
      "1960/1960 [==============================] - 1s 565us/step - loss: 2.7350 - acc: 0.1347 - val_loss: 2.2959 - val_acc: 0.2512\n",
      "Epoch 6/400\n",
      "1960/1960 [==============================] - 1s 324us/step - loss: 2.6217 - acc: 0.1495 - val_loss: 2.2379 - val_acc: 0.2667\n",
      "Epoch 7/400\n",
      "1960/1960 [==============================] - 1s 528us/step - loss: 2.5688 - acc: 0.1510 - val_loss: 2.1850 - val_acc: 0.2798\n",
      "Epoch 8/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 2.5050 - acc: 0.1816 - val_loss: 2.1355 - val_acc: 0.3012\n",
      "Epoch 9/400\n",
      "1960/1960 [==============================] - 1s 427us/step - loss: 2.4418 - acc: 0.1995 - val_loss: 2.0846 - val_acc: 0.3226\n",
      "Epoch 10/400\n",
      "1960/1960 [==============================] - 0s 219us/step - loss: 2.4043 - acc: 0.2000 - val_loss: 2.0353 - val_acc: 0.3369\n",
      "Epoch 11/400\n",
      "1960/1960 [==============================] - 1s 294us/step - loss: 2.3247 - acc: 0.2260 - val_loss: 1.9878 - val_acc: 0.3536\n",
      "Epoch 12/400\n",
      "1960/1960 [==============================] - 0s 230us/step - loss: 2.3321 - acc: 0.2276 - val_loss: 1.9437 - val_acc: 0.3679\n",
      "Epoch 13/400\n",
      "1960/1960 [==============================] - 1s 328us/step - loss: 2.2575 - acc: 0.2434 - val_loss: 1.9062 - val_acc: 0.3690\n",
      "Epoch 14/400\n",
      "1960/1960 [==============================] - 0s 225us/step - loss: 2.2102 - acc: 0.2582 - val_loss: 1.8679 - val_acc: 0.3714\n",
      "Epoch 15/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 2.1351 - acc: 0.2770 - val_loss: 1.8333 - val_acc: 0.3762\n",
      "Epoch 16/400\n",
      "1960/1960 [==============================] - 0s 237us/step - loss: 2.1035 - acc: 0.2990 - val_loss: 1.7990 - val_acc: 0.3905\n",
      "Epoch 17/400\n",
      "1960/1960 [==============================] - 1s 323us/step - loss: 2.0596 - acc: 0.3128 - val_loss: 1.7627 - val_acc: 0.4179\n",
      "Epoch 18/400\n",
      "1960/1960 [==============================] - 0s 217us/step - loss: 1.9940 - acc: 0.3362 - val_loss: 1.7270 - val_acc: 0.4286\n",
      "Epoch 19/400\n",
      "1960/1960 [==============================] - 1s 293us/step - loss: 1.9501 - acc: 0.3679 - val_loss: 1.6939 - val_acc: 0.4345\n",
      "Epoch 20/400\n",
      "1960/1960 [==============================] - 0s 194us/step - loss: 1.9292 - acc: 0.3500 - val_loss: 1.6664 - val_acc: 0.4452\n",
      "Epoch 21/400\n",
      "1960/1960 [==============================] - 1s 314us/step - loss: 1.8312 - acc: 0.4015 - val_loss: 1.6416 - val_acc: 0.4488\n",
      "Epoch 22/400\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 1.8117 - acc: 0.4056 - val_loss: 1.6132 - val_acc: 0.4583\n",
      "Epoch 23/400\n",
      "1960/1960 [==============================] - 1s 317us/step - loss: 1.7569 - acc: 0.4199 - val_loss: 1.5782 - val_acc: 0.4655\n",
      "Epoch 24/400\n",
      "1960/1960 [==============================] - 0s 219us/step - loss: 1.7068 - acc: 0.4296 - val_loss: 1.5323 - val_acc: 0.4750\n",
      "Epoch 25/400\n",
      "1960/1960 [==============================] - 1s 306us/step - loss: 1.6688 - acc: 0.4658 - val_loss: 1.4909 - val_acc: 0.4893\n",
      "Epoch 26/400\n",
      "1960/1960 [==============================] - 0s 228us/step - loss: 1.6232 - acc: 0.4643 - val_loss: 1.4483 - val_acc: 0.5036\n",
      "Epoch 27/400\n",
      "1960/1960 [==============================] - 1s 294us/step - loss: 1.5957 - acc: 0.4653 - val_loss: 1.4050 - val_acc: 0.5262\n",
      "Epoch 28/400\n",
      "1960/1960 [==============================] - 0s 225us/step - loss: 1.5189 - acc: 0.5112 - val_loss: 1.3668 - val_acc: 0.5381\n",
      "Epoch 29/400\n",
      "1960/1960 [==============================] - 1s 297us/step - loss: 1.5146 - acc: 0.5020 - val_loss: 1.3317 - val_acc: 0.5476\n",
      "Epoch 30/400\n",
      "1960/1960 [==============================] - 0s 217us/step - loss: 1.5014 - acc: 0.4980 - val_loss: 1.2928 - val_acc: 0.5643\n",
      "Epoch 31/400\n",
      "1960/1960 [==============================] - 1s 318us/step - loss: 1.3890 - acc: 0.5515 - val_loss: 1.2571 - val_acc: 0.5798\n",
      "Epoch 32/400\n",
      "1960/1960 [==============================] - 0s 199us/step - loss: 1.3579 - acc: 0.5526 - val_loss: 1.2221 - val_acc: 0.6036\n",
      "Epoch 33/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 1.3347 - acc: 0.5633 - val_loss: 1.1870 - val_acc: 0.6190\n",
      "Epoch 34/400\n",
      "1960/1960 [==============================] - 0s 244us/step - loss: 1.2980 - acc: 0.5791 - val_loss: 1.1559 - val_acc: 0.6310\n",
      "Epoch 35/400\n",
      "1960/1960 [==============================] - 1s 343us/step - loss: 1.2642 - acc: 0.6005 - val_loss: 1.1277 - val_acc: 0.6464\n",
      "Epoch 36/400\n",
      "1960/1960 [==============================] - 0s 231us/step - loss: 1.2432 - acc: 0.6077 - val_loss: 1.1063 - val_acc: 0.6548\n",
      "Epoch 37/400\n",
      "1960/1960 [==============================] - 1s 322us/step - loss: 1.2044 - acc: 0.6000 - val_loss: 1.0801 - val_acc: 0.6607\n",
      "Epoch 38/400\n",
      "1960/1960 [==============================] - 0s 228us/step - loss: 1.1561 - acc: 0.6403 - val_loss: 1.0533 - val_acc: 0.6655\n",
      "Epoch 39/400\n",
      "1960/1960 [==============================] - 1s 339us/step - loss: 1.1327 - acc: 0.6240 - val_loss: 1.0274 - val_acc: 0.6714\n",
      "Epoch 40/400\n",
      "1960/1960 [==============================] - 0s 224us/step - loss: 1.1034 - acc: 0.6536 - val_loss: 0.9982 - val_acc: 0.6774\n",
      "Epoch 41/400\n",
      "1960/1960 [==============================] - 1s 348us/step - loss: 1.0658 - acc: 0.6571 - val_loss: 0.9695 - val_acc: 0.6869\n",
      "Epoch 42/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 1.0403 - acc: 0.6730 - val_loss: 0.9452 - val_acc: 0.6940\n",
      "Epoch 43/400\n",
      "1960/1960 [==============================] - 1s 286us/step - loss: 1.0224 - acc: 0.6842 - val_loss: 0.9227 - val_acc: 0.6964\n",
      "Epoch 44/400\n",
      "1960/1960 [==============================] - 0s 195us/step - loss: 0.9933 - acc: 0.6837 - val_loss: 0.8969 - val_acc: 0.7143\n",
      "Epoch 45/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 0.9764 - acc: 0.6878 - val_loss: 0.8707 - val_acc: 0.7250\n",
      "Epoch 46/400\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 0.9427 - acc: 0.7107 - val_loss: 0.8464 - val_acc: 0.7333\n",
      "Epoch 47/400\n",
      "1960/1960 [==============================] - 1s 323us/step - loss: 0.9137 - acc: 0.7107 - val_loss: 0.8253 - val_acc: 0.7405\n",
      "Epoch 48/400\n",
      "1960/1960 [==============================] - 0s 218us/step - loss: 0.8916 - acc: 0.7316 - val_loss: 0.8076 - val_acc: 0.7417\n",
      "Epoch 49/400\n",
      "1960/1960 [==============================] - 1s 313us/step - loss: 0.8695 - acc: 0.7265 - val_loss: 0.7938 - val_acc: 0.7429\n",
      "Epoch 50/400\n",
      "1960/1960 [==============================] - 0s 209us/step - loss: 0.8226 - acc: 0.7418 - val_loss: 0.7837 - val_acc: 0.7488\n",
      "Epoch 51/400\n",
      "1960/1960 [==============================] - 1s 302us/step - loss: 0.8186 - acc: 0.7383 - val_loss: 0.7792 - val_acc: 0.7488\n",
      "Epoch 52/400\n",
      "1960/1960 [==============================] - 0s 225us/step - loss: 0.7897 - acc: 0.7546 - val_loss: 0.7764 - val_acc: 0.7464\n",
      "Epoch 53/400\n",
      "1960/1960 [==============================] - 1s 287us/step - loss: 0.7726 - acc: 0.7740 - val_loss: 0.7741 - val_acc: 0.7488\n",
      "Epoch 54/400\n",
      "1960/1960 [==============================] - 0s 241us/step - loss: 0.7645 - acc: 0.7668 - val_loss: 0.7694 - val_acc: 0.7500\n",
      "Epoch 55/400\n",
      "1960/1960 [==============================] - 1s 333us/step - loss: 0.7586 - acc: 0.7699 - val_loss: 0.7604 - val_acc: 0.7536\n",
      "Epoch 56/400\n",
      "1960/1960 [==============================] - 0s 217us/step - loss: 0.7049 - acc: 0.7724 - val_loss: 0.7506 - val_acc: 0.7512\n",
      "Epoch 57/400\n",
      "1960/1960 [==============================] - 1s 320us/step - loss: 0.7185 - acc: 0.7872 - val_loss: 0.7436 - val_acc: 0.7488\n",
      "Epoch 58/400\n",
      "1960/1960 [==============================] - 0s 212us/step - loss: 0.6981 - acc: 0.7888 - val_loss: 0.7350 - val_acc: 0.7524\n",
      "Epoch 59/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.6781 - acc: 0.7990 - val_loss: 0.7314 - val_acc: 0.7548\n",
      "Epoch 60/400\n",
      "1960/1960 [==============================] - 0s 226us/step - loss: 0.6557 - acc: 0.7985 - val_loss: 0.7267 - val_acc: 0.7548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/400\n",
      "1960/1960 [==============================] - 1s 287us/step - loss: 0.6277 - acc: 0.7974 - val_loss: 0.7238 - val_acc: 0.7548\n",
      "Epoch 62/400\n",
      "1960/1960 [==============================] - 0s 220us/step - loss: 0.6337 - acc: 0.8194 - val_loss: 0.7230 - val_acc: 0.7476\n",
      "Epoch 63/400\n",
      "1960/1960 [==============================] - 1s 271us/step - loss: 0.6308 - acc: 0.8010 - val_loss: 0.7266 - val_acc: 0.7464\n",
      "Epoch 64/400\n",
      "1960/1960 [==============================] - 1s 261us/step - loss: 0.6057 - acc: 0.8184 - val_loss: 0.7330 - val_acc: 0.7440\n",
      "Epoch 65/400\n",
      "1960/1960 [==============================] - 1s 326us/step - loss: 0.5823 - acc: 0.8342 - val_loss: 0.7359 - val_acc: 0.7524\n",
      "Epoch 66/400\n",
      "1960/1960 [==============================] - 0s 255us/step - loss: 0.5690 - acc: 0.8286 - val_loss: 0.7339 - val_acc: 0.7500\n",
      "Epoch 67/400\n",
      "1960/1960 [==============================] - 1s 357us/step - loss: 0.5613 - acc: 0.8398 - val_loss: 0.7269 - val_acc: 0.7524\n",
      "Epoch 68/400\n",
      "1960/1960 [==============================] - 0s 226us/step - loss: 0.5447 - acc: 0.8301 - val_loss: 0.7172 - val_acc: 0.7571\n",
      "Epoch 69/400\n",
      "1960/1960 [==============================] - 1s 331us/step - loss: 0.5472 - acc: 0.8332 - val_loss: 0.7064 - val_acc: 0.7690\n",
      "Epoch 70/400\n",
      "1960/1960 [==============================] - 0s 243us/step - loss: 0.5318 - acc: 0.8464 - val_loss: 0.6911 - val_acc: 0.7750\n",
      "Epoch 71/400\n",
      "1960/1960 [==============================] - 1s 346us/step - loss: 0.5091 - acc: 0.8551 - val_loss: 0.6713 - val_acc: 0.7810\n",
      "Epoch 72/400\n",
      "1960/1960 [==============================] - 0s 237us/step - loss: 0.5006 - acc: 0.8597 - val_loss: 0.6533 - val_acc: 0.7929\n",
      "Epoch 73/400\n",
      "1960/1960 [==============================] - 1s 316us/step - loss: 0.4735 - acc: 0.8577 - val_loss: 0.6351 - val_acc: 0.7952\n",
      "Epoch 74/400\n",
      "1960/1960 [==============================] - 0s 228us/step - loss: 0.4953 - acc: 0.8474 - val_loss: 0.6194 - val_acc: 0.7988\n",
      "Epoch 75/400\n",
      "1960/1960 [==============================] - 1s 311us/step - loss: 0.4765 - acc: 0.8689 - val_loss: 0.6079 - val_acc: 0.8012\n",
      "Epoch 76/400\n",
      "1960/1960 [==============================] - 0s 214us/step - loss: 0.4474 - acc: 0.8689 - val_loss: 0.5937 - val_acc: 0.8024\n",
      "Epoch 77/400\n",
      "1960/1960 [==============================] - 1s 298us/step - loss: 0.4481 - acc: 0.8663 - val_loss: 0.5838 - val_acc: 0.8083\n",
      "Epoch 78/400\n",
      "1960/1960 [==============================] - 0s 224us/step - loss: 0.4416 - acc: 0.8679 - val_loss: 0.5780 - val_acc: 0.8107\n",
      "Epoch 79/400\n",
      "1960/1960 [==============================] - 1s 272us/step - loss: 0.4455 - acc: 0.8755 - val_loss: 0.5718 - val_acc: 0.8143\n",
      "Epoch 80/400\n",
      "1960/1960 [==============================] - 0s 213us/step - loss: 0.4360 - acc: 0.8724 - val_loss: 0.5627 - val_acc: 0.8143\n",
      "Epoch 81/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.4277 - acc: 0.8633 - val_loss: 0.5515 - val_acc: 0.8202\n",
      "Epoch 82/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.4325 - acc: 0.8689 - val_loss: 0.5398 - val_acc: 0.8274\n",
      "Epoch 83/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 0.4283 - acc: 0.8699 - val_loss: 0.5268 - val_acc: 0.8321\n",
      "Epoch 84/400\n",
      "1960/1960 [==============================] - 1s 271us/step - loss: 0.3745 - acc: 0.8908 - val_loss: 0.5119 - val_acc: 0.8357\n",
      "Epoch 85/400\n",
      "1960/1960 [==============================] - 1s 325us/step - loss: 0.4076 - acc: 0.8821 - val_loss: 0.4983 - val_acc: 0.8345\n",
      "Epoch 86/400\n",
      "1960/1960 [==============================] - 0s 238us/step - loss: 0.3822 - acc: 0.8934 - val_loss: 0.4882 - val_acc: 0.8417\n",
      "Epoch 87/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.3917 - acc: 0.8908 - val_loss: 0.4811 - val_acc: 0.8512\n",
      "Epoch 88/400\n",
      "1960/1960 [==============================] - 0s 236us/step - loss: 0.3663 - acc: 0.8959 - val_loss: 0.4759 - val_acc: 0.8512\n",
      "Epoch 89/400\n",
      "1960/1960 [==============================] - 1s 297us/step - loss: 0.3675 - acc: 0.8903 - val_loss: 0.4732 - val_acc: 0.8512\n",
      "Epoch 90/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.3625 - acc: 0.8949 - val_loss: 0.4704 - val_acc: 0.8488\n",
      "Epoch 91/400\n",
      "1960/1960 [==============================] - 1s 370us/step - loss: 0.3591 - acc: 0.8939 - val_loss: 0.4678 - val_acc: 0.8488\n",
      "Epoch 92/400\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 0.3404 - acc: 0.9020 - val_loss: 0.4640 - val_acc: 0.8536\n",
      "Epoch 93/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 0.3468 - acc: 0.8995 - val_loss: 0.4622 - val_acc: 0.8548\n",
      "Epoch 94/400\n",
      "1960/1960 [==============================] - 0s 230us/step - loss: 0.3291 - acc: 0.9066 - val_loss: 0.4593 - val_acc: 0.8571\n",
      "Epoch 95/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 0.3179 - acc: 0.9097 - val_loss: 0.4533 - val_acc: 0.8583\n",
      "Epoch 96/400\n",
      "1960/1960 [==============================] - 0s 255us/step - loss: 0.3047 - acc: 0.9143 - val_loss: 0.4458 - val_acc: 0.8631\n",
      "Epoch 97/400\n",
      "1960/1960 [==============================] - 1s 289us/step - loss: 0.3431 - acc: 0.9015 - val_loss: 0.4382 - val_acc: 0.8631\n",
      "Epoch 98/400\n",
      "1960/1960 [==============================] - 0s 228us/step - loss: 0.3160 - acc: 0.9128 - val_loss: 0.4307 - val_acc: 0.8667\n",
      "Epoch 99/400\n",
      "1960/1960 [==============================] - 1s 326us/step - loss: 0.3113 - acc: 0.9179 - val_loss: 0.4197 - val_acc: 0.8679\n",
      "Epoch 100/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 0.3168 - acc: 0.9112 - val_loss: 0.4097 - val_acc: 0.8690\n",
      "Epoch 101/400\n",
      "1960/1960 [==============================] - 1s 486us/step - loss: 0.2715 - acc: 0.9235 - val_loss: 0.4018 - val_acc: 0.8726\n",
      "Epoch 102/400\n",
      "1960/1960 [==============================] - 1s 369us/step - loss: 0.3050 - acc: 0.9138 - val_loss: 0.3950 - val_acc: 0.8738\n",
      "Epoch 103/400\n",
      "1960/1960 [==============================] - 1s 550us/step - loss: 0.2965 - acc: 0.9158 - val_loss: 0.3896 - val_acc: 0.8750\n",
      "Epoch 104/400\n",
      "1960/1960 [==============================] - 1s 369us/step - loss: 0.2696 - acc: 0.9235 - val_loss: 0.3865 - val_acc: 0.8738\n",
      "Epoch 105/400\n",
      "1960/1960 [==============================] - 1s 465us/step - loss: 0.2635 - acc: 0.9301 - val_loss: 0.3815 - val_acc: 0.8762\n",
      "Epoch 106/400\n",
      "1960/1960 [==============================] - 1s 389us/step - loss: 0.2736 - acc: 0.9204 - val_loss: 0.3787 - val_acc: 0.8774\n",
      "Epoch 107/400\n",
      "1960/1960 [==============================] - 1s 509us/step - loss: 0.2590 - acc: 0.9286 - val_loss: 0.3783 - val_acc: 0.8798\n",
      "Epoch 108/400\n",
      "1960/1960 [==============================] - 1s 375us/step - loss: 0.2461 - acc: 0.9270 - val_loss: 0.3780 - val_acc: 0.8786\n",
      "Epoch 109/400\n",
      "1960/1960 [==============================] - 1s 430us/step - loss: 0.2695 - acc: 0.9179 - val_loss: 0.3828 - val_acc: 0.8726\n",
      "Epoch 110/400\n",
      "1960/1960 [==============================] - 1s 344us/step - loss: 0.2502 - acc: 0.9296 - val_loss: 0.3912 - val_acc: 0.8726\n",
      "Epoch 111/400\n",
      "1960/1960 [==============================] - 1s 506us/step - loss: 0.2421 - acc: 0.9347 - val_loss: 0.3997 - val_acc: 0.8774\n",
      "Epoch 112/400\n",
      "1960/1960 [==============================] - 1s 371us/step - loss: 0.2487 - acc: 0.9214 - val_loss: 0.4028 - val_acc: 0.8786\n",
      "Epoch 113/400\n",
      "1960/1960 [==============================] - 1s 491us/step - loss: 0.2468 - acc: 0.9332 - val_loss: 0.4023 - val_acc: 0.8786\n",
      "Epoch 114/400\n",
      "1960/1960 [==============================] - 1s 372us/step - loss: 0.2431 - acc: 0.9306 - val_loss: 0.3976 - val_acc: 0.8762\n",
      "Epoch 115/400\n",
      "1960/1960 [==============================] - 1s 529us/step - loss: 0.2430 - acc: 0.9327 - val_loss: 0.3899 - val_acc: 0.8786\n",
      "Epoch 116/400\n",
      "1960/1960 [==============================] - 1s 380us/step - loss: 0.2273 - acc: 0.9357 - val_loss: 0.3839 - val_acc: 0.8798\n",
      "Epoch 117/400\n",
      "1960/1960 [==============================] - 1s 447us/step - loss: 0.2228 - acc: 0.9332 - val_loss: 0.3806 - val_acc: 0.8774\n",
      "Epoch 118/400\n",
      "1960/1960 [==============================] - 1s 388us/step - loss: 0.2378 - acc: 0.9301 - val_loss: 0.3765 - val_acc: 0.8762\n",
      "Epoch 119/400\n",
      "1960/1960 [==============================] - 1s 492us/step - loss: 0.2205 - acc: 0.9372 - val_loss: 0.3700 - val_acc: 0.8786\n",
      "Epoch 120/400\n",
      "1960/1960 [==============================] - 1s 298us/step - loss: 0.2143 - acc: 0.9429 - val_loss: 0.3601 - val_acc: 0.8833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/400\n",
      "1960/1960 [==============================] - 1s 474us/step - loss: 0.2227 - acc: 0.9372 - val_loss: 0.3498 - val_acc: 0.8893\n",
      "Epoch 122/400\n",
      "1960/1960 [==============================] - 1s 420us/step - loss: 0.2174 - acc: 0.9357 - val_loss: 0.3444 - val_acc: 0.8881\n",
      "Epoch 123/400\n",
      "1960/1960 [==============================] - 1s 473us/step - loss: 0.2124 - acc: 0.9413 - val_loss: 0.3387 - val_acc: 0.8940\n",
      "Epoch 124/400\n",
      "1960/1960 [==============================] - 1s 367us/step - loss: 0.2034 - acc: 0.9469 - val_loss: 0.3337 - val_acc: 0.8952\n",
      "Epoch 125/400\n",
      "1960/1960 [==============================] - 1s 540us/step - loss: 0.2156 - acc: 0.9372 - val_loss: 0.3322 - val_acc: 0.8940\n",
      "Epoch 126/400\n",
      "1960/1960 [==============================] - 1s 352us/step - loss: 0.2028 - acc: 0.9444 - val_loss: 0.3333 - val_acc: 0.8940\n",
      "Epoch 127/400\n",
      "1960/1960 [==============================] - 1s 529us/step - loss: 0.2207 - acc: 0.9388 - val_loss: 0.3308 - val_acc: 0.8940\n",
      "Epoch 128/400\n",
      "1960/1960 [==============================] - 1s 425us/step - loss: 0.1957 - acc: 0.9469 - val_loss: 0.3275 - val_acc: 0.8964\n",
      "Epoch 129/400\n",
      "1960/1960 [==============================] - 1s 550us/step - loss: 0.2030 - acc: 0.9469 - val_loss: 0.3231 - val_acc: 0.8976\n",
      "Epoch 130/400\n",
      "1960/1960 [==============================] - 1s 374us/step - loss: 0.1948 - acc: 0.9515 - val_loss: 0.3185 - val_acc: 0.8988\n",
      "Epoch 131/400\n",
      "1960/1960 [==============================] - 1s 514us/step - loss: 0.1939 - acc: 0.9454 - val_loss: 0.3140 - val_acc: 0.9012\n",
      "Epoch 132/400\n",
      "1960/1960 [==============================] - 1s 347us/step - loss: 0.1928 - acc: 0.9459 - val_loss: 0.3129 - val_acc: 0.9024\n",
      "Epoch 133/400\n",
      "1960/1960 [==============================] - 1s 477us/step - loss: 0.1846 - acc: 0.9495 - val_loss: 0.3109 - val_acc: 0.9024\n",
      "Epoch 134/400\n",
      "1960/1960 [==============================] - 1s 345us/step - loss: 0.1832 - acc: 0.9500 - val_loss: 0.3096 - val_acc: 0.9060\n",
      "Epoch 135/400\n",
      "1960/1960 [==============================] - 1s 382us/step - loss: 0.1808 - acc: 0.9510 - val_loss: 0.3085 - val_acc: 0.9095\n",
      "Epoch 136/400\n",
      "1960/1960 [==============================] - 1s 445us/step - loss: 0.1790 - acc: 0.9526 - val_loss: 0.3083 - val_acc: 0.9119\n",
      "Epoch 137/400\n",
      "1960/1960 [==============================] - 1s 564us/step - loss: 0.1814 - acc: 0.9520 - val_loss: 0.3085 - val_acc: 0.9131\n",
      "Epoch 138/400\n",
      "1960/1960 [==============================] - 1s 364us/step - loss: 0.1673 - acc: 0.9520 - val_loss: 0.3082 - val_acc: 0.9131\n",
      "Epoch 139/400\n",
      "1960/1960 [==============================] - 1s 554us/step - loss: 0.1865 - acc: 0.9454 - val_loss: 0.3049 - val_acc: 0.9155\n",
      "Epoch 140/400\n",
      "1960/1960 [==============================] - 1s 398us/step - loss: 0.1824 - acc: 0.9520 - val_loss: 0.3022 - val_acc: 0.9119\n",
      "Epoch 141/400\n",
      "1960/1960 [==============================] - 1s 522us/step - loss: 0.1743 - acc: 0.9510 - val_loss: 0.2963 - val_acc: 0.9143\n",
      "Epoch 142/400\n",
      "1960/1960 [==============================] - 1s 403us/step - loss: 0.1710 - acc: 0.9531 - val_loss: 0.2915 - val_acc: 0.9143\n",
      "Epoch 143/400\n",
      "1960/1960 [==============================] - 1s 535us/step - loss: 0.1619 - acc: 0.9566 - val_loss: 0.2883 - val_acc: 0.9143\n",
      "Epoch 144/400\n",
      "1960/1960 [==============================] - 1s 391us/step - loss: 0.1761 - acc: 0.9485 - val_loss: 0.2875 - val_acc: 0.9167\n",
      "Epoch 145/400\n",
      "1960/1960 [==============================] - 1s 552us/step - loss: 0.1704 - acc: 0.9520 - val_loss: 0.2876 - val_acc: 0.9131\n",
      "Epoch 146/400\n",
      "1960/1960 [==============================] - 1s 398us/step - loss: 0.1703 - acc: 0.9531 - val_loss: 0.2876 - val_acc: 0.9131\n",
      "Epoch 147/400\n",
      "1960/1960 [==============================] - 1s 442us/step - loss: 0.1497 - acc: 0.9597 - val_loss: 0.2874 - val_acc: 0.9155\n",
      "Epoch 148/400\n",
      "1960/1960 [==============================] - 1s 411us/step - loss: 0.1372 - acc: 0.9628 - val_loss: 0.2884 - val_acc: 0.9143\n",
      "Epoch 149/400\n",
      "1960/1960 [==============================] - 1s 491us/step - loss: 0.1530 - acc: 0.9541 - val_loss: 0.2930 - val_acc: 0.9143\n",
      "Epoch 150/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 0.1584 - acc: 0.9531 - val_loss: 0.2987 - val_acc: 0.9131\n",
      "Epoch 151/400\n",
      "1960/1960 [==============================] - 1s 518us/step - loss: 0.1567 - acc: 0.9546 - val_loss: 0.3022 - val_acc: 0.9143\n",
      "Epoch 152/400\n",
      "1960/1960 [==============================] - 1s 359us/step - loss: 0.1492 - acc: 0.9592 - val_loss: 0.3007 - val_acc: 0.9155\n",
      "Epoch 153/400\n",
      "1960/1960 [==============================] - 1s 609us/step - loss: 0.1299 - acc: 0.9663 - val_loss: 0.2981 - val_acc: 0.9155\n",
      "Epoch 154/400\n",
      "1960/1960 [==============================] - 1s 415us/step - loss: 0.1361 - acc: 0.9643 - val_loss: 0.2974 - val_acc: 0.9155\n",
      "Epoch 155/400\n",
      "1960/1960 [==============================] - 1s 555us/step - loss: 0.1527 - acc: 0.9592 - val_loss: 0.2961 - val_acc: 0.9155\n",
      "Epoch 156/400\n",
      "1960/1960 [==============================] - 1s 364us/step - loss: 0.1510 - acc: 0.9582 - val_loss: 0.2950 - val_acc: 0.9155\n",
      "Epoch 157/400\n",
      "1960/1960 [==============================] - 1s 541us/step - loss: 0.1480 - acc: 0.9592 - val_loss: 0.2961 - val_acc: 0.9155\n",
      "Epoch 158/400\n",
      "1960/1960 [==============================] - 1s 350us/step - loss: 0.1425 - acc: 0.9571 - val_loss: 0.2981 - val_acc: 0.9143\n",
      "Epoch 159/400\n",
      "1960/1960 [==============================] - 1s 509us/step - loss: 0.1264 - acc: 0.9694 - val_loss: 0.2991 - val_acc: 0.9131\n",
      "Epoch 160/400\n",
      "1960/1960 [==============================] - 1s 399us/step - loss: 0.1405 - acc: 0.9592 - val_loss: 0.2999 - val_acc: 0.9131\n",
      "Epoch 161/400\n",
      "1960/1960 [==============================] - 1s 546us/step - loss: 0.1390 - acc: 0.9612 - val_loss: 0.3007 - val_acc: 0.9131\n",
      "Epoch 162/400\n",
      "1960/1960 [==============================] - 1s 319us/step - loss: 0.1420 - acc: 0.9633 - val_loss: 0.3007 - val_acc: 0.9119\n",
      "Epoch 163/400\n",
      "1960/1960 [==============================] - 1s 470us/step - loss: 0.1356 - acc: 0.9663 - val_loss: 0.2989 - val_acc: 0.9131\n",
      "Epoch 164/400\n",
      "1960/1960 [==============================] - 1s 378us/step - loss: 0.1420 - acc: 0.9617 - val_loss: 0.2967 - val_acc: 0.9179\n",
      "Epoch 165/400\n",
      "1960/1960 [==============================] - 1s 548us/step - loss: 0.1467 - acc: 0.9582 - val_loss: 0.2939 - val_acc: 0.9179\n",
      "Epoch 166/400\n",
      "1960/1960 [==============================] - 1s 367us/step - loss: 0.1343 - acc: 0.9617 - val_loss: 0.2907 - val_acc: 0.9190\n",
      "Epoch 167/400\n",
      "1960/1960 [==============================] - 1s 505us/step - loss: 0.1255 - acc: 0.9689 - val_loss: 0.2884 - val_acc: 0.9214\n",
      "Epoch 168/400\n",
      "1960/1960 [==============================] - 1s 343us/step - loss: 0.1263 - acc: 0.9617 - val_loss: 0.2860 - val_acc: 0.9226\n",
      "Epoch 169/400\n",
      "1960/1960 [==============================] - 1s 563us/step - loss: 0.1403 - acc: 0.9648 - val_loss: 0.2829 - val_acc: 0.9202\n",
      "Epoch 170/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 0.1274 - acc: 0.9673 - val_loss: 0.2811 - val_acc: 0.9190\n",
      "Epoch 171/400\n",
      "1960/1960 [==============================] - 1s 505us/step - loss: 0.1250 - acc: 0.9673 - val_loss: 0.2799 - val_acc: 0.9190\n",
      "Epoch 172/400\n",
      "1960/1960 [==============================] - 1s 365us/step - loss: 0.1290 - acc: 0.9673 - val_loss: 0.2798 - val_acc: 0.9202\n",
      "Epoch 173/400\n",
      "1960/1960 [==============================] - 1s 488us/step - loss: 0.1163 - acc: 0.9694 - val_loss: 0.2802 - val_acc: 0.9202\n",
      "Epoch 174/400\n",
      "1960/1960 [==============================] - 1s 333us/step - loss: 0.1220 - acc: 0.9709 - val_loss: 0.2810 - val_acc: 0.9214\n",
      "Epoch 175/400\n",
      "1960/1960 [==============================] - 1s 494us/step - loss: 0.1282 - acc: 0.9668 - val_loss: 0.2829 - val_acc: 0.9214\n",
      "Epoch 176/400\n",
      "1960/1960 [==============================] - 1s 334us/step - loss: 0.1267 - acc: 0.9694 - val_loss: 0.2845 - val_acc: 0.9190\n",
      "Epoch 177/400\n",
      "1960/1960 [==============================] - 1s 414us/step - loss: 0.1286 - acc: 0.9663 - val_loss: 0.2847 - val_acc: 0.9179\n",
      "Epoch 178/400\n",
      "1960/1960 [==============================] - 1s 307us/step - loss: 0.1251 - acc: 0.9643 - val_loss: 0.2843 - val_acc: 0.9167\n",
      "Epoch 179/400\n",
      "1960/1960 [==============================] - 1s 577us/step - loss: 0.1261 - acc: 0.9648 - val_loss: 0.2834 - val_acc: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/400\n",
      "1960/1960 [==============================] - 1s 404us/step - loss: 0.1293 - acc: 0.9643 - val_loss: 0.2831 - val_acc: 0.9167\n",
      "Epoch 181/400\n",
      "1960/1960 [==============================] - 1s 527us/step - loss: 0.1257 - acc: 0.9663 - val_loss: 0.2823 - val_acc: 0.9167\n",
      "Epoch 182/400\n",
      "1960/1960 [==============================] - 1s 370us/step - loss: 0.1278 - acc: 0.9628 - val_loss: 0.2813 - val_acc: 0.9167\n",
      "Epoch 183/400\n",
      "1960/1960 [==============================] - 1s 515us/step - loss: 0.1293 - acc: 0.9638 - val_loss: 0.2798 - val_acc: 0.9179\n",
      "Epoch 184/400\n",
      "1960/1960 [==============================] - 1s 414us/step - loss: 0.1267 - acc: 0.9658 - val_loss: 0.2773 - val_acc: 0.9155\n",
      "Epoch 185/400\n",
      "1960/1960 [==============================] - 1s 516us/step - loss: 0.1158 - acc: 0.9684 - val_loss: 0.2748 - val_acc: 0.9155\n",
      "Epoch 186/400\n",
      "1960/1960 [==============================] - 1s 340us/step - loss: 0.1209 - acc: 0.9694 - val_loss: 0.2721 - val_acc: 0.9202\n",
      "Epoch 187/400\n",
      "1960/1960 [==============================] - 1s 463us/step - loss: 0.1261 - acc: 0.9689 - val_loss: 0.2693 - val_acc: 0.9238\n",
      "Epoch 188/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 0.1314 - acc: 0.9653 - val_loss: 0.2671 - val_acc: 0.9238\n",
      "Epoch 189/400\n",
      "1960/1960 [==============================] - 1s 507us/step - loss: 0.1177 - acc: 0.9668 - val_loss: 0.2648 - val_acc: 0.9226\n",
      "Epoch 190/400\n",
      "1960/1960 [==============================] - 1s 384us/step - loss: 0.1334 - acc: 0.9633 - val_loss: 0.2626 - val_acc: 0.9238\n",
      "Epoch 191/400\n",
      "1960/1960 [==============================] - 1s 479us/step - loss: 0.1082 - acc: 0.9719 - val_loss: 0.2609 - val_acc: 0.9238\n",
      "Epoch 192/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 0.1129 - acc: 0.9709 - val_loss: 0.2593 - val_acc: 0.9262\n",
      "Epoch 193/400\n",
      "1960/1960 [==============================] - 1s 530us/step - loss: 0.1122 - acc: 0.9714 - val_loss: 0.2582 - val_acc: 0.9262\n",
      "Epoch 194/400\n",
      "1960/1960 [==============================] - 1s 379us/step - loss: 0.1206 - acc: 0.9658 - val_loss: 0.2573 - val_acc: 0.9274\n",
      "Epoch 195/400\n",
      "1960/1960 [==============================] - 1s 506us/step - loss: 0.1254 - acc: 0.9684 - val_loss: 0.2563 - val_acc: 0.9274\n",
      "Epoch 196/400\n",
      "1960/1960 [==============================] - 1s 350us/step - loss: 0.1213 - acc: 0.9673 - val_loss: 0.2554 - val_acc: 0.9274\n",
      "Epoch 197/400\n",
      "1960/1960 [==============================] - 1s 525us/step - loss: 0.1231 - acc: 0.9668 - val_loss: 0.2543 - val_acc: 0.9286\n",
      "Epoch 198/400\n",
      "1960/1960 [==============================] - 1s 408us/step - loss: 0.1001 - acc: 0.9750 - val_loss: 0.2533 - val_acc: 0.9274\n",
      "Epoch 199/400\n",
      "1960/1960 [==============================] - 1s 523us/step - loss: 0.1089 - acc: 0.9709 - val_loss: 0.2523 - val_acc: 0.9262\n",
      "Epoch 200/400\n",
      "1960/1960 [==============================] - 1s 352us/step - loss: 0.1104 - acc: 0.9684 - val_loss: 0.2515 - val_acc: 0.9274\n",
      "Epoch 201/400\n",
      "1960/1960 [==============================] - 1s 531us/step - loss: 0.1130 - acc: 0.9719 - val_loss: 0.2506 - val_acc: 0.9274\n",
      "Epoch 202/400\n",
      "1960/1960 [==============================] - 1s 382us/step - loss: 0.1137 - acc: 0.9689 - val_loss: 0.2500 - val_acc: 0.9286\n",
      "Epoch 203/400\n",
      "1960/1960 [==============================] - 1s 559us/step - loss: 0.1192 - acc: 0.9679 - val_loss: 0.2493 - val_acc: 0.9286\n",
      "Epoch 204/400\n",
      "1960/1960 [==============================] - 1s 401us/step - loss: 0.1109 - acc: 0.9694 - val_loss: 0.2486 - val_acc: 0.9286\n",
      "Epoch 205/400\n",
      "1960/1960 [==============================] - 1s 515us/step - loss: 0.1229 - acc: 0.9617 - val_loss: 0.2478 - val_acc: 0.9286\n",
      "Epoch 206/400\n",
      "1960/1960 [==============================] - 1s 408us/step - loss: 0.1118 - acc: 0.9719 - val_loss: 0.2471 - val_acc: 0.9286\n",
      "Epoch 207/400\n",
      "1960/1960 [==============================] - 1s 521us/step - loss: 0.1273 - acc: 0.9622 - val_loss: 0.2463 - val_acc: 0.9286\n",
      "Epoch 208/400\n",
      "1960/1960 [==============================] - 1s 365us/step - loss: 0.1046 - acc: 0.9709 - val_loss: 0.2455 - val_acc: 0.9286\n",
      "Epoch 209/400\n",
      "1960/1960 [==============================] - 1s 454us/step - loss: 0.1187 - acc: 0.9694 - val_loss: 0.2448 - val_acc: 0.9298\n",
      "Epoch 210/400\n",
      "1960/1960 [==============================] - 1s 366us/step - loss: 0.1304 - acc: 0.9633 - val_loss: 0.2440 - val_acc: 0.9310\n",
      "Epoch 211/400\n",
      "1960/1960 [==============================] - 1s 476us/step - loss: 0.1006 - acc: 0.9760 - val_loss: 0.2432 - val_acc: 0.9310\n",
      "Epoch 212/400\n",
      "1960/1960 [==============================] - 1s 371us/step - loss: 0.1051 - acc: 0.9699 - val_loss: 0.2425 - val_acc: 0.9310\n",
      "Epoch 213/400\n",
      "1960/1960 [==============================] - 1s 389us/step - loss: 0.1262 - acc: 0.9663 - val_loss: 0.2419 - val_acc: 0.9321\n",
      "Epoch 214/400\n",
      "1960/1960 [==============================] - 1s 397us/step - loss: 0.1239 - acc: 0.9668 - val_loss: 0.2413 - val_acc: 0.9333\n",
      "Epoch 215/400\n",
      "1960/1960 [==============================] - 1s 497us/step - loss: 0.1148 - acc: 0.9684 - val_loss: 0.2407 - val_acc: 0.9345\n",
      "Epoch 216/400\n",
      "1960/1960 [==============================] - 1s 334us/step - loss: 0.1041 - acc: 0.9740 - val_loss: 0.2400 - val_acc: 0.9345\n",
      "Epoch 217/400\n",
      "1960/1960 [==============================] - 1s 423us/step - loss: 0.1108 - acc: 0.9704 - val_loss: 0.2393 - val_acc: 0.9345\n",
      "Epoch 218/400\n",
      "1960/1960 [==============================] - 1s 356us/step - loss: 0.1135 - acc: 0.9668 - val_loss: 0.2386 - val_acc: 0.9345\n",
      "Epoch 219/400\n",
      "1960/1960 [==============================] - 1s 466us/step - loss: 0.1167 - acc: 0.9699 - val_loss: 0.2379 - val_acc: 0.9357\n",
      "Epoch 220/400\n",
      "1960/1960 [==============================] - 1s 347us/step - loss: 0.1136 - acc: 0.9689 - val_loss: 0.2372 - val_acc: 0.9357\n",
      "Epoch 221/400\n",
      "1960/1960 [==============================] - 1s 450us/step - loss: 0.1073 - acc: 0.9714 - val_loss: 0.2366 - val_acc: 0.9357\n",
      "Epoch 222/400\n",
      "1960/1960 [==============================] - 1s 391us/step - loss: 0.1131 - acc: 0.9755 - val_loss: 0.2361 - val_acc: 0.9357\n",
      "Epoch 223/400\n",
      "1960/1960 [==============================] - 1s 503us/step - loss: 0.1135 - acc: 0.9699 - val_loss: 0.2356 - val_acc: 0.9345\n",
      "Epoch 224/400\n",
      "1960/1960 [==============================] - 1s 360us/step - loss: 0.1190 - acc: 0.9653 - val_loss: 0.2351 - val_acc: 0.9357\n",
      "Epoch 225/400\n",
      "1960/1960 [==============================] - 1s 461us/step - loss: 0.1125 - acc: 0.9704 - val_loss: 0.2346 - val_acc: 0.9357\n",
      "Epoch 226/400\n",
      "1960/1960 [==============================] - 1s 284us/step - loss: 0.1245 - acc: 0.9633 - val_loss: 0.2342 - val_acc: 0.9357\n",
      "Epoch 227/400\n",
      "1960/1960 [==============================] - 1s 464us/step - loss: 0.1181 - acc: 0.9689 - val_loss: 0.2338 - val_acc: 0.9357\n",
      "Epoch 228/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 0.1150 - acc: 0.9699 - val_loss: 0.2334 - val_acc: 0.9345\n",
      "Epoch 229/400\n",
      "1960/1960 [==============================] - 1s 531us/step - loss: 0.1144 - acc: 0.9719 - val_loss: 0.2331 - val_acc: 0.9345\n",
      "Epoch 230/400\n",
      "1960/1960 [==============================] - 1s 380us/step - loss: 0.1091 - acc: 0.9724 - val_loss: 0.2327 - val_acc: 0.9345\n",
      "Epoch 231/400\n",
      "1960/1960 [==============================] - 1s 461us/step - loss: 0.1226 - acc: 0.9668 - val_loss: 0.2324 - val_acc: 0.9345\n",
      "Epoch 232/400\n",
      "1960/1960 [==============================] - 1s 332us/step - loss: 0.1089 - acc: 0.9724 - val_loss: 0.2320 - val_acc: 0.9345\n",
      "Epoch 233/400\n",
      "1960/1960 [==============================] - 1s 483us/step - loss: 0.1022 - acc: 0.9740 - val_loss: 0.2316 - val_acc: 0.9357\n",
      "Epoch 234/400\n",
      "1960/1960 [==============================] - 1s 361us/step - loss: 0.1094 - acc: 0.9719 - val_loss: 0.2313 - val_acc: 0.9357\n",
      "Epoch 235/400\n",
      "1960/1960 [==============================] - 1s 396us/step - loss: 0.1059 - acc: 0.9724 - val_loss: 0.2310 - val_acc: 0.9357\n",
      "Epoch 236/400\n",
      "1960/1960 [==============================] - 1s 354us/step - loss: 0.1323 - acc: 0.9571 - val_loss: 0.2307 - val_acc: 0.9357\n",
      "Epoch 237/400\n",
      "1960/1960 [==============================] - 1s 484us/step - loss: 0.1176 - acc: 0.9658 - val_loss: 0.2304 - val_acc: 0.9357\n",
      "Epoch 238/400\n",
      "1960/1960 [==============================] - 1s 380us/step - loss: 0.1127 - acc: 0.9694 - val_loss: 0.2301 - val_acc: 0.9369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/400\n",
      "1960/1960 [==============================] - 1s 488us/step - loss: 0.1168 - acc: 0.9684 - val_loss: 0.2298 - val_acc: 0.9369\n",
      "Epoch 240/400\n",
      "1960/1960 [==============================] - 1s 307us/step - loss: 0.1297 - acc: 0.9592 - val_loss: 0.2295 - val_acc: 0.9369\n",
      "Epoch 241/400\n",
      "1960/1960 [==============================] - 1s 433us/step - loss: 0.1129 - acc: 0.9709 - val_loss: 0.2293 - val_acc: 0.9369\n",
      "Epoch 242/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 0.1050 - acc: 0.9724 - val_loss: 0.2290 - val_acc: 0.9369\n",
      "Epoch 243/400\n",
      "1960/1960 [==============================] - 1s 459us/step - loss: 0.1028 - acc: 0.9735 - val_loss: 0.2287 - val_acc: 0.9369\n",
      "Epoch 244/400\n",
      "1960/1960 [==============================] - 1s 354us/step - loss: 0.1110 - acc: 0.9709 - val_loss: 0.2285 - val_acc: 0.9369\n",
      "Epoch 245/400\n",
      "1960/1960 [==============================] - 1s 517us/step - loss: 0.1082 - acc: 0.9704 - val_loss: 0.2282 - val_acc: 0.9369\n",
      "Epoch 246/400\n",
      "1960/1960 [==============================] - 1s 360us/step - loss: 0.1253 - acc: 0.9668 - val_loss: 0.2280 - val_acc: 0.9357\n",
      "Epoch 247/400\n",
      "1960/1960 [==============================] - 1s 520us/step - loss: 0.1087 - acc: 0.9694 - val_loss: 0.2278 - val_acc: 0.9357\n",
      "Epoch 248/400\n",
      "1960/1960 [==============================] - 1s 395us/step - loss: 0.1080 - acc: 0.9704 - val_loss: 0.2275 - val_acc: 0.9357\n",
      "Epoch 249/400\n",
      "1960/1960 [==============================] - 1s 516us/step - loss: 0.1255 - acc: 0.9643 - val_loss: 0.2273 - val_acc: 0.9357\n",
      "Epoch 250/400\n",
      "1960/1960 [==============================] - 1s 364us/step - loss: 0.1127 - acc: 0.9684 - val_loss: 0.2271 - val_acc: 0.9357\n",
      "Epoch 251/400\n",
      "1960/1960 [==============================] - 1s 435us/step - loss: 0.1215 - acc: 0.9643 - val_loss: 0.2269 - val_acc: 0.9357\n",
      "Epoch 252/400\n",
      "1960/1960 [==============================] - 1s 401us/step - loss: 0.1212 - acc: 0.9745 - val_loss: 0.2267 - val_acc: 0.9357\n",
      "Epoch 253/400\n",
      "1960/1960 [==============================] - 1s 378us/step - loss: 0.1168 - acc: 0.9735 - val_loss: 0.2265 - val_acc: 0.9357\n",
      "Epoch 254/400\n",
      "1960/1960 [==============================] - 1s 405us/step - loss: 0.1235 - acc: 0.9699 - val_loss: 0.2263 - val_acc: 0.9357\n",
      "Epoch 255/400\n",
      "1960/1960 [==============================] - 1s 503us/step - loss: 0.1081 - acc: 0.9684 - val_loss: 0.2261 - val_acc: 0.9357\n",
      "Epoch 256/400\n",
      "1960/1960 [==============================] - 1s 365us/step - loss: 0.1135 - acc: 0.9684 - val_loss: 0.2259 - val_acc: 0.9357\n",
      "Epoch 257/400\n",
      "1960/1960 [==============================] - 1s 442us/step - loss: 0.1242 - acc: 0.9622 - val_loss: 0.2257 - val_acc: 0.9357\n",
      "Epoch 258/400\n",
      "1960/1960 [==============================] - 1s 395us/step - loss: 0.1151 - acc: 0.9704 - val_loss: 0.2256 - val_acc: 0.9357\n",
      "Epoch 259/400\n",
      "1960/1960 [==============================] - 1s 457us/step - loss: 0.1099 - acc: 0.9653 - val_loss: 0.2254 - val_acc: 0.9357\n",
      "Epoch 260/400\n",
      "1960/1960 [==============================] - 1s 348us/step - loss: 0.1171 - acc: 0.9689 - val_loss: 0.2252 - val_acc: 0.9357\n",
      "Epoch 261/400\n",
      "1960/1960 [==============================] - 1s 508us/step - loss: 0.1098 - acc: 0.9699 - val_loss: 0.2251 - val_acc: 0.9369\n",
      "Epoch 262/400\n",
      "1960/1960 [==============================] - 1s 327us/step - loss: 0.1254 - acc: 0.9694 - val_loss: 0.2249 - val_acc: 0.9369\n",
      "Epoch 263/400\n",
      "1960/1960 [==============================] - 1s 501us/step - loss: 0.1187 - acc: 0.9658 - val_loss: 0.2248 - val_acc: 0.9369\n",
      "Epoch 264/400\n",
      "1960/1960 [==============================] - 1s 373us/step - loss: 0.1061 - acc: 0.9719 - val_loss: 0.2246 - val_acc: 0.9369\n",
      "Epoch 265/400\n",
      "1960/1960 [==============================] - 1s 461us/step - loss: 0.1094 - acc: 0.9709 - val_loss: 0.2245 - val_acc: 0.9369\n",
      "Epoch 266/400\n",
      "1960/1960 [==============================] - 1s 338us/step - loss: 0.1103 - acc: 0.9673 - val_loss: 0.2243 - val_acc: 0.9369\n",
      "Epoch 267/400\n",
      "1960/1960 [==============================] - 1s 356us/step - loss: 0.1028 - acc: 0.9694 - val_loss: 0.2242 - val_acc: 0.9369\n",
      "Epoch 268/400\n",
      "1960/1960 [==============================] - 1s 346us/step - loss: 0.1094 - acc: 0.9668 - val_loss: 0.2241 - val_acc: 0.9369\n",
      "Epoch 269/400\n",
      "1960/1960 [==============================] - 1s 505us/step - loss: 0.1053 - acc: 0.9786 - val_loss: 0.2239 - val_acc: 0.9369\n",
      "Epoch 270/400\n",
      "1960/1960 [==============================] - 1s 360us/step - loss: 0.1131 - acc: 0.9679 - val_loss: 0.2238 - val_acc: 0.9369\n",
      "Epoch 271/400\n",
      "1960/1960 [==============================] - 1s 479us/step - loss: 0.1230 - acc: 0.9679 - val_loss: 0.2237 - val_acc: 0.9369\n",
      "Epoch 272/400\n",
      "1960/1960 [==============================] - 1s 388us/step - loss: 0.1185 - acc: 0.9668 - val_loss: 0.2236 - val_acc: 0.9381\n",
      "Epoch 273/400\n",
      "1960/1960 [==============================] - 1s 524us/step - loss: 0.1094 - acc: 0.9719 - val_loss: 0.2235 - val_acc: 0.9381\n",
      "Epoch 274/400\n",
      "1960/1960 [==============================] - 1s 400us/step - loss: 0.1039 - acc: 0.9745 - val_loss: 0.2233 - val_acc: 0.9381\n",
      "Epoch 275/400\n",
      "1960/1960 [==============================] - 1s 499us/step - loss: 0.1189 - acc: 0.9663 - val_loss: 0.2232 - val_acc: 0.9393\n",
      "Epoch 276/400\n",
      "1960/1960 [==============================] - 1s 387us/step - loss: 0.1069 - acc: 0.9714 - val_loss: 0.2232 - val_acc: 0.9393\n",
      "Epoch 277/400\n",
      "1960/1960 [==============================] - 1s 486us/step - loss: 0.1009 - acc: 0.9796 - val_loss: 0.2231 - val_acc: 0.9393\n",
      "Epoch 278/400\n",
      "1960/1960 [==============================] - 1s 345us/step - loss: 0.1126 - acc: 0.9709 - val_loss: 0.2230 - val_acc: 0.9393\n",
      "Epoch 279/400\n",
      "1960/1960 [==============================] - 1s 488us/step - loss: 0.1213 - acc: 0.9668 - val_loss: 0.2229 - val_acc: 0.9393\n",
      "Epoch 280/400\n",
      "1960/1960 [==============================] - 1s 385us/step - loss: 0.1116 - acc: 0.9704 - val_loss: 0.2228 - val_acc: 0.9393\n",
      "Epoch 281/400\n",
      "1960/1960 [==============================] - 1s 489us/step - loss: 0.1290 - acc: 0.9602 - val_loss: 0.2227 - val_acc: 0.9393\n",
      "Epoch 282/400\n",
      "1960/1960 [==============================] - 1s 338us/step - loss: 0.1126 - acc: 0.9745 - val_loss: 0.2226 - val_acc: 0.9393\n",
      "Epoch 283/400\n",
      "1960/1960 [==============================] - 1s 395us/step - loss: 0.1101 - acc: 0.9684 - val_loss: 0.2226 - val_acc: 0.9393\n",
      "Epoch 284/400\n",
      "1960/1960 [==============================] - 1s 423us/step - loss: 0.1158 - acc: 0.9679 - val_loss: 0.2225 - val_acc: 0.9393\n",
      "Epoch 285/400\n",
      "1960/1960 [==============================] - 1s 379us/step - loss: 0.1162 - acc: 0.9704 - val_loss: 0.2225 - val_acc: 0.9393\n",
      "Epoch 286/400\n",
      "1960/1960 [==============================] - 1s 401us/step - loss: 0.1155 - acc: 0.9709 - val_loss: 0.2225 - val_acc: 0.9393\n",
      "Epoch 287/400\n",
      "1960/1960 [==============================] - 1s 482us/step - loss: 0.1014 - acc: 0.9740 - val_loss: 0.2224 - val_acc: 0.9393\n",
      "Epoch 288/400\n",
      "1960/1960 [==============================] - 1s 413us/step - loss: 0.1087 - acc: 0.9699 - val_loss: 0.2224 - val_acc: 0.9393\n",
      "Epoch 289/400\n",
      "1960/1960 [==============================] - 1s 416us/step - loss: 0.1117 - acc: 0.9699 - val_loss: 0.2223 - val_acc: 0.9393\n",
      "Epoch 290/400\n",
      "1960/1960 [==============================] - 1s 404us/step - loss: 0.1113 - acc: 0.9694 - val_loss: 0.2223 - val_acc: 0.9393\n",
      "Epoch 291/400\n",
      "1960/1960 [==============================] - 1s 464us/step - loss: 0.1020 - acc: 0.9755 - val_loss: 0.2222 - val_acc: 0.9393\n",
      "Epoch 292/400\n",
      "1960/1960 [==============================] - 1s 410us/step - loss: 0.1122 - acc: 0.9709 - val_loss: 0.2222 - val_acc: 0.9381\n",
      "Epoch 293/400\n",
      "1960/1960 [==============================] - 1s 432us/step - loss: 0.1094 - acc: 0.9714 - val_loss: 0.2222 - val_acc: 0.9381\n",
      "Epoch 294/400\n",
      "1960/1960 [==============================] - 1s 413us/step - loss: 0.1112 - acc: 0.9730 - val_loss: 0.2222 - val_acc: 0.9381\n",
      "Epoch 295/400\n",
      "1960/1960 [==============================] - 1s 458us/step - loss: 0.1156 - acc: 0.9719 - val_loss: 0.2221 - val_acc: 0.9381\n",
      "Epoch 296/400\n",
      "1960/1960 [==============================] - 1s 371us/step - loss: 0.1242 - acc: 0.9663 - val_loss: 0.2221 - val_acc: 0.9381\n",
      "Epoch 297/400\n",
      "1960/1960 [==============================] - 1s 466us/step - loss: 0.1116 - acc: 0.9679 - val_loss: 0.2220 - val_acc: 0.9381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/400\n",
      "1960/1960 [==============================] - 1s 357us/step - loss: 0.1110 - acc: 0.9719 - val_loss: 0.2220 - val_acc: 0.9381\n",
      "Epoch 299/400\n",
      "1960/1960 [==============================] - 1s 438us/step - loss: 0.1066 - acc: 0.9709 - val_loss: 0.2220 - val_acc: 0.9393\n",
      "Epoch 300/400\n",
      "1960/1960 [==============================] - 1s 375us/step - loss: 0.1081 - acc: 0.9673 - val_loss: 0.2219 - val_acc: 0.9393\n",
      "Epoch 301/400\n",
      "1960/1960 [==============================] - 1s 451us/step - loss: 0.1072 - acc: 0.9745 - val_loss: 0.2219 - val_acc: 0.9393\n",
      "Epoch 302/400\n",
      "1960/1960 [==============================] - 1s 401us/step - loss: 0.1060 - acc: 0.9638 - val_loss: 0.2218 - val_acc: 0.9393\n",
      "Epoch 303/400\n",
      "1960/1960 [==============================] - 1s 473us/step - loss: 0.0932 - acc: 0.9745 - val_loss: 0.2218 - val_acc: 0.9405\n",
      "Epoch 304/400\n",
      "1960/1960 [==============================] - 1s 383us/step - loss: 0.1092 - acc: 0.9755 - val_loss: 0.2218 - val_acc: 0.9405\n",
      "Epoch 305/400\n",
      "1960/1960 [==============================] - 1s 368us/step - loss: 0.1134 - acc: 0.9689 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 306/400\n",
      "1960/1960 [==============================] - 1s 412us/step - loss: 0.1053 - acc: 0.9709 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 307/400\n",
      "1960/1960 [==============================] - 1s 467us/step - loss: 0.1157 - acc: 0.9724 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 308/400\n",
      "1960/1960 [==============================] - 1s 374us/step - loss: 0.1184 - acc: 0.9704 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 309/400\n",
      "1960/1960 [==============================] - 1s 453us/step - loss: 0.1130 - acc: 0.9699 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 310/400\n",
      "1960/1960 [==============================] - 1s 379us/step - loss: 0.1190 - acc: 0.9689 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 311/400\n",
      "1960/1960 [==============================] - 1s 452us/step - loss: 0.1102 - acc: 0.9730 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 312/400\n",
      "1960/1960 [==============================] - 1s 382us/step - loss: 0.1073 - acc: 0.9699 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 313/400\n",
      "1960/1960 [==============================] - 1s 415us/step - loss: 0.1163 - acc: 0.9714 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 314/400\n",
      "1960/1960 [==============================] - 1s 331us/step - loss: 0.1030 - acc: 0.9704 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 315/400\n",
      "1960/1960 [==============================] - 1s 485us/step - loss: 0.1072 - acc: 0.9709 - val_loss: 0.2217 - val_acc: 0.9405\n",
      "Epoch 316/400\n",
      "1960/1960 [==============================] - 1s 407us/step - loss: 0.1137 - acc: 0.9699 - val_loss: 0.2216 - val_acc: 0.9405\n",
      "Epoch 317/400\n",
      "1960/1960 [==============================] - 1s 463us/step - loss: 0.1118 - acc: 0.9694 - val_loss: 0.2216 - val_acc: 0.9405\n",
      "Epoch 318/400\n",
      "1960/1960 [==============================] - 1s 357us/step - loss: 0.1263 - acc: 0.9684 - val_loss: 0.2216 - val_acc: 0.9405\n",
      "Epoch 319/400\n",
      "1960/1960 [==============================] - 1s 427us/step - loss: 0.1180 - acc: 0.9648 - val_loss: 0.2215 - val_acc: 0.9405\n",
      "Epoch 320/400\n",
      "1960/1960 [==============================] - 1s 390us/step - loss: 0.1130 - acc: 0.9745 - val_loss: 0.2215 - val_acc: 0.9405\n",
      "Epoch 321/400\n",
      "1960/1960 [==============================] - 1s 375us/step - loss: 0.1144 - acc: 0.9704 - val_loss: 0.2215 - val_acc: 0.9405\n",
      "Epoch 322/400\n",
      "1960/1960 [==============================] - 1s 397us/step - loss: 0.1171 - acc: 0.9673 - val_loss: 0.2215 - val_acc: 0.9405\n",
      "Epoch 323/400\n",
      "1960/1960 [==============================] - 1s 475us/step - loss: 0.1157 - acc: 0.9684 - val_loss: 0.2215 - val_acc: 0.9405\n",
      "Epoch 324/400\n",
      "1960/1960 [==============================] - 1s 388us/step - loss: 0.1007 - acc: 0.9755 - val_loss: 0.2215 - val_acc: 0.9405\n",
      "Epoch 325/400\n",
      "1960/1960 [==============================] - 1s 413us/step - loss: 0.1199 - acc: 0.9714 - val_loss: 0.2214 - val_acc: 0.9405\n",
      "Epoch 326/400\n",
      "1960/1960 [==============================] - 1s 390us/step - loss: 0.1257 - acc: 0.9622 - val_loss: 0.2214 - val_acc: 0.9405\n",
      "Epoch 327/400\n",
      "1960/1960 [==============================] - 1s 417us/step - loss: 0.1234 - acc: 0.9653 - val_loss: 0.2214 - val_acc: 0.9405\n",
      "Epoch 328/400\n",
      "1960/1960 [==============================] - 1s 424us/step - loss: 0.1150 - acc: 0.9694 - val_loss: 0.2214 - val_acc: 0.9405\n",
      "Epoch 329/400\n",
      "1960/1960 [==============================] - 1s 352us/step - loss: 0.1216 - acc: 0.9663 - val_loss: 0.2213 - val_acc: 0.9405\n",
      "Epoch 330/400\n",
      "1960/1960 [==============================] - 1s 391us/step - loss: 0.1052 - acc: 0.9765 - val_loss: 0.2213 - val_acc: 0.9405\n",
      "Epoch 331/400\n",
      "1960/1960 [==============================] - 1s 333us/step - loss: 0.1186 - acc: 0.9745 - val_loss: 0.2213 - val_acc: 0.9405\n",
      "Epoch 332/400\n",
      "1960/1960 [==============================] - 1s 396us/step - loss: 0.1206 - acc: 0.9673 - val_loss: 0.2212 - val_acc: 0.9405\n",
      "Epoch 333/400\n",
      "1960/1960 [==============================] - 1s 482us/step - loss: 0.1286 - acc: 0.9653 - val_loss: 0.2212 - val_acc: 0.9405\n",
      "Epoch 334/400\n",
      "1960/1960 [==============================] - 1s 400us/step - loss: 0.1299 - acc: 0.9648 - val_loss: 0.2212 - val_acc: 0.9405\n",
      "Epoch 335/400\n",
      "1960/1960 [==============================] - 1s 527us/step - loss: 0.0970 - acc: 0.9776 - val_loss: 0.2212 - val_acc: 0.9405\n",
      "Epoch 336/400\n",
      "1960/1960 [==============================] - 1s 450us/step - loss: 0.1061 - acc: 0.9709 - val_loss: 0.2212 - val_acc: 0.9405\n",
      "Epoch 337/400\n",
      "1960/1960 [==============================] - 1s 484us/step - loss: 0.1105 - acc: 0.9689 - val_loss: 0.2212 - val_acc: 0.9405\n",
      "Epoch 338/400\n",
      "1960/1960 [==============================] - 1s 391us/step - loss: 0.1270 - acc: 0.9658 - val_loss: 0.2211 - val_acc: 0.9405\n",
      "Epoch 339/400\n",
      "1960/1960 [==============================] - 1s 434us/step - loss: 0.1269 - acc: 0.9633 - val_loss: 0.2211 - val_acc: 0.9405\n",
      "Epoch 340/400\n",
      "1960/1960 [==============================] - 1s 385us/step - loss: 0.1233 - acc: 0.9673 - val_loss: 0.2211 - val_acc: 0.9405\n",
      "Epoch 341/400\n",
      "1960/1960 [==============================] - 1s 402us/step - loss: 0.1059 - acc: 0.9719 - val_loss: 0.2211 - val_acc: 0.9405\n",
      "Epoch 342/400\n",
      "1960/1960 [==============================] - 1s 396us/step - loss: 0.1090 - acc: 0.9694 - val_loss: 0.2211 - val_acc: 0.9405\n",
      "Epoch 343/400\n",
      "1960/1960 [==============================] - 1s 373us/step - loss: 0.1190 - acc: 0.9699 - val_loss: 0.2211 - val_acc: 0.9405\n",
      "Epoch 344/400\n",
      "1960/1960 [==============================] - 1s 402us/step - loss: 0.1199 - acc: 0.9658 - val_loss: 0.2211 - val_acc: 0.9405\n",
      "Epoch 345/400\n",
      "1960/1960 [==============================] - 1s 342us/step - loss: 0.1228 - acc: 0.9643 - val_loss: 0.2211 - val_acc: 0.9405\n",
      "Epoch 346/400\n",
      "1960/1960 [==============================] - 1s 411us/step - loss: 0.1164 - acc: 0.9699 - val_loss: 0.2210 - val_acc: 0.9405\n",
      "Epoch 347/400\n",
      "1960/1960 [==============================] - 1s 501us/step - loss: 0.1042 - acc: 0.9745 - val_loss: 0.2210 - val_acc: 0.9405\n",
      "Epoch 348/400\n",
      "1960/1960 [==============================] - 1s 450us/step - loss: 0.1155 - acc: 0.9673 - val_loss: 0.2209 - val_acc: 0.9405\n",
      "Epoch 349/400\n",
      "1960/1960 [==============================] - 1s 465us/step - loss: 0.1122 - acc: 0.9694 - val_loss: 0.2209 - val_acc: 0.9405\n",
      "Epoch 350/400\n",
      "1960/1960 [==============================] - 1s 347us/step - loss: 0.1057 - acc: 0.9724 - val_loss: 0.2209 - val_acc: 0.9405\n",
      "Epoch 351/400\n",
      "1960/1960 [==============================] - 1s 512us/step - loss: 0.1040 - acc: 0.9699 - val_loss: 0.2208 - val_acc: 0.9405\n",
      "Epoch 352/400\n",
      "1960/1960 [==============================] - 1s 399us/step - loss: 0.1131 - acc: 0.9714 - val_loss: 0.2208 - val_acc: 0.9405\n",
      "Epoch 353/400\n",
      "1960/1960 [==============================] - 1s 471us/step - loss: 0.1149 - acc: 0.9668 - val_loss: 0.2208 - val_acc: 0.9405\n",
      "Epoch 354/400\n",
      "1960/1960 [==============================] - 1s 334us/step - loss: 0.1200 - acc: 0.9694 - val_loss: 0.2207 - val_acc: 0.9405\n",
      "Epoch 355/400\n",
      "1960/1960 [==============================] - 1s 463us/step - loss: 0.1111 - acc: 0.9704 - val_loss: 0.2207 - val_acc: 0.9405\n",
      "Epoch 356/400\n",
      "1960/1960 [==============================] - 1s 425us/step - loss: 0.1058 - acc: 0.9724 - val_loss: 0.2206 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357/400\n",
      "1960/1960 [==============================] - 1s 465us/step - loss: 0.1080 - acc: 0.9689 - val_loss: 0.2206 - val_acc: 0.9405\n",
      "Epoch 358/400\n",
      "1960/1960 [==============================] - 1s 329us/step - loss: 0.1142 - acc: 0.9709 - val_loss: 0.2206 - val_acc: 0.9405\n",
      "Epoch 359/400\n",
      "1960/1960 [==============================] - 1s 479us/step - loss: 0.1088 - acc: 0.9668 - val_loss: 0.2206 - val_acc: 0.9405\n",
      "Epoch 360/400\n",
      "1960/1960 [==============================] - 1s 362us/step - loss: 0.1097 - acc: 0.9694 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 361/400\n",
      "1960/1960 [==============================] - 1s 447us/step - loss: 0.1160 - acc: 0.9668 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 362/400\n",
      "1960/1960 [==============================] - 1s 417us/step - loss: 0.1170 - acc: 0.9735 - val_loss: 0.2204 - val_acc: 0.9405\n",
      "Epoch 363/400\n",
      "1960/1960 [==============================] - 1s 404us/step - loss: 0.1076 - acc: 0.9735 - val_loss: 0.2204 - val_acc: 0.9405\n",
      "Epoch 364/400\n",
      "1960/1960 [==============================] - 1s 388us/step - loss: 0.1201 - acc: 0.9684 - val_loss: 0.2204 - val_acc: 0.9405\n",
      "Epoch 365/400\n",
      "1960/1960 [==============================] - 1s 522us/step - loss: 0.1101 - acc: 0.9704 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 366/400\n",
      "1960/1960 [==============================] - 1s 398us/step - loss: 0.1103 - acc: 0.9724 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 367/400\n",
      "1960/1960 [==============================] - 1s 495us/step - loss: 0.1095 - acc: 0.9684 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 368/400\n",
      "1960/1960 [==============================] - 1s 386us/step - loss: 0.1017 - acc: 0.9714 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 369/400\n",
      "1960/1960 [==============================] - 1s 518us/step - loss: 0.1247 - acc: 0.9699 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 370/400\n",
      "1960/1960 [==============================] - 1s 382us/step - loss: 0.1209 - acc: 0.9668 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 371/400\n",
      "1960/1960 [==============================] - 1s 470us/step - loss: 0.1063 - acc: 0.9770 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 372/400\n",
      "1960/1960 [==============================] - 1s 348us/step - loss: 0.1157 - acc: 0.9638 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 373/400\n",
      "1960/1960 [==============================] - 1s 460us/step - loss: 0.1132 - acc: 0.9679 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 374/400\n",
      "1960/1960 [==============================] - 1s 392us/step - loss: 0.1154 - acc: 0.9668 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 375/400\n",
      "1960/1960 [==============================] - 1s 404us/step - loss: 0.1085 - acc: 0.9709 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 376/400\n",
      "1960/1960 [==============================] - 1s 423us/step - loss: 0.1040 - acc: 0.9745 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 377/400\n",
      "1960/1960 [==============================] - 1s 539us/step - loss: 0.1165 - acc: 0.9699 - val_loss: 0.2203 - val_acc: 0.9405\n",
      "Epoch 378/400\n",
      "1960/1960 [==============================] - 1s 349us/step - loss: 0.1255 - acc: 0.9638 - val_loss: 0.2204 - val_acc: 0.9405\n",
      "Epoch 379/400\n",
      "1960/1960 [==============================] - 1s 543us/step - loss: 0.1154 - acc: 0.9689 - val_loss: 0.2204 - val_acc: 0.9405\n",
      "Epoch 380/400\n",
      "1960/1960 [==============================] - 1s 354us/step - loss: 0.1195 - acc: 0.9663 - val_loss: 0.2204 - val_acc: 0.9405\n",
      "Epoch 381/400\n",
      "1960/1960 [==============================] - 1s 455us/step - loss: 0.1174 - acc: 0.9704 - val_loss: 0.2204 - val_acc: 0.9405\n",
      "Epoch 382/400\n",
      "1960/1960 [==============================] - 1s 351us/step - loss: 0.1091 - acc: 0.9745 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 383/400\n",
      "1960/1960 [==============================] - 1s 454us/step - loss: 0.1148 - acc: 0.9679 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 384/400\n",
      "1960/1960 [==============================] - 1s 356us/step - loss: 0.1061 - acc: 0.9719 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 385/400\n",
      "1960/1960 [==============================] - 1s 390us/step - loss: 0.1067 - acc: 0.9704 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 386/400\n",
      "1960/1960 [==============================] - 1s 341us/step - loss: 0.1106 - acc: 0.9714 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 387/400\n",
      "1960/1960 [==============================] - 1s 494us/step - loss: 0.1091 - acc: 0.9735 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 388/400\n",
      "1960/1960 [==============================] - 1s 355us/step - loss: 0.1191 - acc: 0.9643 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 389/400\n",
      "1960/1960 [==============================] - 1s 509us/step - loss: 0.1284 - acc: 0.9628 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 390/400\n",
      "1960/1960 [==============================] - 1s 388us/step - loss: 0.1270 - acc: 0.9679 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 391/400\n",
      "1960/1960 [==============================] - 1s 477us/step - loss: 0.1037 - acc: 0.9724 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 392/400\n",
      "1960/1960 [==============================] - 1s 307us/step - loss: 0.1175 - acc: 0.9668 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 393/400\n",
      "1960/1960 [==============================] - 1s 415us/step - loss: 0.1033 - acc: 0.9745 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 394/400\n",
      "1960/1960 [==============================] - 1s 304us/step - loss: 0.1086 - acc: 0.9699 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 395/400\n",
      "1960/1960 [==============================] - 1s 466us/step - loss: 0.1145 - acc: 0.9653 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 396/400\n",
      "1960/1960 [==============================] - 1s 345us/step - loss: 0.1280 - acc: 0.9658 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 397/400\n",
      "1960/1960 [==============================] - 1s 462us/step - loss: 0.1175 - acc: 0.9714 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 398/400\n",
      "1960/1960 [==============================] - 1s 340us/step - loss: 0.1158 - acc: 0.9658 - val_loss: 0.2205 - val_acc: 0.9405\n",
      "Epoch 399/400\n",
      "1960/1960 [==============================] - 1s 465us/step - loss: 0.1036 - acc: 0.9745 - val_loss: 0.2206 - val_acc: 0.9405\n",
      "Epoch 400/400\n",
      "1960/1960 [==============================] - 1s 406us/step - loss: 0.1002 - acc: 0.9745 - val_loss: 0.2206 - val_acc: 0.9405\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_0,X_1],Y,\n",
    "                    batch_size=len(Y),\n",
    "                    epochs=400,\n",
    "                    verbose=True,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[lrScheduler],\n",
    "                    validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DD_Net.save_weights('weights/coarse_lite.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in tqdm(range(len(Train['pose']))): \n",
    "\n",
    "        label = np.zeros(C.clc_coarse)\n",
    "        label[Train['coarse_label'][i]-1] = 1 \n",
    "        \n",
    "        p = np.copy(Train['pose'][i]).reshape([-1,22,3])\n",
    "        p = sampling_frame(p,C)\n",
    "        \n",
    "        M = get_CG(p,C)\n",
    "        \n",
    "        X_0.append(M)\n",
    "        X_1.append(p)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    Y = np.stack(Y)\n",
    "   \n",
    "\n",
    "    DD_Net.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate time (excute it twice, the first time initialize takes extra times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "y = DD_Net.predict([X_0,X_1])\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAI/CAYAAACf7mYiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n24bnetkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])\n",
    "cnf_matrix = confusion_matrix(np.argmax(Y_test,axis=1),np.argmax(Y_pred,axis=1))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cnf_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
