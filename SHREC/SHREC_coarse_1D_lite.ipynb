{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 12 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joints\n",
    "        self.clc_coarse = 14 # the number of coarse class\n",
    "        self.clc_fine = 28 # the number of fine-grained class\n",
    "        self.feat_d = 90\n",
    "        self.filters = 16\n",
    "        self.joint_ind = np.array([0,1,2,5,6,9,10,13,14,17,18,21])\n",
    "        self.data_dir = '/mnt/nasbi/homes/fan/projects/action/skeleton/data/SHREC/'\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def get_CG(p,C):\n",
    "    M = []\n",
    "    iu = np.triu_indices(C.joint_n,0,C.joint_n+1)\n",
    "    for f in range(C.frame_l):\n",
    "        #distance max \n",
    "        d_m = cdist(p[f],np.concatenate([p[f],np.zeros([1,C.joint_d])]),'euclidean')       \n",
    "        d_m = d_m[iu] \n",
    "        M.append(d_m)\n",
    "    M = np.stack(M)   \n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,:1,...],x[:,:-1,...])\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "    \n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=12,joint_d=3,feat_d=90,filters=16):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "    \n",
    "    x = block(M,filters)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    \n",
    "    x_d_slow = block(diff_slow,filters)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    \n",
    "    x_d_fast = block(diff_fast,filters)\n",
    "   \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    return Model(inputs=[M,P],outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_AR_single(frame_l=32,joint_n=22,joint_d=3,feat_d=90,clc_num=14,filters=16):\n",
    "    M = Input(name='M', shape=(frame_l,feat_d))  \n",
    "    P = Input(name='P', shape=(frame_l,joint_n,joint_d)) \n",
    "    \n",
    "    FM = build_FM(frame_l,joint_n,joint_d,feat_d,filters)\n",
    "    \n",
    "    x = FM([M,P])\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M,P],outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_single = build_AR_single(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.clc_coarse,C.filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M (InputLayer)                  (None, 32, 90)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P (InputLayer)                  (None, 32, 12, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 128)       112096      M[0][0]                          \n",
      "                                                                 P[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16384       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 128)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16384       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 128)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 14)           1806        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 147,694\n",
      "Trainable params: 146,094\n",
      "Non-trainable params: 1,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AR_single.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_single.load_weights('weights/coarse_lite.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train.pkl\", \"rb\"))\n",
    "Test = pickle.load(open(C.data_dir+\"test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normlize_range(p):\n",
    "    # normolize to start point, use the center for hand case\n",
    "    p[:,:,0] = p[:,:,0]-np.mean(p[:,:,0])\n",
    "    p[:,:,1] = p[:,:,1]-np.mean(p[:,:,1])\n",
    "    p[:,:,2] = p[:,:,2]-np.mean(p[:,:,2])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/1960 [00:00<00:11, 164.36it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 1960/1960 [00:05<00:00, 389.86it/s]\n"
     ]
    }
   ],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(Train['pose']))): \n",
    "    p = np.copy(Train['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Train['coarse_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_0.append(M)\n",
    "    X_1.append(p)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  \n",
    "X_1 = np.stack(X_1) \n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 42/840 [00:00<00:01, 413.01it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 840/840 [00:02<00:00, 413.63it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_0 = []\n",
    "X_test_1 = []\n",
    "Y_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    p = np.copy(Test['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Test['coarse_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_test_0.append(M)\n",
    "    X_test_1.append(p)\n",
    "    Y_test.append(label)\n",
    "\n",
    "X_test_0 = np.stack(X_test_0) \n",
    "X_test_1 = np.stack(X_test_1)  \n",
    "Y_test = np.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0434782608695654"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_test)/276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/300\n",
      "1960/1960 [==============================] - 5s 3ms/step - loss: 0.0256 - acc: 0.9974 - val_loss: 0.3908 - val_acc: 0.9107\n",
      "Epoch 2/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0212 - acc: 0.9985 - val_loss: 0.3903 - val_acc: 0.9119\n",
      "Epoch 3/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0301 - acc: 0.9964 - val_loss: 0.3900 - val_acc: 0.9119\n",
      "Epoch 4/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0307 - acc: 0.9944 - val_loss: 0.3899 - val_acc: 0.9119\n",
      "Epoch 5/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0264 - acc: 0.9980 - val_loss: 0.3899 - val_acc: 0.9119\n",
      "Epoch 6/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0333 - acc: 0.9934 - val_loss: 0.3903 - val_acc: 0.9107\n",
      "Epoch 7/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0275 - acc: 0.9954 - val_loss: 0.3907 - val_acc: 0.9107\n",
      "Epoch 8/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0275 - acc: 0.9944 - val_loss: 0.3909 - val_acc: 0.9107\n",
      "Epoch 9/300\n",
      "1960/1960 [==============================] - 0s 37us/step - loss: 0.0313 - acc: 0.9944 - val_loss: 0.3909 - val_acc: 0.9107\n",
      "Epoch 10/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0287 - acc: 0.9949 - val_loss: 0.3908 - val_acc: 0.9107\n",
      "Epoch 11/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0265 - acc: 0.9949 - val_loss: 0.3904 - val_acc: 0.9107\n",
      "Epoch 12/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0312 - acc: 0.9939 - val_loss: 0.3898 - val_acc: 0.9107\n",
      "Epoch 13/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0240 - acc: 0.9964 - val_loss: 0.3892 - val_acc: 0.9107\n",
      "Epoch 14/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0292 - acc: 0.9959 - val_loss: 0.3891 - val_acc: 0.9107\n",
      "Epoch 15/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0261 - acc: 0.9964 - val_loss: 0.3887 - val_acc: 0.9107\n",
      "Epoch 16/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0242 - acc: 0.9980 - val_loss: 0.3887 - val_acc: 0.9107\n",
      "Epoch 17/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0288 - acc: 0.9974 - val_loss: 0.3886 - val_acc: 0.9107\n",
      "Epoch 18/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0281 - acc: 0.9980 - val_loss: 0.3886 - val_acc: 0.9107\n",
      "Epoch 19/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0279 - acc: 0.9944 - val_loss: 0.3886 - val_acc: 0.9119\n",
      "Epoch 20/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0253 - acc: 0.9959 - val_loss: 0.3885 - val_acc: 0.9119\n",
      "Epoch 21/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0354 - acc: 0.9929 - val_loss: 0.3884 - val_acc: 0.9119\n",
      "Epoch 22/300\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0251 - acc: 0.9944 - val_loss: 0.3883 - val_acc: 0.9119\n",
      "Epoch 23/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0294 - acc: 0.9949 - val_loss: 0.3883 - val_acc: 0.9119\n",
      "Epoch 24/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0219 - acc: 0.9964 - val_loss: 0.3884 - val_acc: 0.9119\n",
      "Epoch 25/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0270 - acc: 0.9959 - val_loss: 0.3886 - val_acc: 0.9119\n",
      "Epoch 26/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0261 - acc: 0.9969 - val_loss: 0.3887 - val_acc: 0.9119\n",
      "Epoch 27/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0200 - acc: 0.9980 - val_loss: 0.3887 - val_acc: 0.9119\n",
      "Epoch 28/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0282 - acc: 0.9964 - val_loss: 0.3887 - val_acc: 0.9119\n",
      "Epoch 29/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0242 - acc: 0.9949 - val_loss: 0.3886 - val_acc: 0.9119\n",
      "Epoch 30/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0298 - acc: 0.9944 - val_loss: 0.3885 - val_acc: 0.9119\n",
      "Epoch 31/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0286 - acc: 0.9954 - val_loss: 0.3885 - val_acc: 0.9119\n",
      "Epoch 32/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0226 - acc: 0.9964 - val_loss: 0.3884 - val_acc: 0.9107\n",
      "Epoch 33/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0251 - acc: 0.9959 - val_loss: 0.3883 - val_acc: 0.9107\n",
      "Epoch 34/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0264 - acc: 0.9969 - val_loss: 0.3883 - val_acc: 0.9107\n",
      "Epoch 35/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0233 - acc: 0.9969 - val_loss: 0.3883 - val_acc: 0.9107\n",
      "Epoch 36/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0302 - acc: 0.9929 - val_loss: 0.3883 - val_acc: 0.9107\n",
      "Epoch 37/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0315 - acc: 0.9944 - val_loss: 0.3883 - val_acc: 0.9107\n",
      "Epoch 38/300\n",
      "1960/1960 [==============================] - 0s 57us/step - loss: 0.0281 - acc: 0.9954 - val_loss: 0.3882 - val_acc: 0.9107\n",
      "Epoch 39/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0288 - acc: 0.9949 - val_loss: 0.3882 - val_acc: 0.9107\n",
      "Epoch 40/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0267 - acc: 0.9969 - val_loss: 0.3883 - val_acc: 0.9107\n",
      "Epoch 41/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0221 - acc: 0.9985 - val_loss: 0.3884 - val_acc: 0.9107\n",
      "Epoch 42/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0240 - acc: 0.9985 - val_loss: 0.3885 - val_acc: 0.9119\n",
      "Epoch 43/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0218 - acc: 0.9985 - val_loss: 0.3885 - val_acc: 0.9119\n",
      "Epoch 44/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0244 - acc: 0.9969 - val_loss: 0.3886 - val_acc: 0.9119\n",
      "Epoch 45/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0254 - acc: 0.9969 - val_loss: 0.3887 - val_acc: 0.9119\n",
      "Epoch 46/300\n",
      "1960/1960 [==============================] - 0s 58us/step - loss: 0.0280 - acc: 0.9944 - val_loss: 0.3888 - val_acc: 0.9119\n",
      "Epoch 47/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0340 - acc: 0.9959 - val_loss: 0.3888 - val_acc: 0.9107\n",
      "Epoch 48/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0266 - acc: 0.9949 - val_loss: 0.3889 - val_acc: 0.9107\n",
      "Epoch 49/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0292 - acc: 0.9939 - val_loss: 0.3890 - val_acc: 0.9107\n",
      "Epoch 50/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0303 - acc: 0.9934 - val_loss: 0.3890 - val_acc: 0.9107\n",
      "Epoch 51/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0243 - acc: 0.9980 - val_loss: 0.3891 - val_acc: 0.9107\n",
      "Epoch 52/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0224 - acc: 0.9974 - val_loss: 0.3892 - val_acc: 0.9107\n",
      "Epoch 53/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0281 - acc: 0.9954 - val_loss: 0.3893 - val_acc: 0.9107\n",
      "Epoch 54/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0270 - acc: 0.9959 - val_loss: 0.3894 - val_acc: 0.9107\n",
      "Epoch 55/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0296 - acc: 0.9944 - val_loss: 0.3895 - val_acc: 0.9107\n",
      "Epoch 56/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0219 - acc: 0.9964 - val_loss: 0.3897 - val_acc: 0.9107\n",
      "Epoch 57/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0259 - acc: 0.9949 - val_loss: 0.3899 - val_acc: 0.9107\n",
      "Epoch 58/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0265 - acc: 0.9964 - val_loss: 0.3901 - val_acc: 0.9107\n",
      "Epoch 59/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0285 - acc: 0.9949 - val_loss: 0.3903 - val_acc: 0.9107\n",
      "Epoch 60/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0326 - acc: 0.9934 - val_loss: 0.3905 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300\n",
      "1960/1960 [==============================] - 0s 57us/step - loss: 0.0296 - acc: 0.9949 - val_loss: 0.3906 - val_acc: 0.9107\n",
      "Epoch 62/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0303 - acc: 0.9954 - val_loss: 0.3907 - val_acc: 0.9107\n",
      "Epoch 63/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0258 - acc: 0.9964 - val_loss: 0.3909 - val_acc: 0.9107\n",
      "Epoch 64/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0242 - acc: 0.9974 - val_loss: 0.3910 - val_acc: 0.9107\n",
      "Epoch 65/300\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0261 - acc: 0.9959 - val_loss: 0.3913 - val_acc: 0.9107\n",
      "Epoch 66/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0267 - acc: 0.9949 - val_loss: 0.3915 - val_acc: 0.9107\n",
      "Epoch 67/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0344 - acc: 0.9929 - val_loss: 0.3918 - val_acc: 0.9107\n",
      "Epoch 68/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0296 - acc: 0.9944 - val_loss: 0.3920 - val_acc: 0.9107\n",
      "Epoch 69/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0246 - acc: 0.9964 - val_loss: 0.3922 - val_acc: 0.9107\n",
      "Epoch 70/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0279 - acc: 0.9969 - val_loss: 0.3924 - val_acc: 0.9095\n",
      "Epoch 71/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0289 - acc: 0.9969 - val_loss: 0.3927 - val_acc: 0.9095\n",
      "Epoch 72/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0283 - acc: 0.9944 - val_loss: 0.3928 - val_acc: 0.9095\n",
      "Epoch 73/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0312 - acc: 0.9934 - val_loss: 0.3929 - val_acc: 0.9095\n",
      "Epoch 74/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0261 - acc: 0.9969 - val_loss: 0.3931 - val_acc: 0.9095\n",
      "Epoch 75/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0280 - acc: 0.9964 - val_loss: 0.3932 - val_acc: 0.9095\n",
      "Epoch 76/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0294 - acc: 0.9944 - val_loss: 0.3932 - val_acc: 0.9095\n",
      "Epoch 77/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0286 - acc: 0.9944 - val_loss: 0.3933 - val_acc: 0.9095\n",
      "Epoch 78/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0242 - acc: 0.9959 - val_loss: 0.3933 - val_acc: 0.9095\n",
      "Epoch 79/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0253 - acc: 0.9959 - val_loss: 0.3934 - val_acc: 0.9095\n",
      "Epoch 80/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0282 - acc: 0.9954 - val_loss: 0.3935 - val_acc: 0.9095\n",
      "Epoch 81/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0230 - acc: 0.9969 - val_loss: 0.3936 - val_acc: 0.9095\n",
      "Epoch 82/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0275 - acc: 0.9944 - val_loss: 0.3937 - val_acc: 0.9095\n",
      "Epoch 83/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0235 - acc: 0.9974 - val_loss: 0.3937 - val_acc: 0.9095\n",
      "Epoch 84/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0265 - acc: 0.9969 - val_loss: 0.3939 - val_acc: 0.9095\n",
      "Epoch 85/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0247 - acc: 0.9959 - val_loss: 0.3940 - val_acc: 0.9095\n",
      "Epoch 86/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0316 - acc: 0.9939 - val_loss: 0.3941 - val_acc: 0.9095\n",
      "Epoch 87/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0240 - acc: 0.9969 - val_loss: 0.3942 - val_acc: 0.9095\n",
      "Epoch 88/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0284 - acc: 0.9954 - val_loss: 0.3944 - val_acc: 0.9095\n",
      "Epoch 89/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0221 - acc: 0.9985 - val_loss: 0.3944 - val_acc: 0.9095\n",
      "Epoch 90/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0315 - acc: 0.9944 - val_loss: 0.3945 - val_acc: 0.9095\n",
      "Epoch 91/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0230 - acc: 0.9974 - val_loss: 0.3945 - val_acc: 0.9095\n",
      "Epoch 92/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0278 - acc: 0.9959 - val_loss: 0.3944 - val_acc: 0.9095\n",
      "Epoch 93/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0293 - acc: 0.9944 - val_loss: 0.3944 - val_acc: 0.9095\n",
      "Epoch 94/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0273 - acc: 0.9949 - val_loss: 0.3942 - val_acc: 0.9095\n",
      "Epoch 95/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0199 - acc: 0.9980 - val_loss: 0.3941 - val_acc: 0.9095\n",
      "Epoch 96/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0230 - acc: 0.9974 - val_loss: 0.3940 - val_acc: 0.9095\n",
      "Epoch 97/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0307 - acc: 0.9954 - val_loss: 0.3939 - val_acc: 0.9095\n",
      "Epoch 98/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0261 - acc: 0.9954 - val_loss: 0.3938 - val_acc: 0.9095\n",
      "Epoch 99/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0299 - acc: 0.9934 - val_loss: 0.3938 - val_acc: 0.9107\n",
      "Epoch 100/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0213 - acc: 0.9969 - val_loss: 0.3939 - val_acc: 0.9107\n",
      "Epoch 101/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0242 - acc: 0.9964 - val_loss: 0.3940 - val_acc: 0.9107\n",
      "Epoch 102/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0286 - acc: 0.9954 - val_loss: 0.3942 - val_acc: 0.9107\n",
      "Epoch 103/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0229 - acc: 0.9949 - val_loss: 0.3944 - val_acc: 0.9107\n",
      "Epoch 104/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0218 - acc: 0.9980 - val_loss: 0.3946 - val_acc: 0.9107\n",
      "Epoch 105/300\n",
      "1960/1960 [==============================] - 0s 58us/step - loss: 0.0249 - acc: 0.9959 - val_loss: 0.3947 - val_acc: 0.9107\n",
      "Epoch 106/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0252 - acc: 0.9964 - val_loss: 0.3949 - val_acc: 0.9107\n",
      "Epoch 107/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0275 - acc: 0.9964 - val_loss: 0.3950 - val_acc: 0.9107\n",
      "Epoch 108/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0232 - acc: 0.9959 - val_loss: 0.3951 - val_acc: 0.9107\n",
      "Epoch 109/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0234 - acc: 0.9959 - val_loss: 0.3951 - val_acc: 0.9107\n",
      "Epoch 110/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0234 - acc: 0.9980 - val_loss: 0.3951 - val_acc: 0.9107\n",
      "Epoch 111/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0243 - acc: 0.9959 - val_loss: 0.3952 - val_acc: 0.9107\n",
      "Epoch 112/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0292 - acc: 0.9959 - val_loss: 0.3952 - val_acc: 0.9107\n",
      "Epoch 113/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0288 - acc: 0.9969 - val_loss: 0.3952 - val_acc: 0.9119\n",
      "Epoch 114/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0266 - acc: 0.9949 - val_loss: 0.3952 - val_acc: 0.9107\n",
      "Epoch 115/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0207 - acc: 0.9980 - val_loss: 0.3952 - val_acc: 0.9107\n",
      "Epoch 116/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0267 - acc: 0.9959 - val_loss: 0.3951 - val_acc: 0.9107\n",
      "Epoch 117/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0237 - acc: 0.9974 - val_loss: 0.3950 - val_acc: 0.9107\n",
      "Epoch 118/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0335 - acc: 0.9923 - val_loss: 0.3950 - val_acc: 0.9107\n",
      "Epoch 119/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0233 - acc: 0.9964 - val_loss: 0.3951 - val_acc: 0.9095\n",
      "Epoch 120/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0236 - acc: 0.9974 - val_loss: 0.3951 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0217 - acc: 0.9969 - val_loss: 0.3951 - val_acc: 0.9095\n",
      "Epoch 122/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0256 - acc: 0.9974 - val_loss: 0.3952 - val_acc: 0.9095\n",
      "Epoch 123/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0247 - acc: 0.9969 - val_loss: 0.3953 - val_acc: 0.9095\n",
      "Epoch 124/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0257 - acc: 0.9954 - val_loss: 0.3953 - val_acc: 0.9095\n",
      "Epoch 125/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0296 - acc: 0.9939 - val_loss: 0.3954 - val_acc: 0.9095\n",
      "Epoch 126/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0220 - acc: 0.9974 - val_loss: 0.3954 - val_acc: 0.9095\n",
      "Epoch 127/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0328 - acc: 0.9944 - val_loss: 0.3954 - val_acc: 0.9095\n",
      "Epoch 128/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0205 - acc: 0.9985 - val_loss: 0.3954 - val_acc: 0.9095\n",
      "Epoch 129/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0244 - acc: 0.9964 - val_loss: 0.3955 - val_acc: 0.9095\n",
      "Epoch 130/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0248 - acc: 0.9944 - val_loss: 0.3955 - val_acc: 0.9095\n",
      "Epoch 131/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0251 - acc: 0.9959 - val_loss: 0.3955 - val_acc: 0.9095\n",
      "Epoch 132/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0276 - acc: 0.9959 - val_loss: 0.3954 - val_acc: 0.9095\n",
      "Epoch 133/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0234 - acc: 0.9985 - val_loss: 0.3953 - val_acc: 0.9095\n",
      "Epoch 134/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0289 - acc: 0.9934 - val_loss: 0.3952 - val_acc: 0.9095\n",
      "Epoch 135/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0242 - acc: 0.9974 - val_loss: 0.3950 - val_acc: 0.9095\n",
      "Epoch 136/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0251 - acc: 0.9959 - val_loss: 0.3949 - val_acc: 0.9095\n",
      "Epoch 137/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0203 - acc: 0.9985 - val_loss: 0.3947 - val_acc: 0.9095\n",
      "Epoch 138/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0261 - acc: 0.9964 - val_loss: 0.3946 - val_acc: 0.9095\n",
      "Epoch 139/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0214 - acc: 0.9974 - val_loss: 0.3944 - val_acc: 0.9095\n",
      "Epoch 140/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0242 - acc: 0.9964 - val_loss: 0.3944 - val_acc: 0.9095\n",
      "Epoch 141/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0244 - acc: 0.9964 - val_loss: 0.3943 - val_acc: 0.9095\n",
      "Epoch 142/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0218 - acc: 0.9980 - val_loss: 0.3943 - val_acc: 0.9083\n",
      "Epoch 143/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0248 - acc: 0.9959 - val_loss: 0.3942 - val_acc: 0.9083\n",
      "Epoch 144/300\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0250 - acc: 0.9964 - val_loss: 0.3940 - val_acc: 0.9083\n",
      "Epoch 145/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0278 - acc: 0.9959 - val_loss: 0.3939 - val_acc: 0.9083\n",
      "Epoch 146/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0226 - acc: 0.9969 - val_loss: 0.3937 - val_acc: 0.9083\n",
      "Epoch 147/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0259 - acc: 0.9964 - val_loss: 0.3936 - val_acc: 0.9083\n",
      "Epoch 148/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0233 - acc: 0.9969 - val_loss: 0.3934 - val_acc: 0.9083\n",
      "Epoch 149/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0263 - acc: 0.9954 - val_loss: 0.3933 - val_acc: 0.9083\n",
      "Epoch 150/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0260 - acc: 0.9964 - val_loss: 0.3932 - val_acc: 0.9095\n",
      "Epoch 151/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0254 - acc: 0.9954 - val_loss: 0.3931 - val_acc: 0.9095\n",
      "Epoch 152/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0226 - acc: 0.9964 - val_loss: 0.3929 - val_acc: 0.9095\n",
      "Epoch 153/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0271 - acc: 0.9980 - val_loss: 0.3929 - val_acc: 0.9095\n",
      "Epoch 154/300\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0225 - acc: 0.9969 - val_loss: 0.3929 - val_acc: 0.9095\n",
      "Epoch 155/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0243 - acc: 0.9964 - val_loss: 0.3929 - val_acc: 0.9095\n",
      "Epoch 156/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0246 - acc: 0.9954 - val_loss: 0.3928 - val_acc: 0.9095\n",
      "Epoch 157/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0242 - acc: 0.9954 - val_loss: 0.3927 - val_acc: 0.9095\n",
      "Epoch 158/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0281 - acc: 0.9969 - val_loss: 0.3927 - val_acc: 0.9107\n",
      "Epoch 159/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0284 - acc: 0.9949 - val_loss: 0.3926 - val_acc: 0.9107\n",
      "Epoch 160/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0202 - acc: 0.9980 - val_loss: 0.3926 - val_acc: 0.9107\n",
      "Epoch 161/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0240 - acc: 0.9959 - val_loss: 0.3926 - val_acc: 0.9107\n",
      "Epoch 162/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0293 - acc: 0.9949 - val_loss: 0.3925 - val_acc: 0.9107\n",
      "Epoch 163/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0258 - acc: 0.9959 - val_loss: 0.3924 - val_acc: 0.9107\n",
      "Epoch 164/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0260 - acc: 0.9959 - val_loss: 0.3923 - val_acc: 0.9107\n",
      "Epoch 165/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0249 - acc: 0.9959 - val_loss: 0.3923 - val_acc: 0.9095\n",
      "Epoch 166/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0275 - acc: 0.9954 - val_loss: 0.3923 - val_acc: 0.9095\n",
      "Epoch 167/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0245 - acc: 0.9969 - val_loss: 0.3923 - val_acc: 0.9095\n",
      "Epoch 168/300\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0261 - acc: 0.9934 - val_loss: 0.3923 - val_acc: 0.9107\n",
      "Epoch 169/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0239 - acc: 0.9985 - val_loss: 0.3922 - val_acc: 0.9107\n",
      "Epoch 170/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0273 - acc: 0.9959 - val_loss: 0.3923 - val_acc: 0.9107\n",
      "Epoch 171/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0252 - acc: 0.9964 - val_loss: 0.3923 - val_acc: 0.9095\n",
      "Epoch 172/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0238 - acc: 0.9969 - val_loss: 0.3924 - val_acc: 0.9095\n",
      "Epoch 173/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0229 - acc: 0.9969 - val_loss: 0.3924 - val_acc: 0.9083\n",
      "Epoch 174/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0261 - acc: 0.9959 - val_loss: 0.3924 - val_acc: 0.9083\n",
      "Epoch 175/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0251 - acc: 0.9934 - val_loss: 0.3923 - val_acc: 0.9083\n",
      "Epoch 176/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0233 - acc: 0.9985 - val_loss: 0.3923 - val_acc: 0.9083\n",
      "Epoch 177/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0237 - acc: 0.9974 - val_loss: 0.3923 - val_acc: 0.9083\n",
      "Epoch 178/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0300 - acc: 0.9954 - val_loss: 0.3922 - val_acc: 0.9083\n",
      "Epoch 179/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0249 - acc: 0.9969 - val_loss: 0.3921 - val_acc: 0.9083\n",
      "Epoch 180/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0224 - acc: 0.9980 - val_loss: 0.3920 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0229 - acc: 0.9974 - val_loss: 0.3919 - val_acc: 0.9083\n",
      "Epoch 182/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0250 - acc: 0.9959 - val_loss: 0.3919 - val_acc: 0.9083\n",
      "Epoch 183/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0250 - acc: 0.9954 - val_loss: 0.3919 - val_acc: 0.9083\n",
      "Epoch 184/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0261 - acc: 0.9969 - val_loss: 0.3918 - val_acc: 0.9083\n",
      "Epoch 185/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0269 - acc: 0.9949 - val_loss: 0.3918 - val_acc: 0.9083\n",
      "Epoch 186/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0202 - acc: 0.9995 - val_loss: 0.3919 - val_acc: 0.9083\n",
      "Epoch 187/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0244 - acc: 0.9980 - val_loss: 0.3919 - val_acc: 0.9083\n",
      "Epoch 188/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0250 - acc: 0.9969 - val_loss: 0.3921 - val_acc: 0.9083\n",
      "Epoch 189/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0258 - acc: 0.9969 - val_loss: 0.3922 - val_acc: 0.9083\n",
      "Epoch 190/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0233 - acc: 0.9964 - val_loss: 0.3923 - val_acc: 0.9083\n",
      "Epoch 191/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0238 - acc: 0.9974 - val_loss: 0.3923 - val_acc: 0.9083\n",
      "Epoch 192/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0285 - acc: 0.9959 - val_loss: 0.3925 - val_acc: 0.9083\n",
      "Epoch 193/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0287 - acc: 0.9959 - val_loss: 0.3925 - val_acc: 0.9083\n",
      "Epoch 194/300\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0243 - acc: 0.9974 - val_loss: 0.3926 - val_acc: 0.9083\n",
      "Epoch 195/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0251 - acc: 0.9974 - val_loss: 0.3926 - val_acc: 0.9083\n",
      "Epoch 196/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0209 - acc: 0.9974 - val_loss: 0.3927 - val_acc: 0.9083\n",
      "Epoch 197/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0219 - acc: 0.9969 - val_loss: 0.3928 - val_acc: 0.9083\n",
      "Epoch 198/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0248 - acc: 0.9969 - val_loss: 0.3929 - val_acc: 0.9083\n",
      "Epoch 199/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0273 - acc: 0.9954 - val_loss: 0.3931 - val_acc: 0.9083\n",
      "Epoch 200/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0241 - acc: 0.9949 - val_loss: 0.3933 - val_acc: 0.9083\n",
      "Epoch 201/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0215 - acc: 0.9964 - val_loss: 0.3934 - val_acc: 0.9083\n",
      "Epoch 202/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0259 - acc: 0.9969 - val_loss: 0.3936 - val_acc: 0.9083\n",
      "Epoch 203/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0233 - acc: 0.9969 - val_loss: 0.3939 - val_acc: 0.9083\n",
      "Epoch 204/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0241 - acc: 0.9964 - val_loss: 0.3941 - val_acc: 0.9083\n",
      "Epoch 205/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0230 - acc: 0.9959 - val_loss: 0.3945 - val_acc: 0.9083\n",
      "Epoch 206/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0236 - acc: 0.9949 - val_loss: 0.3947 - val_acc: 0.9083\n",
      "Epoch 207/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0196 - acc: 0.9985 - val_loss: 0.3949 - val_acc: 0.9083\n",
      "Epoch 208/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0242 - acc: 0.9974 - val_loss: 0.3952 - val_acc: 0.9095\n",
      "Epoch 209/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0244 - acc: 0.9969 - val_loss: 0.3954 - val_acc: 0.9095\n",
      "Epoch 210/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0217 - acc: 0.9980 - val_loss: 0.3956 - val_acc: 0.9095\n",
      "Epoch 211/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0253 - acc: 0.9964 - val_loss: 0.3957 - val_acc: 0.9095\n",
      "Epoch 212/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0218 - acc: 0.9985 - val_loss: 0.3959 - val_acc: 0.9095\n",
      "Epoch 213/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0229 - acc: 0.9964 - val_loss: 0.3960 - val_acc: 0.9095\n",
      "Epoch 214/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0255 - acc: 0.9974 - val_loss: 0.3962 - val_acc: 0.9095\n",
      "Epoch 215/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0290 - acc: 0.9944 - val_loss: 0.3964 - val_acc: 0.9095\n",
      "Epoch 216/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0239 - acc: 0.9974 - val_loss: 0.3965 - val_acc: 0.9083\n",
      "Epoch 217/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0257 - acc: 0.9954 - val_loss: 0.3967 - val_acc: 0.9083\n",
      "Epoch 218/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0238 - acc: 0.9959 - val_loss: 0.3967 - val_acc: 0.9083\n",
      "Epoch 219/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0274 - acc: 0.9944 - val_loss: 0.3968 - val_acc: 0.9083\n",
      "Epoch 220/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0237 - acc: 0.9964 - val_loss: 0.3969 - val_acc: 0.9095\n",
      "Epoch 221/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0213 - acc: 0.9974 - val_loss: 0.3971 - val_acc: 0.9095\n",
      "Epoch 222/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0245 - acc: 0.9985 - val_loss: 0.3972 - val_acc: 0.9095\n",
      "Epoch 223/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0201 - acc: 0.9980 - val_loss: 0.3973 - val_acc: 0.9095\n",
      "Epoch 224/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0239 - acc: 0.9974 - val_loss: 0.3973 - val_acc: 0.9095\n",
      "Epoch 225/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0227 - acc: 0.9980 - val_loss: 0.3974 - val_acc: 0.9095\n",
      "Epoch 226/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0249 - acc: 0.9974 - val_loss: 0.3974 - val_acc: 0.9095\n",
      "Epoch 227/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0246 - acc: 0.9954 - val_loss: 0.3975 - val_acc: 0.9095\n",
      "Epoch 228/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0257 - acc: 0.9959 - val_loss: 0.3975 - val_acc: 0.9095\n",
      "Epoch 229/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0236 - acc: 0.9969 - val_loss: 0.3975 - val_acc: 0.9095\n",
      "Epoch 230/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0297 - acc: 0.9954 - val_loss: 0.3974 - val_acc: 0.9095\n",
      "Epoch 231/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0245 - acc: 0.9959 - val_loss: 0.3974 - val_acc: 0.9095\n",
      "Epoch 232/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0265 - acc: 0.9964 - val_loss: 0.3973 - val_acc: 0.9095\n",
      "Epoch 233/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0238 - acc: 0.9985 - val_loss: 0.3972 - val_acc: 0.9095\n",
      "Epoch 234/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0297 - acc: 0.9949 - val_loss: 0.3973 - val_acc: 0.9107\n",
      "Epoch 235/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0235 - acc: 0.9969 - val_loss: 0.3972 - val_acc: 0.9107\n",
      "Epoch 236/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0227 - acc: 0.9980 - val_loss: 0.3973 - val_acc: 0.9107\n",
      "Epoch 237/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0216 - acc: 0.9980 - val_loss: 0.3974 - val_acc: 0.9107\n",
      "Epoch 238/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0267 - acc: 0.9939 - val_loss: 0.3975 - val_acc: 0.9107\n",
      "Epoch 239/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0248 - acc: 0.9954 - val_loss: 0.3976 - val_acc: 0.9107\n",
      "Epoch 240/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0289 - acc: 0.9949 - val_loss: 0.3977 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0221 - acc: 0.9974 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 242/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0271 - acc: 0.9959 - val_loss: 0.3979 - val_acc: 0.9107\n",
      "Epoch 243/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0247 - acc: 0.9939 - val_loss: 0.3979 - val_acc: 0.9107\n",
      "Epoch 244/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0265 - acc: 0.9949 - val_loss: 0.3979 - val_acc: 0.9107\n",
      "Epoch 245/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0251 - acc: 0.9959 - val_loss: 0.3979 - val_acc: 0.9107\n",
      "Epoch 246/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0269 - acc: 0.9969 - val_loss: 0.3979 - val_acc: 0.9107\n",
      "Epoch 247/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0220 - acc: 0.9974 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 248/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0208 - acc: 0.9980 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 249/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0237 - acc: 0.9954 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 250/300\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0243 - acc: 0.9964 - val_loss: 0.3977 - val_acc: 0.9107\n",
      "Epoch 251/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0261 - acc: 0.9964 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 252/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0214 - acc: 0.9985 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 253/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0238 - acc: 0.9964 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 254/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0255 - acc: 0.9944 - val_loss: 0.3977 - val_acc: 0.9107\n",
      "Epoch 255/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0249 - acc: 0.9985 - val_loss: 0.3977 - val_acc: 0.9107\n",
      "Epoch 256/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0249 - acc: 0.9959 - val_loss: 0.3977 - val_acc: 0.9107\n",
      "Epoch 257/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0216 - acc: 0.9974 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 258/300\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0263 - acc: 0.9974 - val_loss: 0.3980 - val_acc: 0.9107\n",
      "Epoch 259/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0205 - acc: 0.9980 - val_loss: 0.3981 - val_acc: 0.9107\n",
      "Epoch 260/300\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0250 - acc: 0.9949 - val_loss: 0.3982 - val_acc: 0.9107\n",
      "Epoch 261/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0207 - acc: 0.9964 - val_loss: 0.3982 - val_acc: 0.9107\n",
      "Epoch 262/300\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0242 - acc: 0.9959 - val_loss: 0.3981 - val_acc: 0.9107\n",
      "Epoch 263/300\n",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.0230 - acc: 0.9974 - val_loss: 0.3981 - val_acc: 0.9107\n",
      "Epoch 264/300\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0247 - acc: 0.9959 - val_loss: 0.3981 - val_acc: 0.9107\n",
      "Epoch 265/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0228 - acc: 0.9969 - val_loss: 0.3980 - val_acc: 0.9107\n",
      "Epoch 266/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0215 - acc: 0.9974 - val_loss: 0.3979 - val_acc: 0.9107\n",
      "Epoch 267/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0204 - acc: 0.9985 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 268/300\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0226 - acc: 0.9964 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 269/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0221 - acc: 0.9959 - val_loss: 0.3979 - val_acc: 0.9107\n",
      "Epoch 270/300\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0210 - acc: 0.9974 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 271/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0273 - acc: 0.9954 - val_loss: 0.3978 - val_acc: 0.9107\n",
      "Epoch 272/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0282 - acc: 0.9949 - val_loss: 0.3977 - val_acc: 0.9107\n",
      "Epoch 273/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0256 - acc: 0.9944 - val_loss: 0.3976 - val_acc: 0.9107\n",
      "Epoch 274/300\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0190 - acc: 0.9990 - val_loss: 0.3976 - val_acc: 0.9107\n",
      "Epoch 275/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0238 - acc: 0.9969 - val_loss: 0.3976 - val_acc: 0.9107\n",
      "Epoch 276/300\n",
      "1960/1960 [==============================] - 0s 56us/step - loss: 0.0260 - acc: 0.9934 - val_loss: 0.3975 - val_acc: 0.9107\n",
      "Epoch 277/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0263 - acc: 0.9939 - val_loss: 0.3973 - val_acc: 0.9107\n",
      "Epoch 278/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0225 - acc: 0.9974 - val_loss: 0.3972 - val_acc: 0.9107\n",
      "Epoch 279/300\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0226 - acc: 0.9985 - val_loss: 0.3971 - val_acc: 0.9107\n",
      "Epoch 280/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0233 - acc: 0.9959 - val_loss: 0.3970 - val_acc: 0.9107\n",
      "Epoch 281/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0222 - acc: 0.9985 - val_loss: 0.3969 - val_acc: 0.9107\n",
      "Epoch 282/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0195 - acc: 0.9974 - val_loss: 0.3969 - val_acc: 0.9107\n",
      "Epoch 283/300\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0196 - acc: 0.9980 - val_loss: 0.3968 - val_acc: 0.9107\n",
      "Epoch 284/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0217 - acc: 0.9959 - val_loss: 0.3967 - val_acc: 0.9107\n",
      "Epoch 285/300\n",
      "1960/1960 [==============================] - 0s 57us/step - loss: 0.0250 - acc: 0.9939 - val_loss: 0.3966 - val_acc: 0.9107\n",
      "Epoch 286/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0264 - acc: 0.9954 - val_loss: 0.3966 - val_acc: 0.9107\n",
      "Epoch 287/300\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0244 - acc: 0.9954 - val_loss: 0.3965 - val_acc: 0.9107\n",
      "Epoch 288/300\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0271 - acc: 0.9949 - val_loss: 0.3965 - val_acc: 0.9107\n",
      "Epoch 289/300\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0250 - acc: 0.9959 - val_loss: 0.3963 - val_acc: 0.9107\n",
      "Epoch 290/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0213 - acc: 0.9969 - val_loss: 0.3962 - val_acc: 0.9107\n",
      "Epoch 291/300\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0239 - acc: 0.9964 - val_loss: 0.3962 - val_acc: 0.9107\n",
      "Epoch 292/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0231 - acc: 0.9964 - val_loss: 0.3961 - val_acc: 0.9107\n",
      "Epoch 293/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0253 - acc: 0.9964 - val_loss: 0.3960 - val_acc: 0.9107\n",
      "Epoch 294/300\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0263 - acc: 0.9969 - val_loss: 0.3959 - val_acc: 0.9107\n",
      "Epoch 295/300\n",
      "1960/1960 [==============================] - 0s 58us/step - loss: 0.0299 - acc: 0.9944 - val_loss: 0.3959 - val_acc: 0.9107\n",
      "Epoch 296/300\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0202 - acc: 0.9980 - val_loss: 0.3959 - val_acc: 0.9107\n",
      "Epoch 297/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0235 - acc: 0.9974 - val_loss: 0.3959 - val_acc: 0.9107\n",
      "Epoch 298/300\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0232 - acc: 0.9974 - val_loss: 0.3959 - val_acc: 0.9107\n",
      "Epoch 299/300\n",
      "1960/1960 [==============================] - 0s 55us/step - loss: 0.0288 - acc: 0.9964 - val_loss: 0.3959 - val_acc: 0.9107\n",
      "Epoch 300/300\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0263 - acc: 0.9949 - val_loss: 0.3959 - val_acc: 0.9107\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "lr = 1e-3\n",
    "AR_single.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=2e-5)\n",
    "history = AR_single.fit([X_0,X_1],Y,\n",
    "        batch_size=len(Y),\n",
    "        epochs=300,\n",
    "        verbose=True,\n",
    "        shuffle=True,\n",
    "        callbacks=[lrScheduler],\n",
    "        validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AR_single.save_weights('weights/coarse_lite.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUsoRTRhKKaUc0YShlFLKkf8HSO+PQuq56a0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84f0384ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_frame(p,C):\n",
    "    full_l = p.shape[0] # full length\n",
    "    if random.uniform(0,1)<0.5: # aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.9,1)*full_l)\n",
    "        s = random.randint(0, full_l-int(valid_l))\n",
    "        e = s+valid_l # sample end point\n",
    "        p = p[int(s):int(e),:,:]    \n",
    "    else: # without aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.9,1)*full_l)\n",
    "        index = np.sort(np.random.choice(range(0,full_l),int(valid_l),replace=False))\n",
    "        p = p[index,:,:]\n",
    "    p = zoom(p,C.frame_l,C.joint_n,C.joint_d)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "AR_single.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 6s 3ms/step - loss: 0.0866 - acc: 0.9796 - val_loss: 0.3958 - val_acc: 0.9107\n",
      "epoch1\n",
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0959 - acc: 0.9765 - val_loss: 0.3958 - val_acc: 0.9119\n",
      "epoch2\n",
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0954 - acc: 0.9765 - val_loss: 0.3958 - val_acc: 0.9119\n",
      "epoch3\n",
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0949 - acc: 0.9776 - val_loss: 0.3960 - val_acc: 0.9119\n",
      "epoch4\n",
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0877 - acc: 0.9765 - val_loss: 0.3963 - val_acc: 0.9119\n",
      "epoch5\n",
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.1046 - acc: 0.9719 - val_loss: 0.3964 - val_acc: 0.9119\n",
      "epoch6\n",
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0877 - acc: 0.9760 - val_loss: 0.3964 - val_acc: 0.9119\n",
      "epoch7\n",
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0810 - acc: 0.9770 - val_loss: 0.3965 - val_acc: 0.9107\n",
      "epoch8\n",
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.1090 - acc: 0.9704 - val_loss: 0.3965 - val_acc: 0.9107\n",
      "epoch9\n",
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 0s 37us/step - loss: 0.0790 - acc: 0.9776 - val_loss: 0.3965 - val_acc: 0.9107\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    Y = []\n",
    "    \n",
    "    #for i in tqdm(range(len(Train['pose']))): \n",
    "    for i in range(len(Train['pose'])): \n",
    "    \n",
    "        label = np.zeros(C.clc_coarse)\n",
    "        label[Train['coarse_label'][i]-1] = 1 \n",
    "        \n",
    "        p = np.copy(Train['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "        p = sampling_frame(p,C)\n",
    "        \n",
    "        #rotation\n",
    "        x_angle = np.random.uniform(-0.2,0.2)\n",
    "        y_angle = np.random.uniform(-0.2,0.2)\n",
    "        z_angle = np.random.uniform(-0.2,0.2)\n",
    "        R = euler2mat(x_angle, y_angle, z_angle, 'sxyz')\n",
    "        p = rotaion_one(p,R)\n",
    "        \n",
    "        p = normlize_range(p)\n",
    "  \n",
    "        p[:,:,0] = p[:,:,0]*random.uniform(0.9, 1.1)+p[:,:,0]*random.uniform(-0.1,0.1)\n",
    "        p[:,:,1] = p[:,:,1]*random.uniform(0.9, 1.1)+p[:,:,1]*random.uniform(-0.1,0.1)\n",
    "        p[:,:,2] = p[:,:,2]*random.uniform(0.9, 1.1)+p[:,:,2]*random.uniform(-0.1,0.1)\n",
    "        \n",
    "        M = get_CG(p,C)\n",
    "        \n",
    "        X_0.append(M)\n",
    "        X_1.append(p)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    Y = np.stack(Y)\n",
    "   \n",
    "\n",
    "    AR_single.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )\n",
    "\n",
    "    if e%10==0:\n",
    "        AR_single.save_weights('weights/coarse_lite_aug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkUAAAJCCAYAAADOe7N5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84f02f0c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = AR_single.predict([X_test_0,X_test_1])\n",
    "cnf_matrix = confusion_matrix(np.argmax(Y_test,axis=1),np.argmax(Y_pred,axis=1))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cnf_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
