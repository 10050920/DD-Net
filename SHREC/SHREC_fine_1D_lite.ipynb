{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 12 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joints\n",
    "        self.clc_coarse = 14 # the number of coarse class\n",
    "        self.clc_fine = 28 # the number of fine-grained class\n",
    "        self.feat_d = 66\n",
    "        self.filters = 16\n",
    "        self.joint_ind = np.array([0,1,2,5,6,9,10,13,14,17,18,21])\n",
    "        self.data_dir = '/mnt/nasbi/homes/fan/projects/action/skeleton/data/SHREC/'\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,:1,...],x[:,:-1,...])\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "    \n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=12,joint_d=3,feat_d=90,filters=16):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "    \n",
    "    x = block(M,filters)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    \n",
    "    x_d_slow = block(diff_slow,filters)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    \n",
    "    x_d_fast = block(diff_fast,filters)\n",
    "   \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    return Model(inputs=[M,P],outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_AR_single(frame_l=32,joint_n=22,joint_d=3,feat_d=66,clc_num=14,filters=16):\n",
    "    M = Input(name='M', shape=(frame_l,feat_d))  \n",
    "    P = Input(name='P', shape=(frame_l,joint_n,joint_d)) \n",
    "    \n",
    "    FM = build_FM(frame_l,joint_n,joint_d,feat_d,filters)\n",
    "    \n",
    "    x = FM([M,P])\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M,P],outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_single = build_AR_single(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.clc_fine,C.filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M (InputLayer)                  (None, 32, 90)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P (InputLayer)                  (None, 32, 12, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 128)       112096      M[0][0]                          \n",
      "                                                                 P[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16384       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 128)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16384       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 128)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 28)           3612        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 149,500\n",
      "Trainable params: 147,900\n",
      "Non-trainable params: 1,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AR_single.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_single.load_weights('weights/fine_lite.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train.pkl\", \"rb\"))\n",
    "Test = pickle.load(open(C.data_dir+\"test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normlize_range(p):\n",
    "    # normolize to start point, use the center for hand case\n",
    "    p[:,:,0] = p[:,:,0]-np.mean(p[:,:,0])\n",
    "    p[:,:,1] = p[:,:,1]-np.mean(p[:,:,1])\n",
    "    p[:,:,2] = p[:,:,2]-np.mean(p[:,:,2])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/1960 [00:00<00:09, 208.76it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 1960/1960 [00:04<00:00, 413.42it/s]\n"
     ]
    }
   ],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(Train['pose']))): \n",
    "    p = np.copy(Train['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_fine)\n",
    "    label[Train['fine_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_0.append(M)\n",
    "    X_1.append(p)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  \n",
    "X_1 = np.stack(X_1) \n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 43/840 [00:00<00:01, 428.55it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 840/840 [00:01<00:00, 430.92it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_0 = []\n",
    "X_test_1 = []\n",
    "Y_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    p = np.copy(Test['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_fine)\n",
    "    label[Test['fine_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_test_0.append(M)\n",
    "    X_test_1.append(p)\n",
    "    Y_test.append(label)\n",
    "\n",
    "X_test_0 = np.stack(X_test_0) \n",
    "X_test_1 = np.stack(X_test_1)  \n",
    "Y_test = np.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/400\n",
      "1960/1960 [==============================] - 7s 3ms/step - loss: 3.8661 - acc: 0.0281 - val_loss: 3.2134 - val_acc: 0.0750\n",
      "Epoch 2/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 3.6372 - acc: 0.0454 - val_loss: 3.0647 - val_acc: 0.1274\n",
      "Epoch 3/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 3.4359 - acc: 0.0689 - val_loss: 2.9463 - val_acc: 0.1595\n",
      "Epoch 4/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 3.2918 - acc: 0.0918 - val_loss: 2.8510 - val_acc: 0.1762\n",
      "Epoch 5/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 3.1927 - acc: 0.1077 - val_loss: 2.7632 - val_acc: 0.2083\n",
      "Epoch 6/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 3.0776 - acc: 0.1240 - val_loss: 2.6663 - val_acc: 0.2333\n",
      "Epoch 7/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 2.9831 - acc: 0.1413 - val_loss: 2.5644 - val_acc: 0.2726\n",
      "Epoch 8/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 2.8247 - acc: 0.1883 - val_loss: 2.4473 - val_acc: 0.3143\n",
      "Epoch 9/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 2.7635 - acc: 0.1913 - val_loss: 2.3307 - val_acc: 0.3464\n",
      "Epoch 10/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 2.6888 - acc: 0.2184 - val_loss: 2.2125 - val_acc: 0.3750\n",
      "Epoch 11/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 2.5846 - acc: 0.2444 - val_loss: 2.1113 - val_acc: 0.3869\n",
      "Epoch 12/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 2.5314 - acc: 0.2551 - val_loss: 2.0350 - val_acc: 0.3976\n",
      "Epoch 13/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 2.4116 - acc: 0.3056 - val_loss: 1.9792 - val_acc: 0.4012\n",
      "Epoch 14/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 2.3478 - acc: 0.3163 - val_loss: 1.9389 - val_acc: 0.4155\n",
      "Epoch 15/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 2.2816 - acc: 0.3378 - val_loss: 1.9017 - val_acc: 0.4179\n",
      "Epoch 16/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 2.2212 - acc: 0.3551 - val_loss: 1.8677 - val_acc: 0.4238\n",
      "Epoch 17/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 2.1421 - acc: 0.3781 - val_loss: 1.8417 - val_acc: 0.4226\n",
      "Epoch 18/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 2.0621 - acc: 0.3959 - val_loss: 1.8185 - val_acc: 0.4250\n",
      "Epoch 19/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 2.0021 - acc: 0.4209 - val_loss: 1.8040 - val_acc: 0.4226\n",
      "Epoch 20/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 1.9531 - acc: 0.4148 - val_loss: 1.8070 - val_acc: 0.4155\n",
      "Epoch 21/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 1.8826 - acc: 0.4383 - val_loss: 1.8262 - val_acc: 0.4107\n",
      "Epoch 22/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 1.8126 - acc: 0.4770 - val_loss: 1.8537 - val_acc: 0.3881\n",
      "Epoch 23/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 1.7824 - acc: 0.4796 - val_loss: 1.8757 - val_acc: 0.3857\n",
      "Epoch 24/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 1.7370 - acc: 0.5087 - val_loss: 1.8846 - val_acc: 0.3798\n",
      "Epoch 25/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 1.6769 - acc: 0.5245 - val_loss: 1.8873 - val_acc: 0.3679\n",
      "Epoch 26/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 1.6466 - acc: 0.5219 - val_loss: 1.8830 - val_acc: 0.3667\n",
      "Epoch 27/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 1.6149 - acc: 0.5270 - val_loss: 1.8818 - val_acc: 0.3655\n",
      "Epoch 28/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 1.5518 - acc: 0.5515 - val_loss: 1.8584 - val_acc: 0.3774\n",
      "Epoch 29/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 1.5029 - acc: 0.5821 - val_loss: 1.8268 - val_acc: 0.3821\n",
      "Epoch 30/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 1.4533 - acc: 0.5888 - val_loss: 1.7998 - val_acc: 0.3952\n",
      "Epoch 31/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 1.4344 - acc: 0.5980 - val_loss: 1.7824 - val_acc: 0.3917\n",
      "Epoch 32/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 1.3706 - acc: 0.6051 - val_loss: 1.7759 - val_acc: 0.3917\n",
      "Epoch 33/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 1.3401 - acc: 0.6224 - val_loss: 1.8008 - val_acc: 0.3940\n",
      "Epoch 34/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 1.3353 - acc: 0.6199 - val_loss: 1.8330 - val_acc: 0.3893\n",
      "Epoch 35/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 1.2730 - acc: 0.6423 - val_loss: 1.8611 - val_acc: 0.3857\n",
      "Epoch 36/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 1.2219 - acc: 0.6628 - val_loss: 1.8787 - val_acc: 0.3893\n",
      "Epoch 37/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 1.1976 - acc: 0.6796 - val_loss: 1.8982 - val_acc: 0.3952\n",
      "Epoch 38/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 1.1842 - acc: 0.6714 - val_loss: 1.9019 - val_acc: 0.3964\n",
      "Epoch 39/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 1.1169 - acc: 0.7020 - val_loss: 1.9078 - val_acc: 0.3976\n",
      "Epoch 40/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 1.1215 - acc: 0.6969 - val_loss: 1.8805 - val_acc: 0.4012\n",
      "Epoch 41/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 1.0777 - acc: 0.7102 - val_loss: 1.8394 - val_acc: 0.4155\n",
      "Epoch 42/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 1.0619 - acc: 0.7199 - val_loss: 1.8127 - val_acc: 0.4190\n",
      "Epoch 43/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 1.0246 - acc: 0.7163 - val_loss: 1.7540 - val_acc: 0.4274\n",
      "Epoch 44/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 1.0060 - acc: 0.7281 - val_loss: 1.7231 - val_acc: 0.4345\n",
      "Epoch 45/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.9633 - acc: 0.7423 - val_loss: 1.7518 - val_acc: 0.4274\n",
      "Epoch 46/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.9568 - acc: 0.7536 - val_loss: 1.8028 - val_acc: 0.4274\n",
      "Epoch 47/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.9469 - acc: 0.7505 - val_loss: 1.8558 - val_acc: 0.4202\n",
      "Epoch 48/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.9253 - acc: 0.7444 - val_loss: 1.8686 - val_acc: 0.4190\n",
      "Epoch 49/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.8987 - acc: 0.7694 - val_loss: 1.8583 - val_acc: 0.4298\n",
      "Epoch 50/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.8867 - acc: 0.7638 - val_loss: 1.8190 - val_acc: 0.4310\n",
      "Epoch 51/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.8352 - acc: 0.7883 - val_loss: 1.7718 - val_acc: 0.4345\n",
      "Epoch 52/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.8240 - acc: 0.7862 - val_loss: 1.7318 - val_acc: 0.4488\n",
      "Epoch 53/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.8088 - acc: 0.7908 - val_loss: 1.6777 - val_acc: 0.4548\n",
      "Epoch 54/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.7886 - acc: 0.8026 - val_loss: 1.6458 - val_acc: 0.4512\n",
      "Epoch 55/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.7853 - acc: 0.7913 - val_loss: 1.6155 - val_acc: 0.4536\n",
      "Epoch 56/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.7527 - acc: 0.8005 - val_loss: 1.6040 - val_acc: 0.4595\n",
      "Epoch 57/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.7240 - acc: 0.8133 - val_loss: 1.6011 - val_acc: 0.4643\n",
      "Epoch 58/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.6944 - acc: 0.8240 - val_loss: 1.6022 - val_acc: 0.4655\n",
      "Epoch 59/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.6999 - acc: 0.8235 - val_loss: 1.5935 - val_acc: 0.4690\n",
      "Epoch 60/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.6688 - acc: 0.8291 - val_loss: 1.5800 - val_acc: 0.4714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.6699 - acc: 0.8367 - val_loss: 1.5656 - val_acc: 0.4714\n",
      "Epoch 62/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.6585 - acc: 0.8367 - val_loss: 1.5508 - val_acc: 0.4774\n",
      "Epoch 63/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.6356 - acc: 0.8459 - val_loss: 1.5625 - val_acc: 0.4774\n",
      "Epoch 64/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.6180 - acc: 0.8418 - val_loss: 1.5878 - val_acc: 0.4762\n",
      "Epoch 65/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.5996 - acc: 0.8464 - val_loss: 1.6558 - val_acc: 0.4643\n",
      "Epoch 66/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.6082 - acc: 0.8388 - val_loss: 1.7372 - val_acc: 0.4548\n",
      "Epoch 67/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.5792 - acc: 0.8566 - val_loss: 1.8263 - val_acc: 0.4357\n",
      "Epoch 68/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.5585 - acc: 0.8602 - val_loss: 1.8905 - val_acc: 0.4286\n",
      "Epoch 69/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.5511 - acc: 0.8556 - val_loss: 1.9455 - val_acc: 0.4238\n",
      "Epoch 70/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.5599 - acc: 0.8663 - val_loss: 1.9431 - val_acc: 0.4369\n",
      "Epoch 71/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.5352 - acc: 0.8694 - val_loss: 1.9672 - val_acc: 0.4417\n",
      "Epoch 72/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.5201 - acc: 0.8689 - val_loss: 1.9609 - val_acc: 0.4429\n",
      "Epoch 73/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.5110 - acc: 0.8827 - val_loss: 1.9309 - val_acc: 0.4440\n",
      "Epoch 74/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.5001 - acc: 0.8755 - val_loss: 1.9257 - val_acc: 0.4429\n",
      "Epoch 75/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.5037 - acc: 0.8760 - val_loss: 1.9178 - val_acc: 0.4429\n",
      "Epoch 76/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.4783 - acc: 0.8791 - val_loss: 1.9397 - val_acc: 0.4417\n",
      "Epoch 77/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.4621 - acc: 0.8878 - val_loss: 1.9199 - val_acc: 0.4393\n",
      "Epoch 78/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.4728 - acc: 0.8857 - val_loss: 1.9167 - val_acc: 0.4405\n",
      "Epoch 79/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.4714 - acc: 0.8816 - val_loss: 1.8913 - val_acc: 0.4464\n",
      "Epoch 80/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.4353 - acc: 0.8934 - val_loss: 1.8744 - val_acc: 0.4512\n",
      "Epoch 81/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.4207 - acc: 0.9026 - val_loss: 1.8722 - val_acc: 0.4476\n",
      "Epoch 82/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.4042 - acc: 0.9077 - val_loss: 1.8830 - val_acc: 0.4476\n",
      "Epoch 83/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.4132 - acc: 0.9056 - val_loss: 1.9559 - val_acc: 0.4405\n",
      "Epoch 84/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.4043 - acc: 0.9066 - val_loss: 2.0008 - val_acc: 0.4393\n",
      "Epoch 85/400\n",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.3963 - acc: 0.9020 - val_loss: 2.0441 - val_acc: 0.4357\n",
      "Epoch 86/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.3886 - acc: 0.9036 - val_loss: 2.1037 - val_acc: 0.4333\n",
      "Epoch 87/400\n",
      "1960/1960 [==============================] - 0s 37us/step - loss: 0.3840 - acc: 0.9056 - val_loss: 2.1280 - val_acc: 0.4345\n",
      "Epoch 88/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.3791 - acc: 0.9102 - val_loss: 2.1319 - val_acc: 0.4369\n",
      "Epoch 89/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.3638 - acc: 0.9153 - val_loss: 2.1239 - val_acc: 0.4357\n",
      "Epoch 90/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.3384 - acc: 0.9250 - val_loss: 2.1499 - val_acc: 0.4357\n",
      "Epoch 91/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.3484 - acc: 0.9240 - val_loss: 2.1750 - val_acc: 0.4321\n",
      "Epoch 92/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.3560 - acc: 0.9122 - val_loss: 2.1505 - val_acc: 0.4393\n",
      "Epoch 93/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.3412 - acc: 0.9209 - val_loss: 2.0895 - val_acc: 0.4417\n",
      "Epoch 94/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.3188 - acc: 0.9199 - val_loss: 2.0237 - val_acc: 0.4452\n",
      "Epoch 95/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.3322 - acc: 0.9184 - val_loss: 1.9451 - val_acc: 0.4512\n",
      "Epoch 96/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.3079 - acc: 0.9332 - val_loss: 1.9274 - val_acc: 0.4488\n",
      "Epoch 97/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.3093 - acc: 0.9291 - val_loss: 1.9339 - val_acc: 0.4524\n",
      "Epoch 98/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.3004 - acc: 0.9270 - val_loss: 1.9868 - val_acc: 0.4464\n",
      "Epoch 99/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.3044 - acc: 0.9224 - val_loss: 2.0297 - val_acc: 0.4464\n",
      "Epoch 100/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.2814 - acc: 0.9357 - val_loss: 2.0755 - val_acc: 0.4452\n",
      "Epoch 101/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.2965 - acc: 0.9332 - val_loss: 2.1472 - val_acc: 0.4417\n",
      "Epoch 102/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.2967 - acc: 0.9301 - val_loss: 2.1346 - val_acc: 0.4440\n",
      "Epoch 103/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.2776 - acc: 0.9342 - val_loss: 2.1088 - val_acc: 0.4488\n",
      "Epoch 104/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.2809 - acc: 0.9321 - val_loss: 2.1353 - val_acc: 0.4452\n",
      "Epoch 105/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.2633 - acc: 0.9459 - val_loss: 2.1708 - val_acc: 0.4417\n",
      "Epoch 106/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.2564 - acc: 0.9429 - val_loss: 2.1797 - val_acc: 0.4405\n",
      "Epoch 107/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.2503 - acc: 0.9485 - val_loss: 2.1019 - val_acc: 0.4512\n",
      "Epoch 108/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.2525 - acc: 0.9403 - val_loss: 1.9528 - val_acc: 0.4500\n",
      "Epoch 109/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.2441 - acc: 0.9418 - val_loss: 1.8439 - val_acc: 0.4607\n",
      "Epoch 110/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.2625 - acc: 0.9398 - val_loss: 1.8278 - val_acc: 0.4607\n",
      "Epoch 111/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.2294 - acc: 0.9541 - val_loss: 1.8539 - val_acc: 0.4667\n",
      "Epoch 112/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.2481 - acc: 0.9444 - val_loss: 1.8818 - val_acc: 0.4667\n",
      "Epoch 113/400\n",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.2435 - acc: 0.9439 - val_loss: 1.8885 - val_acc: 0.4726\n",
      "Epoch 114/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.2422 - acc: 0.9439 - val_loss: 1.8686 - val_acc: 0.4774\n",
      "Epoch 115/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.2247 - acc: 0.9531 - val_loss: 1.8895 - val_acc: 0.4762\n",
      "Epoch 116/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.2115 - acc: 0.9602 - val_loss: 1.9803 - val_acc: 0.4667\n",
      "Epoch 117/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.2181 - acc: 0.9561 - val_loss: 2.0645 - val_acc: 0.4643\n",
      "Epoch 118/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.2119 - acc: 0.9546 - val_loss: 1.9569 - val_acc: 0.4726\n",
      "Epoch 119/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.2040 - acc: 0.9566 - val_loss: 1.9191 - val_acc: 0.4750\n",
      "Epoch 120/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.1958 - acc: 0.9628 - val_loss: 1.8931 - val_acc: 0.4845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.1830 - acc: 0.9648 - val_loss: 1.8988 - val_acc: 0.4810\n",
      "Epoch 122/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.2002 - acc: 0.9582 - val_loss: 1.8666 - val_acc: 0.4810\n",
      "Epoch 123/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.1947 - acc: 0.9541 - val_loss: 1.8684 - val_acc: 0.4786\n",
      "Epoch 124/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.1917 - acc: 0.9602 - val_loss: 1.8368 - val_acc: 0.4798\n",
      "Epoch 125/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.1776 - acc: 0.9679 - val_loss: 1.7649 - val_acc: 0.4869\n",
      "Epoch 126/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.1749 - acc: 0.9704 - val_loss: 1.8003 - val_acc: 0.4821\n",
      "Epoch 127/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.1832 - acc: 0.9628 - val_loss: 1.8535 - val_acc: 0.4810\n",
      "Epoch 128/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.1707 - acc: 0.9658 - val_loss: 1.8954 - val_acc: 0.4762\n",
      "Epoch 129/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.1831 - acc: 0.9597 - val_loss: 2.0114 - val_acc: 0.4667\n",
      "Epoch 130/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.1652 - acc: 0.9684 - val_loss: 2.0227 - val_acc: 0.4690\n",
      "Epoch 131/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.1629 - acc: 0.9689 - val_loss: 1.8809 - val_acc: 0.4869\n",
      "Epoch 132/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.1680 - acc: 0.9689 - val_loss: 1.7777 - val_acc: 0.5131\n",
      "Epoch 133/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.1492 - acc: 0.9740 - val_loss: 1.7536 - val_acc: 0.5190\n",
      "Epoch 134/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.1534 - acc: 0.9673 - val_loss: 1.7724 - val_acc: 0.5095\n",
      "Epoch 135/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.1663 - acc: 0.9653 - val_loss: 1.6296 - val_acc: 0.5345\n",
      "Epoch 136/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.1521 - acc: 0.9735 - val_loss: 1.5582 - val_acc: 0.5429\n",
      "Epoch 137/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.1544 - acc: 0.9694 - val_loss: 1.4819 - val_acc: 0.5702\n",
      "Epoch 138/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.1486 - acc: 0.9673 - val_loss: 1.5071 - val_acc: 0.5702\n",
      "Epoch 139/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.1429 - acc: 0.9755 - val_loss: 1.5436 - val_acc: 0.5607\n",
      "Epoch 140/400\n",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.1424 - acc: 0.9740 - val_loss: 1.6920 - val_acc: 0.5381\n",
      "Epoch 141/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.1303 - acc: 0.9786 - val_loss: 1.6355 - val_acc: 0.5417\n",
      "Epoch 142/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.1370 - acc: 0.9735 - val_loss: 1.6230 - val_acc: 0.5381\n",
      "Epoch 143/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.1394 - acc: 0.9704 - val_loss: 1.6247 - val_acc: 0.5286\n",
      "Epoch 144/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.1440 - acc: 0.9714 - val_loss: 1.7634 - val_acc: 0.5107\n",
      "Epoch 145/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.1361 - acc: 0.9755 - val_loss: 1.9405 - val_acc: 0.4893\n",
      "Epoch 146/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.1353 - acc: 0.9791 - val_loss: 1.9921 - val_acc: 0.4869\n",
      "Epoch 147/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.1261 - acc: 0.9765 - val_loss: 1.9959 - val_acc: 0.4940\n",
      "Epoch 148/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.1230 - acc: 0.9765 - val_loss: 1.9250 - val_acc: 0.5012\n",
      "Epoch 149/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.1221 - acc: 0.9796 - val_loss: 1.7905 - val_acc: 0.5274\n",
      "Epoch 150/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.1207 - acc: 0.9786 - val_loss: 1.7522 - val_acc: 0.5369\n",
      "Epoch 151/400\n",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.1346 - acc: 0.9735 - val_loss: 1.7266 - val_acc: 0.5298\n",
      "Epoch 152/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.1160 - acc: 0.9801 - val_loss: 1.7434 - val_acc: 0.5190\n",
      "Epoch 153/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.1226 - acc: 0.9745 - val_loss: 1.8325 - val_acc: 0.5131\n",
      "Epoch 154/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.1138 - acc: 0.9816 - val_loss: 1.9426 - val_acc: 0.5036\n",
      "Epoch 155/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.1288 - acc: 0.9755 - val_loss: 1.9930 - val_acc: 0.5012\n",
      "Epoch 156/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.1111 - acc: 0.9827 - val_loss: 2.0295 - val_acc: 0.5048\n",
      "Epoch 157/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.1143 - acc: 0.9770 - val_loss: 2.1440 - val_acc: 0.5024\n",
      "Epoch 158/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.1203 - acc: 0.9765 - val_loss: 2.1878 - val_acc: 0.5083\n",
      "Epoch 159/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.1082 - acc: 0.9801 - val_loss: 2.1293 - val_acc: 0.5095\n",
      "Epoch 160/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.1055 - acc: 0.9827 - val_loss: 2.0184 - val_acc: 0.5131\n",
      "Epoch 161/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.1085 - acc: 0.9755 - val_loss: 1.8620 - val_acc: 0.5310\n",
      "Epoch 162/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.1105 - acc: 0.9796 - val_loss: 1.6895 - val_acc: 0.5452\n",
      "Epoch 163/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.1077 - acc: 0.9796 - val_loss: 1.5896 - val_acc: 0.5607\n",
      "Epoch 164/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.1030 - acc: 0.9827 - val_loss: 1.4880 - val_acc: 0.5750\n",
      "Epoch 165/400\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.0948 - acc: 0.9852 - val_loss: 1.3759 - val_acc: 0.5929\n",
      "Epoch 166/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.1044 - acc: 0.9816 - val_loss: 1.2848 - val_acc: 0.6190\n",
      "Epoch 167/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0988 - acc: 0.9821 - val_loss: 1.2255 - val_acc: 0.6429\n",
      "Epoch 168/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.1105 - acc: 0.9730 - val_loss: 1.1744 - val_acc: 0.6643\n",
      "Epoch 169/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0891 - acc: 0.9883 - val_loss: 1.2063 - val_acc: 0.6571\n",
      "Epoch 170/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0972 - acc: 0.9811 - val_loss: 1.3058 - val_acc: 0.6381\n",
      "Epoch 171/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0903 - acc: 0.9837 - val_loss: 1.3576 - val_acc: 0.6155\n",
      "Epoch 172/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0867 - acc: 0.9888 - val_loss: 1.5141 - val_acc: 0.5833\n",
      "Epoch 173/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0960 - acc: 0.9852 - val_loss: 1.6131 - val_acc: 0.5512\n",
      "Epoch 174/400\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0867 - acc: 0.9883 - val_loss: 1.6273 - val_acc: 0.5488\n",
      "Epoch 175/400\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0925 - acc: 0.9852 - val_loss: 1.5149 - val_acc: 0.5679\n",
      "Epoch 176/400\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.1005 - acc: 0.9765 - val_loss: 1.3232 - val_acc: 0.6143\n",
      "Epoch 177/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0984 - acc: 0.9796 - val_loss: 1.1080 - val_acc: 0.6869\n",
      "Epoch 178/400\n",
      "1960/1960 [==============================] - 0s 53us/step - loss: 0.1014 - acc: 0.9816 - val_loss: 0.9996 - val_acc: 0.7238\n",
      "Epoch 179/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0787 - acc: 0.9888 - val_loss: 0.9256 - val_acc: 0.7512\n",
      "Epoch 180/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0845 - acc: 0.9872 - val_loss: 0.9058 - val_acc: 0.7595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0756 - acc: 0.9893 - val_loss: 0.9320 - val_acc: 0.7512\n",
      "Epoch 182/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0862 - acc: 0.9867 - val_loss: 1.0260 - val_acc: 0.7238\n",
      "Epoch 183/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0850 - acc: 0.9883 - val_loss: 1.1501 - val_acc: 0.6821\n",
      "Epoch 184/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0883 - acc: 0.9816 - val_loss: 1.2532 - val_acc: 0.6476\n",
      "Epoch 185/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0776 - acc: 0.9852 - val_loss: 1.3854 - val_acc: 0.6179\n",
      "Epoch 186/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0786 - acc: 0.9872 - val_loss: 1.3616 - val_acc: 0.6238\n",
      "Epoch 187/400\n",
      "1960/1960 [==============================] - 0s 57us/step - loss: 0.0933 - acc: 0.9816 - val_loss: 1.3292 - val_acc: 0.6286\n",
      "Epoch 188/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0857 - acc: 0.9852 - val_loss: 1.2424 - val_acc: 0.6548\n",
      "Epoch 189/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0789 - acc: 0.9857 - val_loss: 1.1336 - val_acc: 0.6976\n",
      "Epoch 190/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0712 - acc: 0.9918 - val_loss: 1.0426 - val_acc: 0.7202\n",
      "Epoch 191/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0682 - acc: 0.9908 - val_loss: 0.9397 - val_acc: 0.7571\n",
      "Epoch 192/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0861 - acc: 0.9837 - val_loss: 0.8587 - val_acc: 0.7690\n",
      "Epoch 193/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0758 - acc: 0.9872 - val_loss: 0.8201 - val_acc: 0.7774\n",
      "Epoch 194/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0795 - acc: 0.9862 - val_loss: 0.8035 - val_acc: 0.7821\n",
      "Epoch 195/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0748 - acc: 0.9888 - val_loss: 0.7872 - val_acc: 0.7833\n",
      "Epoch 196/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0799 - acc: 0.9862 - val_loss: 0.7592 - val_acc: 0.7929\n",
      "Epoch 197/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0771 - acc: 0.9857 - val_loss: 0.7307 - val_acc: 0.7988\n",
      "Epoch 198/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0843 - acc: 0.9867 - val_loss: 0.6804 - val_acc: 0.8143\n",
      "Epoch 199/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0696 - acc: 0.9903 - val_loss: 0.6490 - val_acc: 0.8250\n",
      "Epoch 200/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0745 - acc: 0.9903 - val_loss: 0.6226 - val_acc: 0.8250\n",
      "Epoch 201/400\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0748 - acc: 0.9852 - val_loss: 0.6250 - val_acc: 0.8262\n",
      "Epoch 202/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0672 - acc: 0.9913 - val_loss: 0.6335 - val_acc: 0.8262\n",
      "Epoch 203/400\n",
      "1960/1960 [==============================] - 0s 54us/step - loss: 0.0707 - acc: 0.9888 - val_loss: 0.6616 - val_acc: 0.8179\n",
      "Epoch 204/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0792 - acc: 0.9852 - val_loss: 0.6871 - val_acc: 0.8095\n",
      "Epoch 205/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0728 - acc: 0.9888 - val_loss: 0.7310 - val_acc: 0.7976\n",
      "Epoch 206/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0700 - acc: 0.9883 - val_loss: 0.7619 - val_acc: 0.7917\n",
      "Epoch 207/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0715 - acc: 0.9862 - val_loss: 0.7814 - val_acc: 0.7845\n",
      "Epoch 208/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0664 - acc: 0.9898 - val_loss: 0.7842 - val_acc: 0.7845\n",
      "Epoch 209/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0682 - acc: 0.9903 - val_loss: 0.8037 - val_acc: 0.7833\n",
      "Epoch 210/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0708 - acc: 0.9862 - val_loss: 0.8144 - val_acc: 0.7833\n",
      "Epoch 211/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0663 - acc: 0.9923 - val_loss: 0.8275 - val_acc: 0.7810\n",
      "Epoch 212/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0651 - acc: 0.9939 - val_loss: 0.8359 - val_acc: 0.7786\n",
      "Epoch 213/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0678 - acc: 0.9893 - val_loss: 0.8340 - val_acc: 0.7750\n",
      "Epoch 214/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0728 - acc: 0.9872 - val_loss: 0.8168 - val_acc: 0.7798\n",
      "Epoch 215/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0775 - acc: 0.9827 - val_loss: 0.7841 - val_acc: 0.7917\n",
      "Epoch 216/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0703 - acc: 0.9888 - val_loss: 0.7369 - val_acc: 0.8036\n",
      "Epoch 217/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0659 - acc: 0.9923 - val_loss: 0.6954 - val_acc: 0.8131\n",
      "Epoch 218/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0685 - acc: 0.9898 - val_loss: 0.6618 - val_acc: 0.8214\n",
      "Epoch 219/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0617 - acc: 0.9913 - val_loss: 0.6326 - val_acc: 0.8274\n",
      "Epoch 220/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0654 - acc: 0.9888 - val_loss: 0.6063 - val_acc: 0.8417\n",
      "Epoch 221/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0678 - acc: 0.9872 - val_loss: 0.5869 - val_acc: 0.8464\n",
      "Epoch 222/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0654 - acc: 0.9903 - val_loss: 0.5756 - val_acc: 0.8488\n",
      "Epoch 223/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0634 - acc: 0.9893 - val_loss: 0.5709 - val_acc: 0.8512\n",
      "Epoch 224/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0672 - acc: 0.9898 - val_loss: 0.5749 - val_acc: 0.8464\n",
      "Epoch 225/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0659 - acc: 0.9918 - val_loss: 0.5721 - val_acc: 0.8464\n",
      "Epoch 226/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0642 - acc: 0.9898 - val_loss: 0.5812 - val_acc: 0.8440\n",
      "Epoch 227/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0701 - acc: 0.9867 - val_loss: 0.5953 - val_acc: 0.8429\n",
      "Epoch 228/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0678 - acc: 0.9872 - val_loss: 0.6108 - val_acc: 0.8369\n",
      "Epoch 229/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0630 - acc: 0.9918 - val_loss: 0.6186 - val_acc: 0.8333\n",
      "Epoch 230/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0639 - acc: 0.9908 - val_loss: 0.6200 - val_acc: 0.8333\n",
      "Epoch 231/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0667 - acc: 0.9903 - val_loss: 0.6028 - val_acc: 0.8381\n",
      "Epoch 232/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0661 - acc: 0.9908 - val_loss: 0.5943 - val_acc: 0.8381\n",
      "Epoch 233/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0671 - acc: 0.9883 - val_loss: 0.5887 - val_acc: 0.8393\n",
      "Epoch 234/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0576 - acc: 0.9898 - val_loss: 0.5779 - val_acc: 0.8405\n",
      "Epoch 235/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0687 - acc: 0.9908 - val_loss: 0.5747 - val_acc: 0.8417\n",
      "Epoch 236/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0556 - acc: 0.9949 - val_loss: 0.5767 - val_acc: 0.8393\n",
      "Epoch 237/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0598 - acc: 0.9918 - val_loss: 0.5788 - val_acc: 0.8393\n",
      "Epoch 238/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0609 - acc: 0.9929 - val_loss: 0.5720 - val_acc: 0.8405\n",
      "Epoch 239/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0544 - acc: 0.9939 - val_loss: 0.5710 - val_acc: 0.8429\n",
      "Epoch 240/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0639 - acc: 0.9918 - val_loss: 0.5708 - val_acc: 0.8417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0603 - acc: 0.9913 - val_loss: 0.5701 - val_acc: 0.8405\n",
      "Epoch 242/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0579 - acc: 0.9913 - val_loss: 0.5705 - val_acc: 0.8417\n",
      "Epoch 243/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0641 - acc: 0.9893 - val_loss: 0.5741 - val_acc: 0.8417\n",
      "Epoch 244/400\n",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.0646 - acc: 0.9913 - val_loss: 0.5757 - val_acc: 0.8405\n",
      "Epoch 245/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0610 - acc: 0.9913 - val_loss: 0.5732 - val_acc: 0.8417\n",
      "Epoch 246/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0625 - acc: 0.9898 - val_loss: 0.5758 - val_acc: 0.8405\n",
      "Epoch 247/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0600 - acc: 0.9934 - val_loss: 0.5826 - val_acc: 0.8405\n",
      "Epoch 248/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0627 - acc: 0.9908 - val_loss: 0.5900 - val_acc: 0.8381\n",
      "Epoch 249/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0638 - acc: 0.9888 - val_loss: 0.5868 - val_acc: 0.8381\n",
      "Epoch 250/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0634 - acc: 0.9903 - val_loss: 0.5764 - val_acc: 0.8405\n",
      "Epoch 251/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0605 - acc: 0.9923 - val_loss: 0.5647 - val_acc: 0.8417\n",
      "Epoch 252/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0599 - acc: 0.9908 - val_loss: 0.5583 - val_acc: 0.8440\n",
      "Epoch 253/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0591 - acc: 0.9908 - val_loss: 0.5511 - val_acc: 0.8440\n",
      "Epoch 254/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0549 - acc: 0.9929 - val_loss: 0.5442 - val_acc: 0.8476\n",
      "Epoch 255/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0597 - acc: 0.9913 - val_loss: 0.5335 - val_acc: 0.8512\n",
      "Epoch 256/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0629 - acc: 0.9903 - val_loss: 0.5256 - val_acc: 0.8524\n",
      "Epoch 257/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0533 - acc: 0.9929 - val_loss: 0.5202 - val_acc: 0.8524\n",
      "Epoch 258/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0563 - acc: 0.9888 - val_loss: 0.5172 - val_acc: 0.8548\n",
      "Epoch 259/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0632 - acc: 0.9918 - val_loss: 0.5141 - val_acc: 0.8595\n",
      "Epoch 260/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0585 - acc: 0.9929 - val_loss: 0.5131 - val_acc: 0.8607\n",
      "Epoch 261/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0699 - acc: 0.9878 - val_loss: 0.5107 - val_acc: 0.8619\n",
      "Epoch 262/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0604 - acc: 0.9913 - val_loss: 0.5043 - val_acc: 0.8643\n",
      "Epoch 263/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0587 - acc: 0.9898 - val_loss: 0.4985 - val_acc: 0.8643\n",
      "Epoch 264/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0588 - acc: 0.9913 - val_loss: 0.4914 - val_acc: 0.8667\n",
      "Epoch 265/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0628 - acc: 0.9923 - val_loss: 0.4835 - val_acc: 0.8679\n",
      "Epoch 266/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0620 - acc: 0.9878 - val_loss: 0.4759 - val_acc: 0.8679\n",
      "Epoch 267/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0542 - acc: 0.9918 - val_loss: 0.4695 - val_acc: 0.8702\n",
      "Epoch 268/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0630 - acc: 0.9888 - val_loss: 0.4645 - val_acc: 0.8738\n",
      "Epoch 269/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0706 - acc: 0.9888 - val_loss: 0.4605 - val_acc: 0.8750\n",
      "Epoch 270/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0519 - acc: 0.9934 - val_loss: 0.4562 - val_acc: 0.8750\n",
      "Epoch 271/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0605 - acc: 0.9908 - val_loss: 0.4519 - val_acc: 0.8774\n",
      "Epoch 272/400\n",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.0598 - acc: 0.9872 - val_loss: 0.4498 - val_acc: 0.8786\n",
      "Epoch 273/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0561 - acc: 0.9903 - val_loss: 0.4489 - val_acc: 0.8810\n",
      "Epoch 274/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0657 - acc: 0.9893 - val_loss: 0.4489 - val_acc: 0.8798\n",
      "Epoch 275/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0591 - acc: 0.9898 - val_loss: 0.4488 - val_acc: 0.8798\n",
      "Epoch 276/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0578 - acc: 0.9883 - val_loss: 0.4493 - val_acc: 0.8798\n",
      "Epoch 277/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0595 - acc: 0.9903 - val_loss: 0.4511 - val_acc: 0.8798\n",
      "Epoch 278/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0670 - acc: 0.9883 - val_loss: 0.4516 - val_acc: 0.8786\n",
      "Epoch 279/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0580 - acc: 0.9903 - val_loss: 0.4519 - val_acc: 0.8774\n",
      "Epoch 280/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0602 - acc: 0.9893 - val_loss: 0.4525 - val_acc: 0.8774\n",
      "Epoch 281/400\n",
      "1960/1960 [==============================] - 0s 39us/step - loss: 0.0603 - acc: 0.9918 - val_loss: 0.4540 - val_acc: 0.8762\n",
      "Epoch 282/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0592 - acc: 0.9908 - val_loss: 0.4546 - val_acc: 0.8774\n",
      "Epoch 283/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0523 - acc: 0.9929 - val_loss: 0.4553 - val_acc: 0.8774\n",
      "Epoch 284/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0532 - acc: 0.9918 - val_loss: 0.4552 - val_acc: 0.8774\n",
      "Epoch 285/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0566 - acc: 0.9929 - val_loss: 0.4548 - val_acc: 0.8774\n",
      "Epoch 286/400\n",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.0605 - acc: 0.9888 - val_loss: 0.4546 - val_acc: 0.8786\n",
      "Epoch 287/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0537 - acc: 0.9959 - val_loss: 0.4544 - val_acc: 0.8786\n",
      "Epoch 288/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0545 - acc: 0.9939 - val_loss: 0.4537 - val_acc: 0.8798\n",
      "Epoch 289/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0634 - acc: 0.9867 - val_loss: 0.4532 - val_acc: 0.8786\n",
      "Epoch 290/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0591 - acc: 0.9934 - val_loss: 0.4526 - val_acc: 0.8786\n",
      "Epoch 291/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0522 - acc: 0.9934 - val_loss: 0.4518 - val_acc: 0.8798\n",
      "Epoch 292/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0581 - acc: 0.9923 - val_loss: 0.4495 - val_acc: 0.8821\n",
      "Epoch 293/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0576 - acc: 0.9913 - val_loss: 0.4468 - val_acc: 0.8857\n",
      "Epoch 294/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0604 - acc: 0.9913 - val_loss: 0.4449 - val_acc: 0.8857\n",
      "Epoch 295/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0562 - acc: 0.9893 - val_loss: 0.4428 - val_acc: 0.8869\n",
      "Epoch 296/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0516 - acc: 0.9934 - val_loss: 0.4411 - val_acc: 0.8881\n",
      "Epoch 297/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0541 - acc: 0.9913 - val_loss: 0.4394 - val_acc: 0.8881\n",
      "Epoch 298/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0579 - acc: 0.9923 - val_loss: 0.4382 - val_acc: 0.8881\n",
      "Epoch 299/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0546 - acc: 0.9929 - val_loss: 0.4373 - val_acc: 0.8857\n",
      "Epoch 300/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0551 - acc: 0.9913 - val_loss: 0.4367 - val_acc: 0.8857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0585 - acc: 0.9923 - val_loss: 0.4362 - val_acc: 0.8857\n",
      "Epoch 302/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0585 - acc: 0.9923 - val_loss: 0.4353 - val_acc: 0.8857\n",
      "Epoch 303/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0578 - acc: 0.9913 - val_loss: 0.4344 - val_acc: 0.8857\n",
      "Epoch 304/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0576 - acc: 0.9908 - val_loss: 0.4336 - val_acc: 0.8857\n",
      "Epoch 305/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0583 - acc: 0.9929 - val_loss: 0.4322 - val_acc: 0.8857\n",
      "Epoch 306/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0593 - acc: 0.9888 - val_loss: 0.4310 - val_acc: 0.8869\n",
      "Epoch 307/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0573 - acc: 0.9923 - val_loss: 0.4298 - val_acc: 0.8893\n",
      "Epoch 308/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0545 - acc: 0.9923 - val_loss: 0.4289 - val_acc: 0.8893\n",
      "Epoch 309/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0518 - acc: 0.9929 - val_loss: 0.4282 - val_acc: 0.8893\n",
      "Epoch 310/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0551 - acc: 0.9923 - val_loss: 0.4275 - val_acc: 0.8893\n",
      "Epoch 311/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0523 - acc: 0.9913 - val_loss: 0.4267 - val_acc: 0.8893\n",
      "Epoch 312/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0589 - acc: 0.9903 - val_loss: 0.4261 - val_acc: 0.8929\n",
      "Epoch 313/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0528 - acc: 0.9944 - val_loss: 0.4255 - val_acc: 0.8964\n",
      "Epoch 314/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0651 - acc: 0.9903 - val_loss: 0.4248 - val_acc: 0.8952\n",
      "Epoch 315/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0610 - acc: 0.9867 - val_loss: 0.4243 - val_acc: 0.8952\n",
      "Epoch 316/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0577 - acc: 0.9918 - val_loss: 0.4240 - val_acc: 0.8952\n",
      "Epoch 317/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0556 - acc: 0.9913 - val_loss: 0.4237 - val_acc: 0.8964\n",
      "Epoch 318/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0554 - acc: 0.9929 - val_loss: 0.4236 - val_acc: 0.8964\n",
      "Epoch 319/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0525 - acc: 0.9939 - val_loss: 0.4237 - val_acc: 0.8964\n",
      "Epoch 320/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0520 - acc: 0.9934 - val_loss: 0.4238 - val_acc: 0.8964\n",
      "Epoch 321/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0514 - acc: 0.9923 - val_loss: 0.4239 - val_acc: 0.8964\n",
      "Epoch 322/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0536 - acc: 0.9923 - val_loss: 0.4239 - val_acc: 0.8964\n",
      "Epoch 323/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0542 - acc: 0.9913 - val_loss: 0.4240 - val_acc: 0.8964\n",
      "Epoch 324/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0548 - acc: 0.9888 - val_loss: 0.4240 - val_acc: 0.8964\n",
      "Epoch 325/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0577 - acc: 0.9923 - val_loss: 0.4239 - val_acc: 0.8988\n",
      "Epoch 326/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0580 - acc: 0.9923 - val_loss: 0.4235 - val_acc: 0.9000\n",
      "Epoch 327/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0623 - acc: 0.9872 - val_loss: 0.4234 - val_acc: 0.9000\n",
      "Epoch 328/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0508 - acc: 0.9934 - val_loss: 0.4232 - val_acc: 0.8988\n",
      "Epoch 329/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0593 - acc: 0.9929 - val_loss: 0.4231 - val_acc: 0.9000\n",
      "Epoch 330/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0600 - acc: 0.9903 - val_loss: 0.4230 - val_acc: 0.9024\n",
      "Epoch 331/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0518 - acc: 0.9923 - val_loss: 0.4230 - val_acc: 0.9024\n",
      "Epoch 332/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0552 - acc: 0.9934 - val_loss: 0.4230 - val_acc: 0.9024\n",
      "Epoch 333/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0495 - acc: 0.9923 - val_loss: 0.4230 - val_acc: 0.9024\n",
      "Epoch 334/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0586 - acc: 0.9908 - val_loss: 0.4231 - val_acc: 0.9024\n",
      "Epoch 335/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0525 - acc: 0.9929 - val_loss: 0.4230 - val_acc: 0.9024\n",
      "Epoch 336/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0560 - acc: 0.9918 - val_loss: 0.4229 - val_acc: 0.9024\n",
      "Epoch 337/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0505 - acc: 0.9949 - val_loss: 0.4228 - val_acc: 0.9024\n",
      "Epoch 338/400\n",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.0517 - acc: 0.9923 - val_loss: 0.4227 - val_acc: 0.9024\n",
      "Epoch 339/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0546 - acc: 0.9913 - val_loss: 0.4226 - val_acc: 0.9012\n",
      "Epoch 340/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0624 - acc: 0.9888 - val_loss: 0.4226 - val_acc: 0.9012\n",
      "Epoch 341/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0527 - acc: 0.9944 - val_loss: 0.4226 - val_acc: 0.9012\n",
      "Epoch 342/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0571 - acc: 0.9903 - val_loss: 0.4226 - val_acc: 0.9012\n",
      "Epoch 343/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0596 - acc: 0.9903 - val_loss: 0.4225 - val_acc: 0.9012\n",
      "Epoch 344/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0559 - acc: 0.9923 - val_loss: 0.4224 - val_acc: 0.9012\n",
      "Epoch 345/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0531 - acc: 0.9918 - val_loss: 0.4223 - val_acc: 0.9012\n",
      "Epoch 346/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0495 - acc: 0.9934 - val_loss: 0.4221 - val_acc: 0.9012\n",
      "Epoch 347/400\n",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0539 - acc: 0.9908 - val_loss: 0.4220 - val_acc: 0.9012\n",
      "Epoch 348/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0536 - acc: 0.9918 - val_loss: 0.4219 - val_acc: 0.9024\n",
      "Epoch 349/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0559 - acc: 0.9913 - val_loss: 0.4218 - val_acc: 0.9024\n",
      "Epoch 350/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0488 - acc: 0.9954 - val_loss: 0.4218 - val_acc: 0.9024\n",
      "Epoch 351/400\n",
      "1960/1960 [==============================] - 0s 44us/step - loss: 0.0514 - acc: 0.9908 - val_loss: 0.4217 - val_acc: 0.9024\n",
      "Epoch 352/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0552 - acc: 0.9918 - val_loss: 0.4216 - val_acc: 0.9024\n",
      "Epoch 353/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0535 - acc: 0.9923 - val_loss: 0.4215 - val_acc: 0.9024\n",
      "Epoch 354/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0591 - acc: 0.9918 - val_loss: 0.4215 - val_acc: 0.9024\n",
      "Epoch 355/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0627 - acc: 0.9888 - val_loss: 0.4213 - val_acc: 0.9024\n",
      "Epoch 356/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0535 - acc: 0.9903 - val_loss: 0.4212 - val_acc: 0.9024\n",
      "Epoch 357/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0462 - acc: 0.9964 - val_loss: 0.4212 - val_acc: 0.9024\n",
      "Epoch 358/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0479 - acc: 0.9969 - val_loss: 0.4211 - val_acc: 0.9012\n",
      "Epoch 359/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0558 - acc: 0.9903 - val_loss: 0.4211 - val_acc: 0.9012\n",
      "Epoch 360/400\n",
      "1960/1960 [==============================] - 0s 52us/step - loss: 0.0559 - acc: 0.9898 - val_loss: 0.4211 - val_acc: 0.9012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0560 - acc: 0.9939 - val_loss: 0.4212 - val_acc: 0.9012\n",
      "Epoch 362/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0496 - acc: 0.9934 - val_loss: 0.4212 - val_acc: 0.9012\n",
      "Epoch 363/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0586 - acc: 0.9918 - val_loss: 0.4211 - val_acc: 0.9012\n",
      "Epoch 364/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0487 - acc: 0.9944 - val_loss: 0.4212 - val_acc: 0.9012\n",
      "Epoch 365/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0584 - acc: 0.9923 - val_loss: 0.4212 - val_acc: 0.9012\n",
      "Epoch 366/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0528 - acc: 0.9923 - val_loss: 0.4211 - val_acc: 0.9012\n",
      "Epoch 367/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0536 - acc: 0.9934 - val_loss: 0.4211 - val_acc: 0.9012\n",
      "Epoch 368/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0483 - acc: 0.9954 - val_loss: 0.4212 - val_acc: 0.9012\n",
      "Epoch 369/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0508 - acc: 0.9944 - val_loss: 0.4212 - val_acc: 0.9012\n",
      "Epoch 370/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0501 - acc: 0.9918 - val_loss: 0.4212 - val_acc: 0.9012\n",
      "Epoch 371/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0545 - acc: 0.9954 - val_loss: 0.4213 - val_acc: 0.9012\n",
      "Epoch 372/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0555 - acc: 0.9908 - val_loss: 0.4214 - val_acc: 0.9012\n",
      "Epoch 373/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0498 - acc: 0.9939 - val_loss: 0.4216 - val_acc: 0.9012\n",
      "Epoch 374/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0530 - acc: 0.9954 - val_loss: 0.4217 - val_acc: 0.9012\n",
      "Epoch 375/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0538 - acc: 0.9944 - val_loss: 0.4218 - val_acc: 0.9012\n",
      "Epoch 376/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0558 - acc: 0.9918 - val_loss: 0.4220 - val_acc: 0.9024\n",
      "Epoch 377/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0518 - acc: 0.9923 - val_loss: 0.4221 - val_acc: 0.9024\n",
      "Epoch 378/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0573 - acc: 0.9903 - val_loss: 0.4223 - val_acc: 0.9024\n",
      "Epoch 379/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0536 - acc: 0.9929 - val_loss: 0.4224 - val_acc: 0.9024\n",
      "Epoch 380/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0500 - acc: 0.9934 - val_loss: 0.4225 - val_acc: 0.9024\n",
      "Epoch 381/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0465 - acc: 0.9959 - val_loss: 0.4227 - val_acc: 0.9024\n",
      "Epoch 382/400\n",
      "1960/1960 [==============================] - 0s 49us/step - loss: 0.0535 - acc: 0.9918 - val_loss: 0.4228 - val_acc: 0.9024\n",
      "Epoch 383/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0553 - acc: 0.9929 - val_loss: 0.4230 - val_acc: 0.9024\n",
      "Epoch 384/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0510 - acc: 0.9929 - val_loss: 0.4231 - val_acc: 0.9024\n",
      "Epoch 385/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0510 - acc: 0.9964 - val_loss: 0.4233 - val_acc: 0.9024\n",
      "Epoch 386/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0495 - acc: 0.9949 - val_loss: 0.4235 - val_acc: 0.9024\n",
      "Epoch 387/400\n",
      "1960/1960 [==============================] - 0s 46us/step - loss: 0.0536 - acc: 0.9934 - val_loss: 0.4237 - val_acc: 0.9024\n",
      "Epoch 388/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0588 - acc: 0.9888 - val_loss: 0.4239 - val_acc: 0.9024\n",
      "Epoch 389/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0562 - acc: 0.9893 - val_loss: 0.4241 - val_acc: 0.9024\n",
      "Epoch 390/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0533 - acc: 0.9939 - val_loss: 0.4243 - val_acc: 0.9024\n",
      "Epoch 391/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0590 - acc: 0.9918 - val_loss: 0.4244 - val_acc: 0.9000\n",
      "Epoch 392/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0576 - acc: 0.9929 - val_loss: 0.4245 - val_acc: 0.9000\n",
      "Epoch 393/400\n",
      "1960/1960 [==============================] - 0s 47us/step - loss: 0.0522 - acc: 0.9918 - val_loss: 0.4246 - val_acc: 0.9000\n",
      "Epoch 394/400\n",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0551 - acc: 0.9903 - val_loss: 0.4247 - val_acc: 0.9036\n",
      "Epoch 395/400\n",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0502 - acc: 0.9944 - val_loss: 0.4248 - val_acc: 0.9036\n",
      "Epoch 396/400\n",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0478 - acc: 0.9949 - val_loss: 0.4249 - val_acc: 0.9036\n",
      "Epoch 397/400\n",
      "1960/1960 [==============================] - 0s 51us/step - loss: 0.0604 - acc: 0.9908 - val_loss: 0.4250 - val_acc: 0.9036\n",
      "Epoch 398/400\n",
      "1960/1960 [==============================] - 0s 45us/step - loss: 0.0522 - acc: 0.9903 - val_loss: 0.4251 - val_acc: 0.9036\n",
      "Epoch 399/400\n",
      "1960/1960 [==============================] - 0s 50us/step - loss: 0.0580 - acc: 0.9923 - val_loss: 0.4253 - val_acc: 0.9036\n",
      "Epoch 400/400\n",
      "1960/1960 [==============================] - 0s 48us/step - loss: 0.0556 - acc: 0.9908 - val_loss: 0.4255 - val_acc: 0.9024\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "lr = 1e-3\n",
    "AR_single.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8, patience=5, cooldown=5, min_lr=1e-5)\n",
    "history = AR_single.fit([X_0,X_1],Y,\n",
    "        batch_size=len(Y),\n",
    "        epochs=400,\n",
    "        verbose=True,\n",
    "        shuffle=True,\n",
    "        callbacks=[lrScheduler],\n",
    "        validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AR_single.save_weights('weights/fine_lite.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f93f0391a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 2e-6\n",
    "AR_single.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23/1960 [00:00<00:08, 224.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 1960/1960 [00:05<00:00, 390.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 4s 2ms/step - loss: 0.0959 - acc: 0.9801 - val_loss: 0.4277 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/1960 [00:00<00:11, 172.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 373.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0829 - acc: 0.9837 - val_loss: 0.4277 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/1960 [00:00<00:11, 172.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 386.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0815 - acc: 0.9821 - val_loss: 0.4277 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 40/1960 [00:00<00:04, 397.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:04<00:00, 403.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0928 - acc: 0.9781 - val_loss: 0.4277 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 41/1960 [00:00<00:04, 400.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:04<00:00, 402.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 42us/step - loss: 0.0879 - acc: 0.9827 - val_loss: 0.4277 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 30/1960 [00:00<00:06, 296.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:04<00:00, 402.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0858 - acc: 0.9842 - val_loss: 0.4278 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 41/1960 [00:00<00:04, 401.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:04<00:00, 403.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 41us/step - loss: 0.0921 - acc: 0.9811 - val_loss: 0.4278 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/1960 [00:00<00:11, 175.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 380.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 38us/step - loss: 0.0886 - acc: 0.9811 - val_loss: 0.4279 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 37/1960 [00:00<00:05, 360.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:04<00:00, 397.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 40us/step - loss: 0.0846 - acc: 0.9832 - val_loss: 0.4279 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 33/1960 [00:00<00:05, 321.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:04<00:00, 402.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 43us/step - loss: 0.0937 - acc: 0.9796 - val_loss: 0.4280 - val_acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in tqdm(range(len(Train['pose']))): \n",
    "    \n",
    "        label = np.zeros(C.clc_fine)\n",
    "        label[Train['fine_label'][i]-1] = 1 \n",
    "        \n",
    "        p = np.copy(Train['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "        p = sampling_frame(p,C)\n",
    "        \n",
    "        #rotation\n",
    "        x_angle = np.random.uniform(-0.1,0.1)\n",
    "        y_angle = np.random.uniform(-0.1,0.1)\n",
    "        z_angle = np.random.uniform(-0.1,0.1)\n",
    "        R = euler2mat(x_angle, y_angle, z_angle, 'sxyz')\n",
    "        p = rotaion_one(p,R)\n",
    "        \n",
    "        p = normlize_range(p)\n",
    "        M = get_CG(p,C)\n",
    "        \n",
    "        X_0.append(M)\n",
    "        X_1.append(p)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    Y = np.stack(Y)\n",
    "   \n",
    "\n",
    "    AR_single.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )\n",
    "\n",
    "    if e%10==0:\n",
    "        AR_single.save_weights('weights/fine_lite_aug.h5')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkUAAAJCCAYAAADOe7N5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAFSiCACgEkUAAFX9HzrldiIAmEu8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9367e9a278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = AR_single.predict([X_test_0,X_test_1])\n",
    "cnf_matrix = confusion_matrix(np.argmax(Y_test,axis=1),np.argmax(Y_pred,axis=1))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cnf_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
