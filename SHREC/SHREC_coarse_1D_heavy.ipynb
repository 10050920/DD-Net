{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 12 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joints\n",
    "        self.clc_coarse = 14 # the number of coarse class\n",
    "        self.clc_fine = 28 # the number of fine-grained class\n",
    "        self.feat_d = 90\n",
    "        self.filters = 64\n",
    "        self.joint_ind = np.array([0,1,2,5,6,9,10,13,14,17,18,21])\n",
    "        self.data_dir = '/mnt/nasbi/homes/fan/projects/action/skeleton/data/SHREC/'\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def get_CG(p,C):\n",
    "    M = []\n",
    "    iu = np.triu_indices(C.joint_n,0,C.joint_n+1)\n",
    "    for f in range(C.frame_l):\n",
    "        #distance max \n",
    "        d_m = cdist(p[f],np.concatenate([p[f],np.zeros([1,C.joint_d])]),'euclidean')       \n",
    "        d_m = d_m[iu] \n",
    "        M.append(d_m)\n",
    "    M = np.stack(M)   \n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,:1,...],x[:,:-1,...])\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "    \n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=12,joint_d=3,feat_d=90,filters=16):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "    \n",
    "    x = block(M,filters)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    \n",
    "    x_d_slow = block(diff_slow,filters)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    \n",
    "    x_d_fast = block(diff_fast,filters)\n",
    "   \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    return Model(inputs=[M,P],outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_AR_single(frame_l=32,joint_n=22,joint_d=3,feat_d=90,clc_num=14,filters=16):\n",
    "    M = Input(name='M', shape=(frame_l,feat_d))  \n",
    "    P = Input(name='P', shape=(frame_l,joint_n,joint_d)) \n",
    "    \n",
    "    FM = build_FM(frame_l,joint_n,joint_d,feat_d,filters)\n",
    "    \n",
    "    x = FM([M,P])\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M,P],outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_single = build_AR_single(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.clc_coarse,C.filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M (InputLayer)                  (None, 32, 90)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P (InputLayer)                  (None, 32, 12, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 512)       1674112     M[0][0]                          \n",
      "                                                                 P[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65536       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 128)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16384       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 128)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 14)           1806        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,758,862\n",
      "Trainable params: 1,753,998\n",
      "Non-trainable params: 4,864\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AR_single.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_single.load_weights('weights/coarse_heavy.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train.pkl\", \"rb\"))\n",
    "Test = pickle.load(open(C.data_dir+\"test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normlize_range(p):\n",
    "    # normolize to start point, use the center for hand case\n",
    "    p[:,:,0] = p[:,:,0]-np.mean(p[:,:,0])\n",
    "    p[:,:,1] = p[:,:,1]-np.mean(p[:,:,1])\n",
    "    p[:,:,2] = p[:,:,2]-np.mean(p[:,:,2])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/1960 [00:00<00:09, 209.11it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 1960/1960 [00:04<00:00, 420.17it/s]\n"
     ]
    }
   ],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(Train['pose']))): \n",
    "    p = np.copy(Train['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Train['coarse_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_0.append(M)\n",
    "    X_1.append(p)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  \n",
    "X_1 = np.stack(X_1) \n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 44/840 [00:00<00:01, 439.43it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 840/840 [00:01<00:00, 436.59it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_0 = []\n",
    "X_test_1 = []\n",
    "Y_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    p = np.copy(Test['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Test['coarse_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_test_0.append(M)\n",
    "    X_test_1.append(p)\n",
    "    Y_test.append(label)\n",
    "\n",
    "X_test_0 = np.stack(X_test_0) \n",
    "X_test_1 = np.stack(X_test_1)  \n",
    "Y_test = np.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/400\n",
      "1960/1960 [==============================] - 7s 4ms/step - loss: 3.2154 - acc: 0.0888 - val_loss: 1.9010 - val_acc: 0.4321\n",
      "Epoch 2/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 2.4816 - acc: 0.2046 - val_loss: 1.6046 - val_acc: 0.4988\n",
      "Epoch 3/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 2.0848 - acc: 0.3230 - val_loss: 1.5673 - val_acc: 0.5310\n",
      "Epoch 4/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 1.8495 - acc: 0.3995 - val_loss: 1.4916 - val_acc: 0.5560\n",
      "Epoch 5/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 1.7112 - acc: 0.4546 - val_loss: 1.3434 - val_acc: 0.5833\n",
      "Epoch 6/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 1.5149 - acc: 0.5367 - val_loss: 1.1558 - val_acc: 0.6333\n",
      "Epoch 7/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 1.3859 - acc: 0.5801 - val_loss: 0.9868 - val_acc: 0.6929\n",
      "Epoch 8/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 1.3191 - acc: 0.6133 - val_loss: 0.8633 - val_acc: 0.7369\n",
      "Epoch 9/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 1.2252 - acc: 0.6485 - val_loss: 0.7684 - val_acc: 0.7583\n",
      "Epoch 10/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 1.0799 - acc: 0.6913 - val_loss: 0.6767 - val_acc: 0.7810\n",
      "Epoch 11/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 1.0154 - acc: 0.7219 - val_loss: 0.5975 - val_acc: 0.8036\n",
      "Epoch 12/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.9433 - acc: 0.7526 - val_loss: 0.5480 - val_acc: 0.8262\n",
      "Epoch 13/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.8666 - acc: 0.7806 - val_loss: 0.5227 - val_acc: 0.8345\n",
      "Epoch 14/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.8120 - acc: 0.7954 - val_loss: 0.5105 - val_acc: 0.8452\n",
      "Epoch 15/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.7214 - acc: 0.8332 - val_loss: 0.5064 - val_acc: 0.8500\n",
      "Epoch 16/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.7006 - acc: 0.8301 - val_loss: 0.5134 - val_acc: 0.8417\n",
      "Epoch 17/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.6431 - acc: 0.8607 - val_loss: 0.5105 - val_acc: 0.8405\n",
      "Epoch 18/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.5879 - acc: 0.8781 - val_loss: 0.4975 - val_acc: 0.8488\n",
      "Epoch 19/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.5398 - acc: 0.8842 - val_loss: 0.4769 - val_acc: 0.8512\n",
      "Epoch 20/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.5270 - acc: 0.8898 - val_loss: 0.4630 - val_acc: 0.8583\n",
      "Epoch 21/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.4976 - acc: 0.8974 - val_loss: 0.4592 - val_acc: 0.8607\n",
      "Epoch 22/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.4616 - acc: 0.9015 - val_loss: 0.4651 - val_acc: 0.8595\n",
      "Epoch 23/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.4252 - acc: 0.9194 - val_loss: 0.4834 - val_acc: 0.8560\n",
      "Epoch 24/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.4082 - acc: 0.9235 - val_loss: 0.5027 - val_acc: 0.8512\n",
      "Epoch 25/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.3736 - acc: 0.9362 - val_loss: 0.5190 - val_acc: 0.8440\n",
      "Epoch 26/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.3535 - acc: 0.9362 - val_loss: 0.5360 - val_acc: 0.8417\n",
      "Epoch 27/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.3415 - acc: 0.9393 - val_loss: 0.5481 - val_acc: 0.8417\n",
      "Epoch 28/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.3104 - acc: 0.9444 - val_loss: 0.5460 - val_acc: 0.8440\n",
      "Epoch 29/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.3097 - acc: 0.9459 - val_loss: 0.5335 - val_acc: 0.8571\n",
      "Epoch 30/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.2874 - acc: 0.9490 - val_loss: 0.5462 - val_acc: 0.8524\n",
      "Epoch 31/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.2672 - acc: 0.9546 - val_loss: 0.5526 - val_acc: 0.8560\n",
      "Epoch 32/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.2573 - acc: 0.9556 - val_loss: 0.5431 - val_acc: 0.8607\n",
      "Epoch 33/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.2433 - acc: 0.9602 - val_loss: 0.5225 - val_acc: 0.8667\n",
      "Epoch 34/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.2310 - acc: 0.9668 - val_loss: 0.5145 - val_acc: 0.8714\n",
      "Epoch 35/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.2119 - acc: 0.9684 - val_loss: 0.5218 - val_acc: 0.8690\n",
      "Epoch 36/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.2036 - acc: 0.9740 - val_loss: 0.5449 - val_acc: 0.8643\n",
      "Epoch 37/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.1997 - acc: 0.9719 - val_loss: 0.5525 - val_acc: 0.8679\n",
      "Epoch 38/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.1787 - acc: 0.9791 - val_loss: 0.5505 - val_acc: 0.8690\n",
      "Epoch 39/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.1779 - acc: 0.9750 - val_loss: 0.5198 - val_acc: 0.8762\n",
      "Epoch 40/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1710 - acc: 0.9770 - val_loss: 0.4925 - val_acc: 0.8845\n",
      "Epoch 41/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1656 - acc: 0.9776 - val_loss: 0.4736 - val_acc: 0.8893\n",
      "Epoch 42/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1489 - acc: 0.9791 - val_loss: 0.4600 - val_acc: 0.8893\n",
      "Epoch 43/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.1522 - acc: 0.9801 - val_loss: 0.4595 - val_acc: 0.8869\n",
      "Epoch 44/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.1465 - acc: 0.9842 - val_loss: 0.4719 - val_acc: 0.8738\n",
      "Epoch 45/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.1262 - acc: 0.9867 - val_loss: 0.4790 - val_acc: 0.8750\n",
      "Epoch 46/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.1363 - acc: 0.9852 - val_loss: 0.4806 - val_acc: 0.8774\n",
      "Epoch 47/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.1249 - acc: 0.9872 - val_loss: 0.4745 - val_acc: 0.8798\n",
      "Epoch 48/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.1176 - acc: 0.9898 - val_loss: 0.4665 - val_acc: 0.8798\n",
      "Epoch 49/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.1079 - acc: 0.9903 - val_loss: 0.4548 - val_acc: 0.8845\n",
      "Epoch 50/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1099 - acc: 0.9903 - val_loss: 0.4432 - val_acc: 0.8857\n",
      "Epoch 51/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0995 - acc: 0.9888 - val_loss: 0.4363 - val_acc: 0.8881\n",
      "Epoch 52/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.1018 - acc: 0.9898 - val_loss: 0.4261 - val_acc: 0.8929\n",
      "Epoch 53/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.1017 - acc: 0.9918 - val_loss: 0.4162 - val_acc: 0.8952\n",
      "Epoch 54/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1001 - acc: 0.9903 - val_loss: 0.4039 - val_acc: 0.8940\n",
      "Epoch 55/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0945 - acc: 0.9878 - val_loss: 0.3972 - val_acc: 0.8952\n",
      "Epoch 56/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0880 - acc: 0.9918 - val_loss: 0.3918 - val_acc: 0.8988\n",
      "Epoch 57/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0835 - acc: 0.9934 - val_loss: 0.3885 - val_acc: 0.8952\n",
      "Epoch 58/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0878 - acc: 0.9913 - val_loss: 0.3757 - val_acc: 0.9000\n",
      "Epoch 59/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0771 - acc: 0.9949 - val_loss: 0.3634 - val_acc: 0.9036\n",
      "Epoch 60/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0715 - acc: 0.9954 - val_loss: 0.3467 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0785 - acc: 0.9939 - val_loss: 0.3353 - val_acc: 0.9083\n",
      "Epoch 62/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0702 - acc: 0.9964 - val_loss: 0.3286 - val_acc: 0.9107\n",
      "Epoch 63/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0704 - acc: 0.9959 - val_loss: 0.3251 - val_acc: 0.9143\n",
      "Epoch 64/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0697 - acc: 0.9934 - val_loss: 0.3326 - val_acc: 0.9131\n",
      "Epoch 65/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0655 - acc: 0.9934 - val_loss: 0.3483 - val_acc: 0.9107\n",
      "Epoch 66/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0639 - acc: 0.9954 - val_loss: 0.3566 - val_acc: 0.9095\n",
      "Epoch 67/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0618 - acc: 0.9964 - val_loss: 0.3576 - val_acc: 0.9095\n",
      "Epoch 68/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0595 - acc: 0.9959 - val_loss: 0.3538 - val_acc: 0.9107\n",
      "Epoch 69/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0539 - acc: 0.9969 - val_loss: 0.3435 - val_acc: 0.9143\n",
      "Epoch 70/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0534 - acc: 0.9964 - val_loss: 0.3367 - val_acc: 0.9155\n",
      "Epoch 71/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0554 - acc: 0.9959 - val_loss: 0.3302 - val_acc: 0.9155\n",
      "Epoch 72/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0564 - acc: 0.9954 - val_loss: 0.3231 - val_acc: 0.9155\n",
      "Epoch 73/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0493 - acc: 0.9974 - val_loss: 0.3202 - val_acc: 0.9190\n",
      "Epoch 74/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0468 - acc: 0.9969 - val_loss: 0.3278 - val_acc: 0.9155\n",
      "Epoch 75/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0489 - acc: 0.9974 - val_loss: 0.3298 - val_acc: 0.9143\n",
      "Epoch 76/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0464 - acc: 0.9974 - val_loss: 0.3308 - val_acc: 0.9167\n",
      "Epoch 77/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0477 - acc: 0.9969 - val_loss: 0.3347 - val_acc: 0.9179\n",
      "Epoch 78/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0492 - acc: 0.9985 - val_loss: 0.3402 - val_acc: 0.9143\n",
      "Epoch 79/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0414 - acc: 0.9980 - val_loss: 0.3479 - val_acc: 0.9095\n",
      "Epoch 80/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0462 - acc: 0.9959 - val_loss: 0.3550 - val_acc: 0.9071\n",
      "Epoch 81/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0441 - acc: 0.9969 - val_loss: 0.3603 - val_acc: 0.9095\n",
      "Epoch 82/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0455 - acc: 0.9980 - val_loss: 0.3603 - val_acc: 0.9119\n",
      "Epoch 83/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0416 - acc: 0.9974 - val_loss: 0.3587 - val_acc: 0.9119\n",
      "Epoch 84/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0410 - acc: 0.9980 - val_loss: 0.3511 - val_acc: 0.9167\n",
      "Epoch 85/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0384 - acc: 0.9980 - val_loss: 0.3431 - val_acc: 0.9143\n",
      "Epoch 86/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0422 - acc: 0.9980 - val_loss: 0.3373 - val_acc: 0.9155\n",
      "Epoch 87/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0422 - acc: 0.9990 - val_loss: 0.3459 - val_acc: 0.9095\n",
      "Epoch 88/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0362 - acc: 0.9990 - val_loss: 0.3638 - val_acc: 0.9095\n",
      "Epoch 89/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0359 - acc: 0.9969 - val_loss: 0.3553 - val_acc: 0.9083\n",
      "Epoch 90/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0368 - acc: 0.9980 - val_loss: 0.3505 - val_acc: 0.9131\n",
      "Epoch 91/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0353 - acc: 1.0000 - val_loss: 0.3543 - val_acc: 0.9131\n",
      "Epoch 92/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0358 - acc: 0.9974 - val_loss: 0.3619 - val_acc: 0.9095\n",
      "Epoch 93/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0322 - acc: 0.9985 - val_loss: 0.3644 - val_acc: 0.9095\n",
      "Epoch 94/400\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0313 - acc: 0.9990 - val_loss: 0.3628 - val_acc: 0.9083\n",
      "Epoch 95/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0344 - acc: 0.9995 - val_loss: 0.3466 - val_acc: 0.9131\n",
      "Epoch 96/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0310 - acc: 0.9990 - val_loss: 0.3276 - val_acc: 0.9155\n",
      "Epoch 97/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0317 - acc: 0.9995 - val_loss: 0.3210 - val_acc: 0.9167\n",
      "Epoch 98/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0307 - acc: 0.9990 - val_loss: 0.3192 - val_acc: 0.9190\n",
      "Epoch 99/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0318 - acc: 0.9985 - val_loss: 0.3217 - val_acc: 0.9202\n",
      "Epoch 100/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0333 - acc: 0.9974 - val_loss: 0.3246 - val_acc: 0.9190\n",
      "Epoch 101/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0325 - acc: 0.9990 - val_loss: 0.3285 - val_acc: 0.9179\n",
      "Epoch 102/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0295 - acc: 0.9995 - val_loss: 0.3362 - val_acc: 0.9155\n",
      "Epoch 103/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0303 - acc: 0.9990 - val_loss: 0.3431 - val_acc: 0.9107\n",
      "Epoch 104/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0325 - acc: 0.9974 - val_loss: 0.3382 - val_acc: 0.9143\n",
      "Epoch 105/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0272 - acc: 0.9990 - val_loss: 0.3417 - val_acc: 0.9131\n",
      "Epoch 106/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0253 - acc: 0.9995 - val_loss: 0.3450 - val_acc: 0.9155\n",
      "Epoch 107/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0266 - acc: 0.9995 - val_loss: 0.3308 - val_acc: 0.9202\n",
      "Epoch 108/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0255 - acc: 0.9995 - val_loss: 0.3287 - val_acc: 0.9238\n",
      "Epoch 109/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0249 - acc: 0.9995 - val_loss: 0.3305 - val_acc: 0.9226\n",
      "Epoch 110/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.3362 - val_acc: 0.9226\n",
      "Epoch 111/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0269 - acc: 0.9985 - val_loss: 0.3245 - val_acc: 0.9226\n",
      "Epoch 112/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0263 - acc: 0.9990 - val_loss: 0.3210 - val_acc: 0.9214\n",
      "Epoch 113/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0236 - acc: 1.0000 - val_loss: 0.3239 - val_acc: 0.9214\n",
      "Epoch 114/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.3267 - val_acc: 0.9202\n",
      "Epoch 115/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0236 - acc: 1.0000 - val_loss: 0.3262 - val_acc: 0.9226\n",
      "Epoch 116/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0245 - acc: 1.0000 - val_loss: 0.3278 - val_acc: 0.9167\n",
      "Epoch 117/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0196 - acc: 0.9995 - val_loss: 0.3336 - val_acc: 0.9119\n",
      "Epoch 118/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0217 - acc: 1.0000 - val_loss: 0.3415 - val_acc: 0.9155\n",
      "Epoch 119/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0244 - acc: 0.9995 - val_loss: 0.3447 - val_acc: 0.9179\n",
      "Epoch 120/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0209 - acc: 0.9995 - val_loss: 0.3479 - val_acc: 0.9190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.3416 - val_acc: 0.9179\n",
      "Epoch 122/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0248 - acc: 0.9995 - val_loss: 0.3424 - val_acc: 0.9250\n",
      "Epoch 123/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0216 - acc: 0.9995 - val_loss: 0.3434 - val_acc: 0.9262\n",
      "Epoch 124/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0217 - acc: 0.9995 - val_loss: 0.3434 - val_acc: 0.9274\n",
      "Epoch 125/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0222 - acc: 0.9995 - val_loss: 0.3424 - val_acc: 0.9262\n",
      "Epoch 126/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0216 - acc: 0.9995 - val_loss: 0.3434 - val_acc: 0.9238\n",
      "Epoch 127/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.3453 - val_acc: 0.9214\n",
      "Epoch 128/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0196 - acc: 0.9995 - val_loss: 0.3480 - val_acc: 0.9214\n",
      "Epoch 129/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.3509 - val_acc: 0.9238\n",
      "Epoch 130/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.3516 - val_acc: 0.9214\n",
      "Epoch 131/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.3512 - val_acc: 0.9190\n",
      "Epoch 132/400\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.3496 - val_acc: 0.9190\n",
      "Epoch 133/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.3472 - val_acc: 0.9214\n",
      "Epoch 134/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 0.3438 - val_acc: 0.9214\n",
      "Epoch 135/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0208 - acc: 0.9995 - val_loss: 0.3408 - val_acc: 0.9214\n",
      "Epoch 136/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.3363 - val_acc: 0.9214\n",
      "Epoch 137/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.3308 - val_acc: 0.9226\n",
      "Epoch 138/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.3243 - val_acc: 0.9262\n",
      "Epoch 139/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0183 - acc: 0.9995 - val_loss: 0.3190 - val_acc: 0.9274\n",
      "Epoch 140/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.3152 - val_acc: 0.9298\n",
      "Epoch 141/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.3122 - val_acc: 0.9321\n",
      "Epoch 142/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.3106 - val_acc: 0.9357\n",
      "Epoch 143/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.3095 - val_acc: 0.9357\n",
      "Epoch 144/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.3086 - val_acc: 0.9369\n",
      "Epoch 145/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.3079 - val_acc: 0.9345\n",
      "Epoch 146/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.3077 - val_acc: 0.9345\n",
      "Epoch 147/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.3075 - val_acc: 0.9345\n",
      "Epoch 148/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.3071 - val_acc: 0.9345\n",
      "Epoch 149/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.3073 - val_acc: 0.9345\n",
      "Epoch 150/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0204 - acc: 0.9995 - val_loss: 0.3077 - val_acc: 0.9357\n",
      "Epoch 151/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.3075 - val_acc: 0.9345\n",
      "Epoch 152/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.3072 - val_acc: 0.9345\n",
      "Epoch 153/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.3070 - val_acc: 0.9345\n",
      "Epoch 154/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0211 - acc: 0.9995 - val_loss: 0.3070 - val_acc: 0.9345\n",
      "Epoch 155/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.3072 - val_acc: 0.9345\n",
      "Epoch 156/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.3072 - val_acc: 0.9345\n",
      "Epoch 157/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.3074 - val_acc: 0.9345\n",
      "Epoch 158/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.3075 - val_acc: 0.9345\n",
      "Epoch 159/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0205 - acc: 0.9985 - val_loss: 0.3076 - val_acc: 0.9345\n",
      "Epoch 160/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.3077 - val_acc: 0.9345\n",
      "Epoch 161/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0194 - acc: 0.9995 - val_loss: 0.3077 - val_acc: 0.9333\n",
      "Epoch 162/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.3074 - val_acc: 0.9333\n",
      "Epoch 163/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0190 - acc: 0.9995 - val_loss: 0.3071 - val_acc: 0.9345\n",
      "Epoch 164/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0209 - acc: 0.9990 - val_loss: 0.3069 - val_acc: 0.9345\n",
      "Epoch 165/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.3066 - val_acc: 0.9345\n",
      "Epoch 166/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.3063 - val_acc: 0.9345\n",
      "Epoch 167/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.3060 - val_acc: 0.9345\n",
      "Epoch 168/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0218 - acc: 0.9995 - val_loss: 0.3057 - val_acc: 0.9345\n",
      "Epoch 169/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0190 - acc: 0.9995 - val_loss: 0.3054 - val_acc: 0.9345\n",
      "Epoch 170/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.3050 - val_acc: 0.9345\n",
      "Epoch 171/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.3046 - val_acc: 0.9345\n",
      "Epoch 172/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.3041 - val_acc: 0.9345\n",
      "Epoch 173/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0196 - acc: 0.9995 - val_loss: 0.3036 - val_acc: 0.9345\n",
      "Epoch 174/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0186 - acc: 0.9995 - val_loss: 0.3031 - val_acc: 0.9345\n",
      "Epoch 175/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0192 - acc: 0.9980 - val_loss: 0.3026 - val_acc: 0.9345\n",
      "Epoch 176/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0194 - acc: 0.9995 - val_loss: 0.3020 - val_acc: 0.9345\n",
      "Epoch 177/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0194 - acc: 0.9995 - val_loss: 0.3015 - val_acc: 0.9345\n",
      "Epoch 178/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0180 - acc: 0.9995 - val_loss: 0.3011 - val_acc: 0.9345\n",
      "Epoch 179/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.3007 - val_acc: 0.9345\n",
      "Epoch 180/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.9345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.3000 - val_acc: 0.9345\n",
      "Epoch 182/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2996 - val_acc: 0.9357\n",
      "Epoch 183/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2992 - val_acc: 0.9357\n",
      "Epoch 184/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.2989 - val_acc: 0.9369\n",
      "Epoch 185/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0180 - acc: 0.9995 - val_loss: 0.2986 - val_acc: 0.9369\n",
      "Epoch 186/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2983 - val_acc: 0.9369\n",
      "Epoch 187/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.2980 - val_acc: 0.9369\n",
      "Epoch 188/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.2977 - val_acc: 0.9369\n",
      "Epoch 189/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2974 - val_acc: 0.9369\n",
      "Epoch 190/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.2972 - val_acc: 0.9369\n",
      "Epoch 191/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0174 - acc: 0.9995 - val_loss: 0.2969 - val_acc: 0.9369\n",
      "Epoch 192/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0187 - acc: 0.9995 - val_loss: 0.2966 - val_acc: 0.9369\n",
      "Epoch 193/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0156 - acc: 0.9995 - val_loss: 0.2964 - val_acc: 0.9369\n",
      "Epoch 194/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0190 - acc: 0.9995 - val_loss: 0.2961 - val_acc: 0.9369\n",
      "Epoch 195/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.2959 - val_acc: 0.9369\n",
      "Epoch 196/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0178 - acc: 0.9995 - val_loss: 0.2956 - val_acc: 0.9369\n",
      "Epoch 197/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.2954 - val_acc: 0.9369\n",
      "Epoch 198/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0190 - acc: 0.9995 - val_loss: 0.2952 - val_acc: 0.9369\n",
      "Epoch 199/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0204 - acc: 0.9985 - val_loss: 0.2950 - val_acc: 0.9369\n",
      "Epoch 200/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0175 - acc: 0.9995 - val_loss: 0.2948 - val_acc: 0.9369\n",
      "Epoch 201/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.2946 - val_acc: 0.9369\n",
      "Epoch 202/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2944 - val_acc: 0.9369\n",
      "Epoch 203/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.2942 - val_acc: 0.9381\n",
      "Epoch 204/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0206 - acc: 0.9990 - val_loss: 0.2940 - val_acc: 0.9381\n",
      "Epoch 205/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0197 - acc: 0.9995 - val_loss: 0.2938 - val_acc: 0.9381\n",
      "Epoch 206/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0195 - acc: 0.9990 - val_loss: 0.2936 - val_acc: 0.9393\n",
      "Epoch 207/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.2934 - val_acc: 0.9405\n",
      "Epoch 208/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0182 - acc: 0.9995 - val_loss: 0.2933 - val_acc: 0.9405\n",
      "Epoch 209/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0201 - acc: 0.9990 - val_loss: 0.2931 - val_acc: 0.9393\n",
      "Epoch 210/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.2929 - val_acc: 0.9393\n",
      "Epoch 211/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0200 - acc: 0.9995 - val_loss: 0.2927 - val_acc: 0.9393\n",
      "Epoch 212/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0202 - acc: 0.9995 - val_loss: 0.2925 - val_acc: 0.9393\n",
      "Epoch 213/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.2923 - val_acc: 0.9393\n",
      "Epoch 214/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.2922 - val_acc: 0.9393\n",
      "Epoch 215/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.2920 - val_acc: 0.9393\n",
      "Epoch 216/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0185 - acc: 0.9995 - val_loss: 0.2918 - val_acc: 0.9393\n",
      "Epoch 217/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0193 - acc: 0.9995 - val_loss: 0.2917 - val_acc: 0.9393\n",
      "Epoch 218/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2915 - val_acc: 0.9393\n",
      "Epoch 219/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.2913 - val_acc: 0.9393\n",
      "Epoch 220/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0164 - acc: 0.9990 - val_loss: 0.2911 - val_acc: 0.9393\n",
      "Epoch 221/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0188 - acc: 0.9995 - val_loss: 0.2910 - val_acc: 0.9393\n",
      "Epoch 222/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.2908 - val_acc: 0.9393\n",
      "Epoch 223/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.2906 - val_acc: 0.9393\n",
      "Epoch 224/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2905 - val_acc: 0.9393\n",
      "Epoch 225/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0187 - acc: 0.9995 - val_loss: 0.2903 - val_acc: 0.9393\n",
      "Epoch 226/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2902 - val_acc: 0.9393\n",
      "Epoch 227/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.2900 - val_acc: 0.9393\n",
      "Epoch 228/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.2899 - val_acc: 0.9393\n",
      "Epoch 229/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0173 - acc: 0.9995 - val_loss: 0.2898 - val_acc: 0.9393\n",
      "Epoch 230/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2897 - val_acc: 0.9405\n",
      "Epoch 231/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0220 - acc: 0.9985 - val_loss: 0.2895 - val_acc: 0.9405\n",
      "Epoch 232/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0202 - acc: 0.9990 - val_loss: 0.2893 - val_acc: 0.9405\n",
      "Epoch 233/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0168 - acc: 0.9995 - val_loss: 0.2892 - val_acc: 0.9405\n",
      "Epoch 234/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.2890 - val_acc: 0.9405\n",
      "Epoch 235/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2889 - val_acc: 0.9405\n",
      "Epoch 236/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0194 - acc: 0.9995 - val_loss: 0.2888 - val_acc: 0.9405\n",
      "Epoch 237/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0191 - acc: 0.9995 - val_loss: 0.2886 - val_acc: 0.9405\n",
      "Epoch 238/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2885 - val_acc: 0.9405\n",
      "Epoch 239/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0194 - acc: 0.9990 - val_loss: 0.2884 - val_acc: 0.9405\n",
      "Epoch 240/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2882 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0197 - acc: 0.9990 - val_loss: 0.2881 - val_acc: 0.9393\n",
      "Epoch 242/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0188 - acc: 0.9995 - val_loss: 0.2880 - val_acc: 0.9393\n",
      "Epoch 243/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2879 - val_acc: 0.9393\n",
      "Epoch 244/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.2877 - val_acc: 0.9393\n",
      "Epoch 245/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.2876 - val_acc: 0.9393\n",
      "Epoch 246/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0196 - acc: 0.9985 - val_loss: 0.2875 - val_acc: 0.9393\n",
      "Epoch 247/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.2874 - val_acc: 0.9393\n",
      "Epoch 248/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.2872 - val_acc: 0.9393\n",
      "Epoch 249/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.2871 - val_acc: 0.9393\n",
      "Epoch 250/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0184 - acc: 0.9995 - val_loss: 0.2870 - val_acc: 0.9393\n",
      "Epoch 251/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.2869 - val_acc: 0.9393\n",
      "Epoch 252/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0212 - acc: 0.9990 - val_loss: 0.2868 - val_acc: 0.9393\n",
      "Epoch 253/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0182 - acc: 0.9995 - val_loss: 0.2867 - val_acc: 0.9393\n",
      "Epoch 254/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.2866 - val_acc: 0.9393\n",
      "Epoch 255/400\n",
      "1960/1960 [==============================] - 0s 73us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.2865 - val_acc: 0.9393\n",
      "Epoch 256/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0189 - acc: 0.9995 - val_loss: 0.2864 - val_acc: 0.9405\n",
      "Epoch 257/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.2863 - val_acc: 0.9405\n",
      "Epoch 258/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2863 - val_acc: 0.9405\n",
      "Epoch 259/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0199 - acc: 0.9995 - val_loss: 0.2861 - val_acc: 0.9405\n",
      "Epoch 260/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.2861 - val_acc: 0.9405\n",
      "Epoch 261/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.2860 - val_acc: 0.9405\n",
      "Epoch 262/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0184 - acc: 0.9995 - val_loss: 0.2859 - val_acc: 0.9405\n",
      "Epoch 263/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0214 - acc: 0.9995 - val_loss: 0.2858 - val_acc: 0.9405\n",
      "Epoch 264/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0178 - acc: 0.9995 - val_loss: 0.2858 - val_acc: 0.9405\n",
      "Epoch 265/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.2857 - val_acc: 0.9405\n",
      "Epoch 266/400\n",
      "1960/1960 [==============================] - 0s 74us/step - loss: 0.0172 - acc: 0.9995 - val_loss: 0.2856 - val_acc: 0.9405\n",
      "Epoch 267/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.2855 - val_acc: 0.9405\n",
      "Epoch 268/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0194 - acc: 0.9995 - val_loss: 0.2855 - val_acc: 0.9405\n",
      "Epoch 269/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0212 - acc: 1.0000 - val_loss: 0.2854 - val_acc: 0.9405\n",
      "Epoch 270/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.2853 - val_acc: 0.9405\n",
      "Epoch 271/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0201 - acc: 0.9995 - val_loss: 0.2853 - val_acc: 0.9405\n",
      "Epoch 272/400\n",
      "1960/1960 [==============================] - 0s 75us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.2852 - val_acc: 0.9405\n",
      "Epoch 273/400\n",
      "1960/1960 [==============================] - 0s 74us/step - loss: 0.0182 - acc: 0.9995 - val_loss: 0.2851 - val_acc: 0.9405\n",
      "Epoch 274/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.2851 - val_acc: 0.9405\n",
      "Epoch 275/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0190 - acc: 0.9995 - val_loss: 0.2851 - val_acc: 0.9405\n",
      "Epoch 276/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.2850 - val_acc: 0.9405\n",
      "Epoch 277/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0177 - acc: 0.9990 - val_loss: 0.2850 - val_acc: 0.9405\n",
      "Epoch 278/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2850 - val_acc: 0.9405\n",
      "Epoch 279/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2850 - val_acc: 0.9405\n",
      "Epoch 280/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.2850 - val_acc: 0.9405\n",
      "Epoch 281/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.2849 - val_acc: 0.9405\n",
      "Epoch 282/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.2849 - val_acc: 0.9405\n",
      "Epoch 283/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0168 - acc: 0.9995 - val_loss: 0.2849 - val_acc: 0.9405\n",
      "Epoch 284/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0197 - acc: 0.9995 - val_loss: 0.2848 - val_acc: 0.9405\n",
      "Epoch 285/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.2848 - val_acc: 0.9405\n",
      "Epoch 286/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2848 - val_acc: 0.9405\n",
      "Epoch 287/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.2847 - val_acc: 0.9405\n",
      "Epoch 288/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0197 - acc: 0.9995 - val_loss: 0.2847 - val_acc: 0.9405\n",
      "Epoch 289/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2847 - val_acc: 0.9405\n",
      "Epoch 290/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2846 - val_acc: 0.9405\n",
      "Epoch 291/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.2846 - val_acc: 0.9405\n",
      "Epoch 292/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0211 - acc: 0.9985 - val_loss: 0.2845 - val_acc: 0.9405\n",
      "Epoch 293/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.2845 - val_acc: 0.9405\n",
      "Epoch 294/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.2845 - val_acc: 0.9405\n",
      "Epoch 295/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0179 - acc: 0.9990 - val_loss: 0.2844 - val_acc: 0.9405\n",
      "Epoch 296/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.2844 - val_acc: 0.9405\n",
      "Epoch 297/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.2844 - val_acc: 0.9405\n",
      "Epoch 298/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.2843 - val_acc: 0.9405\n",
      "Epoch 299/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.2843 - val_acc: 0.9405\n",
      "Epoch 300/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0199 - acc: 0.9995 - val_loss: 0.2843 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.2842 - val_acc: 0.9405\n",
      "Epoch 302/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0174 - acc: 0.9995 - val_loss: 0.2841 - val_acc: 0.9405\n",
      "Epoch 303/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0188 - acc: 0.9995 - val_loss: 0.2841 - val_acc: 0.9405\n",
      "Epoch 304/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0206 - acc: 0.9995 - val_loss: 0.2840 - val_acc: 0.9405\n",
      "Epoch 305/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0210 - acc: 0.9995 - val_loss: 0.2840 - val_acc: 0.9405\n",
      "Epoch 306/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.2839 - val_acc: 0.9405\n",
      "Epoch 307/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.2839 - val_acc: 0.9405\n",
      "Epoch 308/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2839 - val_acc: 0.9405\n",
      "Epoch 309/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.2839 - val_acc: 0.9405\n",
      "Epoch 310/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.2839 - val_acc: 0.9405\n",
      "Epoch 311/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0186 - acc: 0.9995 - val_loss: 0.2838 - val_acc: 0.9405\n",
      "Epoch 312/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2838 - val_acc: 0.9405\n",
      "Epoch 313/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.2837 - val_acc: 0.9405\n",
      "Epoch 314/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.2837 - val_acc: 0.9405\n",
      "Epoch 315/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2836 - val_acc: 0.9405\n",
      "Epoch 316/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0181 - acc: 0.9995 - val_loss: 0.2836 - val_acc: 0.9405\n",
      "Epoch 317/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.2835 - val_acc: 0.9405\n",
      "Epoch 318/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.2834 - val_acc: 0.9405\n",
      "Epoch 319/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.2833 - val_acc: 0.9405\n",
      "Epoch 320/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0192 - acc: 0.9990 - val_loss: 0.2832 - val_acc: 0.9405\n",
      "Epoch 321/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2832 - val_acc: 0.9405\n",
      "Epoch 322/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0180 - acc: 0.9995 - val_loss: 0.2831 - val_acc: 0.9405\n",
      "Epoch 323/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0177 - acc: 0.9995 - val_loss: 0.2831 - val_acc: 0.9405\n",
      "Epoch 324/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0188 - acc: 0.9990 - val_loss: 0.2831 - val_acc: 0.9405\n",
      "Epoch 325/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0187 - acc: 0.9990 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 326/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 327/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0197 - acc: 0.9995 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 328/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0200 - acc: 0.9990 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 329/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 330/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0177 - acc: 0.9995 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 331/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 332/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 333/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0172 - acc: 0.9995 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 334/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0178 - acc: 0.9995 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 335/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0185 - acc: 0.9995 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 336/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0177 - acc: 0.9995 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 337/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 338/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.2830 - val_acc: 0.9405\n",
      "Epoch 339/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0189 - acc: 0.9990 - val_loss: 0.2829 - val_acc: 0.9405\n",
      "Epoch 340/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.2829 - val_acc: 0.9405\n",
      "Epoch 341/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.2829 - val_acc: 0.9405\n",
      "Epoch 342/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0184 - acc: 0.9995 - val_loss: 0.2829 - val_acc: 0.9405\n",
      "Epoch 343/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.2829 - val_acc: 0.9405\n",
      "Epoch 344/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.2828 - val_acc: 0.9405\n",
      "Epoch 345/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0196 - acc: 0.9990 - val_loss: 0.2828 - val_acc: 0.9405\n",
      "Epoch 346/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.2828 - val_acc: 0.9405\n",
      "Epoch 347/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.2828 - val_acc: 0.9405\n",
      "Epoch 348/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.2828 - val_acc: 0.9405\n",
      "Epoch 349/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.2828 - val_acc: 0.9405\n",
      "Epoch 350/400\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.2828 - val_acc: 0.9405\n",
      "Epoch 351/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0182 - acc: 0.9995 - val_loss: 0.2828 - val_acc: 0.9405\n",
      "Epoch 352/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 0.9405\n",
      "Epoch 353/400\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 0.9405\n",
      "Epoch 354/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 0.9405\n",
      "Epoch 355/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0199 - acc: 0.9995 - val_loss: 0.2827 - val_acc: 0.9405\n",
      "Epoch 356/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0175 - acc: 0.9995 - val_loss: 0.2827 - val_acc: 0.9405\n",
      "Epoch 357/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2826 - val_acc: 0.9405\n",
      "Epoch 358/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.2826 - val_acc: 0.9405\n",
      "Epoch 359/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0190 - acc: 0.9995 - val_loss: 0.2826 - val_acc: 0.9405\n",
      "Epoch 360/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0190 - acc: 0.9980 - val_loss: 0.2825 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0193 - acc: 0.9990 - val_loss: 0.2824 - val_acc: 0.9405\n",
      "Epoch 362/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.2824 - val_acc: 0.9405\n",
      "Epoch 363/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0178 - acc: 0.9995 - val_loss: 0.2823 - val_acc: 0.9405\n",
      "Epoch 364/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0187 - acc: 0.9990 - val_loss: 0.2823 - val_acc: 0.9405\n",
      "Epoch 365/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0195 - acc: 0.9990 - val_loss: 0.2822 - val_acc: 0.9405\n",
      "Epoch 366/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.2822 - val_acc: 0.9405\n",
      "Epoch 367/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2822 - val_acc: 0.9405\n",
      "Epoch 368/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0181 - acc: 0.9990 - val_loss: 0.2822 - val_acc: 0.9405\n",
      "Epoch 369/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.2822 - val_acc: 0.9405\n",
      "Epoch 370/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.2821 - val_acc: 0.9405\n",
      "Epoch 371/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2821 - val_acc: 0.9405\n",
      "Epoch 372/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2821 - val_acc: 0.9405\n",
      "Epoch 373/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.2822 - val_acc: 0.9405\n",
      "Epoch 374/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0177 - acc: 0.9995 - val_loss: 0.2822 - val_acc: 0.9405\n",
      "Epoch 375/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0172 - acc: 0.9995 - val_loss: 0.2822 - val_acc: 0.9405\n",
      "Epoch 376/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0180 - acc: 0.9995 - val_loss: 0.2822 - val_acc: 0.9405\n",
      "Epoch 377/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2823 - val_acc: 0.9405\n",
      "Epoch 378/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0175 - acc: 0.9995 - val_loss: 0.2823 - val_acc: 0.9405\n",
      "Epoch 379/400\n",
      "1960/1960 [==============================] - 0s 73us/step - loss: 0.0197 - acc: 0.9995 - val_loss: 0.2823 - val_acc: 0.9405\n",
      "Epoch 380/400\n",
      "1960/1960 [==============================] - 0s 74us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.2823 - val_acc: 0.9405\n",
      "Epoch 381/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.2823 - val_acc: 0.9405\n",
      "Epoch 382/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0185 - acc: 0.9995 - val_loss: 0.2823 - val_acc: 0.9405\n",
      "Epoch 383/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.2824 - val_acc: 0.9405\n",
      "Epoch 384/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0173 - acc: 0.9995 - val_loss: 0.2824 - val_acc: 0.9405\n",
      "Epoch 385/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0196 - acc: 0.9990 - val_loss: 0.2824 - val_acc: 0.9405\n",
      "Epoch 386/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.2825 - val_acc: 0.9405\n",
      "Epoch 387/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.2825 - val_acc: 0.9405\n",
      "Epoch 388/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0181 - acc: 0.9995 - val_loss: 0.2825 - val_acc: 0.9405\n",
      "Epoch 389/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.2826 - val_acc: 0.9405\n",
      "Epoch 390/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.2826 - val_acc: 0.9405\n",
      "Epoch 391/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2826 - val_acc: 0.9405\n",
      "Epoch 392/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.2826 - val_acc: 0.9405\n",
      "Epoch 393/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 0.9405\n",
      "Epoch 394/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 0.9405\n",
      "Epoch 395/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 0.9393\n",
      "Epoch 396/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0170 - acc: 0.9995 - val_loss: 0.2827 - val_acc: 0.9393\n",
      "Epoch 397/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 0.9393\n",
      "Epoch 398/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 0.9393\n",
      "Epoch 399/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 0.9393\n",
      "Epoch 400/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0172 - acc: 0.9995 - val_loss: 0.2826 - val_acc: 0.9393\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "lr = 1e-3\n",
    "AR_single.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history=AR_single.fit([X_0,X_1],Y,\n",
    "        batch_size=len(Y),\n",
    "        epochs=400,\n",
    "        verbose=True,\n",
    "        shuffle=True,\n",
    "        callbacks=[lrScheduler],\n",
    "        validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AR_single.save_weights('weights/coarse_heavy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88a11ca5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transforms3d.euler import euler2mat, mat2euler\n",
    "def rotaion_one(p,R):\n",
    "    p_new = np.zeros_like(p)\n",
    "    for i in range(len(p)):\n",
    "        p_new[i] = np.dot(R,p[i].T).T\n",
    "    return p_new\n",
    "\n",
    "def sampling_frame(p,C):\n",
    "    full_l = p.shape[0] # full length\n",
    "    if random.uniform(0,1)<0.5: # aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.9,1)*full_l)\n",
    "        s = random.randint(0, full_l-int(valid_l))\n",
    "        e = s+valid_l # sample end point\n",
    "        p = p[int(s):int(e),:,:]    \n",
    "    else: # without aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.9,1)*full_l)\n",
    "        index = np.sort(np.random.choice(range(0,full_l),int(valid_l),replace=False))\n",
    "        p = p[index,:,:]\n",
    "    p = zoom(p,C.frame_l,C.joint_n,C.joint_d)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 5e-6\n",
    "AR_single.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/1960 [00:00<00:09, 213.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 1960/1960 [00:05<00:00, 380.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 3s 2ms/step - loss: 0.0332 - acc: 0.9939 - val_loss: 0.2825 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/1960 [00:00<00:08, 237.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 377.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0430 - acc: 0.9918 - val_loss: 0.2825 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 370.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0390 - acc: 0.9944 - val_loss: 0.2826 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 383.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0445 - acc: 0.9949 - val_loss: 0.2826 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 379.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 58us/step - loss: 0.0377 - acc: 0.9944 - val_loss: 0.2825 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 387.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0415 - acc: 0.9908 - val_loss: 0.2826 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 364.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 57us/step - loss: 0.0389 - acc: 0.9944 - val_loss: 0.2828 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 371.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0379 - acc: 0.9954 - val_loss: 0.2830 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 344.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0400 - acc: 0.9934 - val_loss: 0.2832 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:06<00:00, 316.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0364 - acc: 0.9934 - val_loss: 0.2835 - val_acc: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 186/1960 [00:00<00:04, 369.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9bc612492bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormlize_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_CG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mX_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6247199a77ee>\u001b[0m in \u001b[0;36mget_CG\u001b[0;34m(p, C)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#distance max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0md_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0md_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_m\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2600\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2601\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2602\u001b[0;31m         \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2603\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in tqdm(range(len(Train['pose']))): \n",
    "    #for i in range(len(Train['pose'])): \n",
    "    \n",
    "        label = np.zeros(C.clc_coarse)\n",
    "        label[Train['coarse_label'][i]-1] = 1 \n",
    "        \n",
    "        p = np.copy(Train['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "        p = sampling_frame(p,C)\n",
    "       \n",
    "        #rotation\n",
    "        x_angle = np.random.uniform(-0.2,0.2)\n",
    "        y_angle = np.random.uniform(-0.2,0.2)\n",
    "        z_angle = np.random.uniform(-0.2,0.2)\n",
    "        R = euler2mat(x_angle, y_angle, z_angle, 'sxyz')\n",
    "        p = rotaion_one(p,R)\n",
    "         \n",
    "        p[:,:,0] = p[:,:,0]*random.uniform(0.9, 1.1)+p[:,:,0]*random.uniform(-0.1,0.1)\n",
    "        p[:,:,1] = p[:,:,1]*random.uniform(0.9, 1.1)+p[:,:,1]*random.uniform(-0.1,0.1)\n",
    "        p[:,:,2] = p[:,:,2]*random.uniform(0.9, 1.1)+p[:,:,2]*random.uniform(-0.1,0.1)\n",
    "     \n",
    "        p = normlize_range(p)\n",
    "        M = get_CG(p,C)\n",
    "        \n",
    "        X_0.append(M)\n",
    "        X_1.append(p)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    Y = np.stack(Y)\n",
    "   \n",
    "\n",
    "    AR_single.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )\n",
    "\n",
    "    if e%10==0:\n",
    "        AR_single.save_weights('weights/coarse_1D_heavy_aug.h5')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkUAAAJCCAYAAADOe7N5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noKr/Cxz3YstmFndgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88c81d8d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = AR_single.predict([X_test_0,X_test_1])\n",
    "cnf_matrix = confusion_matrix(np.argmax(Y_test,axis=1),np.argmax(Y_pred,axis=1))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cnf_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
