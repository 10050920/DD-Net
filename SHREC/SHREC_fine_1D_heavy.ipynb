{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 12 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joints\n",
    "        self.clc_coarse = 14 # the number of coarse class\n",
    "        self.clc_fine = 28 # the number of fine-grained class\n",
    "        self.feat_d = 90\n",
    "        self.filters = 64\n",
    "        self.joint_ind = np.array([0,1,2,5,6,9,10,13,14,17,18,21])\n",
    "        self.data_dir = '/mnt/nasbi/homes/fan/projects/action/skeleton/data/SHREC/'\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def get_CG(p,C):\n",
    "    M = []\n",
    "    iu = np.triu_indices(C.joint_n,0,C.joint_n+1)\n",
    "    for f in range(C.frame_l):\n",
    "        #distance max \n",
    "        d_m = cdist(p[f],np.concatenate([p[f],np.zeros([1,C.joint_d])]),'euclidean')       \n",
    "        d_m = d_m[iu] \n",
    "        M.append(d_m)\n",
    "    M = np.stack(M)   \n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,:1,...],x[:,:-1,...])\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "\n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=12,joint_d=3,feat_d=90,filters=16):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "    \n",
    "    x = block(M,filters)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    \n",
    "    x_d_slow = block(diff_slow,filters)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    \n",
    "    x_d_fast = block(diff_fast,filters)\n",
    "   \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    return Model(inputs=[M,P],outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_AR_single(frame_l=32,joint_n=22,joint_d=3,feat_d=90,clc_num=14,filters=16):\n",
    "    M = Input(name='M', shape=(frame_l,feat_d))  \n",
    "    P = Input(name='P', shape=(frame_l,joint_n,joint_d)) \n",
    "    \n",
    "    FM = build_FM(frame_l,joint_n,joint_d,feat_d,filters)\n",
    "    \n",
    "    x = FM([M,P])\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M,P],outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_single = build_AR_single(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.clc_fine,C.filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M (InputLayer)                  (None, 32, 90)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P (InputLayer)                  (None, 32, 12, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 4, 512)       1674112     M[0][0]                          \n",
      "                                                                 P[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65536       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 128)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16384       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 128)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 28)           3612        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,760,668\n",
      "Trainable params: 1,755,804\n",
      "Non-trainable params: 4,864\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AR_single.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AR_single.load_weights('weights/fine_heavy.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train.pkl\", \"rb\"))\n",
    "Test = pickle.load(open(C.data_dir+\"test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normlize_range(p):\n",
    "    # normolize to start point, use the center for hand case\n",
    "    p[:,:,0] = p[:,:,0]-np.mean(p[:,:,0])\n",
    "    p[:,:,1] = p[:,:,1]-np.mean(p[:,:,1])\n",
    "    p[:,:,2] = p[:,:,2]-np.mean(p[:,:,2])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/1960 [00:00<00:10, 184.08it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 1960/1960 [00:05<00:00, 342.41it/s]\n"
     ]
    }
   ],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(Train['pose']))): \n",
    "    p = np.copy(Train['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_fine)\n",
    "    label[Train['fine_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_0.append(M)\n",
    "    X_1.append(p)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  \n",
    "X_1 = np.stack(X_1) \n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 41/840 [00:00<00:01, 403.83it/s]/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 840/840 [00:02<00:00, 388.02it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_0 = []\n",
    "X_test_1 = []\n",
    "Y_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    p = np.copy(Test['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_fine)\n",
    "    label[Test['fine_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_test_0.append(M)\n",
    "    X_test_1.append(p)\n",
    "    Y_test.append(label)\n",
    "\n",
    "X_test_0 = np.stack(X_test_0) \n",
    "X_test_1 = np.stack(X_test_1)  \n",
    "Y_test = np.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/400\n",
      "1960/1960 [==============================] - 3s 2ms/step - loss: 4.0768 - acc: 0.0398 - val_loss: 2.8681 - val_acc: 0.1821\n",
      "Epoch 2/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 3.4845 - acc: 0.0980 - val_loss: 2.5010 - val_acc: 0.3310\n",
      "Epoch 3/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 3.1503 - acc: 0.1276 - val_loss: 2.1860 - val_acc: 0.4131\n",
      "Epoch 4/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 2.8775 - acc: 0.1959 - val_loss: 1.9972 - val_acc: 0.4417\n",
      "Epoch 5/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 2.6821 - acc: 0.2332 - val_loss: 1.8961 - val_acc: 0.4560\n",
      "Epoch 6/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 2.5390 - acc: 0.2704 - val_loss: 1.7847 - val_acc: 0.4869\n",
      "Epoch 7/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 2.4331 - acc: 0.3015 - val_loss: 1.7103 - val_acc: 0.5036\n",
      "Epoch 8/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 2.3067 - acc: 0.3316 - val_loss: 1.6601 - val_acc: 0.5321\n",
      "Epoch 9/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 2.1438 - acc: 0.3801 - val_loss: 1.5660 - val_acc: 0.5464\n",
      "Epoch 10/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 2.0615 - acc: 0.4143 - val_loss: 1.4847 - val_acc: 0.5583\n",
      "Epoch 11/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 1.9660 - acc: 0.4480 - val_loss: 1.4285 - val_acc: 0.5738\n",
      "Epoch 12/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 1.8525 - acc: 0.4786 - val_loss: 1.3519 - val_acc: 0.5940\n",
      "Epoch 13/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 1.7439 - acc: 0.5102 - val_loss: 1.3087 - val_acc: 0.6083\n",
      "Epoch 14/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 1.7114 - acc: 0.5301 - val_loss: 1.2549 - val_acc: 0.6238\n",
      "Epoch 15/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 1.6355 - acc: 0.5434 - val_loss: 1.1948 - val_acc: 0.6381\n",
      "Epoch 16/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 1.5448 - acc: 0.5760 - val_loss: 1.1573 - val_acc: 0.6333\n",
      "Epoch 17/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 1.4849 - acc: 0.6056 - val_loss: 1.1645 - val_acc: 0.6345\n",
      "Epoch 18/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 1.4107 - acc: 0.6362 - val_loss: 1.1674 - val_acc: 0.6333\n",
      "Epoch 19/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 1.3352 - acc: 0.6408 - val_loss: 1.1643 - val_acc: 0.6298\n",
      "Epoch 20/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 1.3094 - acc: 0.6617 - val_loss: 1.1610 - val_acc: 0.6274\n",
      "Epoch 21/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 1.2493 - acc: 0.6714 - val_loss: 1.1222 - val_acc: 0.6381\n",
      "Epoch 22/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 1.1722 - acc: 0.7036 - val_loss: 1.0322 - val_acc: 0.6702\n",
      "Epoch 23/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 1.1388 - acc: 0.7138 - val_loss: 0.9367 - val_acc: 0.6917\n",
      "Epoch 24/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 1.0674 - acc: 0.7301 - val_loss: 0.8636 - val_acc: 0.7274\n",
      "Epoch 25/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 1.0450 - acc: 0.7480 - val_loss: 0.8351 - val_acc: 0.7512\n",
      "Epoch 26/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 1.0224 - acc: 0.7607 - val_loss: 0.8097 - val_acc: 0.7655\n",
      "Epoch 27/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.9323 - acc: 0.7949 - val_loss: 0.7945 - val_acc: 0.7726\n",
      "Epoch 28/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.9194 - acc: 0.7811 - val_loss: 0.7830 - val_acc: 0.7810\n",
      "Epoch 29/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.8728 - acc: 0.8051 - val_loss: 0.7864 - val_acc: 0.7810\n",
      "Epoch 30/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.8556 - acc: 0.8138 - val_loss: 0.8034 - val_acc: 0.7786\n",
      "Epoch 31/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.8040 - acc: 0.8316 - val_loss: 0.8173 - val_acc: 0.7798\n",
      "Epoch 32/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.7689 - acc: 0.8357 - val_loss: 0.8357 - val_acc: 0.7810\n",
      "Epoch 33/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.7354 - acc: 0.8474 - val_loss: 0.8586 - val_acc: 0.7714\n",
      "Epoch 34/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.7325 - acc: 0.8510 - val_loss: 0.8713 - val_acc: 0.7667\n",
      "Epoch 35/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.7056 - acc: 0.8536 - val_loss: 0.8584 - val_acc: 0.7714\n",
      "Epoch 36/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.6472 - acc: 0.8760 - val_loss: 0.8350 - val_acc: 0.7786\n",
      "Epoch 37/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.6317 - acc: 0.8724 - val_loss: 0.8021 - val_acc: 0.7833\n",
      "Epoch 38/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.5975 - acc: 0.8811 - val_loss: 0.7815 - val_acc: 0.7810\n",
      "Epoch 39/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.5866 - acc: 0.8878 - val_loss: 0.7730 - val_acc: 0.7833\n",
      "Epoch 40/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.5554 - acc: 0.8995 - val_loss: 0.7895 - val_acc: 0.7762\n",
      "Epoch 41/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.5315 - acc: 0.9107 - val_loss: 0.7924 - val_acc: 0.7750\n",
      "Epoch 42/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.5164 - acc: 0.9087 - val_loss: 0.8243 - val_acc: 0.7607\n",
      "Epoch 43/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.5031 - acc: 0.9148 - val_loss: 0.8746 - val_acc: 0.7357\n",
      "Epoch 44/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.4762 - acc: 0.9250 - val_loss: 0.9010 - val_acc: 0.7190\n",
      "Epoch 45/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.4649 - acc: 0.9214 - val_loss: 0.8491 - val_acc: 0.7583\n",
      "Epoch 46/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.4403 - acc: 0.9327 - val_loss: 0.8069 - val_acc: 0.7810\n",
      "Epoch 47/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.4264 - acc: 0.9311 - val_loss: 0.8037 - val_acc: 0.7833\n",
      "Epoch 48/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.4151 - acc: 0.9362 - val_loss: 0.7972 - val_acc: 0.7857\n",
      "Epoch 49/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.4035 - acc: 0.9418 - val_loss: 0.7705 - val_acc: 0.7940\n",
      "Epoch 50/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.3917 - acc: 0.9439 - val_loss: 0.7696 - val_acc: 0.8024\n",
      "Epoch 51/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.3664 - acc: 0.9531 - val_loss: 0.7888 - val_acc: 0.7881\n",
      "Epoch 52/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.3652 - acc: 0.9439 - val_loss: 0.8138 - val_acc: 0.7738\n",
      "Epoch 53/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.3400 - acc: 0.9556 - val_loss: 0.8510 - val_acc: 0.7512\n",
      "Epoch 54/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.3370 - acc: 0.9531 - val_loss: 0.8345 - val_acc: 0.7536\n",
      "Epoch 55/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.3035 - acc: 0.9617 - val_loss: 0.7763 - val_acc: 0.7810\n",
      "Epoch 56/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.3013 - acc: 0.9663 - val_loss: 0.7156 - val_acc: 0.8095\n",
      "Epoch 57/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.2867 - acc: 0.9704 - val_loss: 0.6715 - val_acc: 0.8286\n",
      "Epoch 58/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.2835 - acc: 0.9638 - val_loss: 0.6702 - val_acc: 0.8262\n",
      "Epoch 59/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.2828 - acc: 0.9689 - val_loss: 0.6699 - val_acc: 0.8262\n",
      "Epoch 60/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.2877 - acc: 0.9668 - val_loss: 0.6494 - val_acc: 0.8286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.2599 - acc: 0.9684 - val_loss: 0.6133 - val_acc: 0.8298\n",
      "Epoch 62/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.2513 - acc: 0.9719 - val_loss: 0.5844 - val_acc: 0.8405\n",
      "Epoch 63/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.2524 - acc: 0.9714 - val_loss: 0.5742 - val_acc: 0.8524\n",
      "Epoch 64/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.2415 - acc: 0.9735 - val_loss: 0.5925 - val_acc: 0.8464\n",
      "Epoch 65/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.2353 - acc: 0.9765 - val_loss: 0.6149 - val_acc: 0.8357\n",
      "Epoch 66/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.2230 - acc: 0.9801 - val_loss: 0.6308 - val_acc: 0.8310\n",
      "Epoch 67/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.2132 - acc: 0.9796 - val_loss: 0.6345 - val_acc: 0.8333\n",
      "Epoch 68/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.2054 - acc: 0.9832 - val_loss: 0.6397 - val_acc: 0.8369\n",
      "Epoch 69/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.2141 - acc: 0.9755 - val_loss: 0.6731 - val_acc: 0.8345\n",
      "Epoch 70/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.1943 - acc: 0.9862 - val_loss: 0.7360 - val_acc: 0.8143\n",
      "Epoch 71/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.2134 - acc: 0.9786 - val_loss: 0.7885 - val_acc: 0.8000\n",
      "Epoch 72/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.1892 - acc: 0.9801 - val_loss: 0.8222 - val_acc: 0.7893\n",
      "Epoch 73/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.1803 - acc: 0.9878 - val_loss: 0.8161 - val_acc: 0.7881\n",
      "Epoch 74/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.1729 - acc: 0.9862 - val_loss: 0.7765 - val_acc: 0.8107\n",
      "Epoch 75/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1762 - acc: 0.9801 - val_loss: 0.7375 - val_acc: 0.8179\n",
      "Epoch 76/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.1758 - acc: 0.9847 - val_loss: 0.6854 - val_acc: 0.8214\n",
      "Epoch 77/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.1689 - acc: 0.9862 - val_loss: 0.6497 - val_acc: 0.8274\n",
      "Epoch 78/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.1559 - acc: 0.9847 - val_loss: 0.6210 - val_acc: 0.8405\n",
      "Epoch 79/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1647 - acc: 0.9913 - val_loss: 0.5931 - val_acc: 0.8476\n",
      "Epoch 80/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.1470 - acc: 0.9893 - val_loss: 0.5818 - val_acc: 0.8488\n",
      "Epoch 81/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1405 - acc: 0.9883 - val_loss: 0.5939 - val_acc: 0.8452\n",
      "Epoch 82/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.1485 - acc: 0.9857 - val_loss: 0.6162 - val_acc: 0.8417\n",
      "Epoch 83/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.1516 - acc: 0.9852 - val_loss: 0.6582 - val_acc: 0.8321\n",
      "Epoch 84/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.1466 - acc: 0.9867 - val_loss: 0.7096 - val_acc: 0.8179\n",
      "Epoch 85/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.1319 - acc: 0.9908 - val_loss: 0.7224 - val_acc: 0.8202\n",
      "Epoch 86/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.1307 - acc: 0.9903 - val_loss: 0.6957 - val_acc: 0.8226\n",
      "Epoch 87/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.1281 - acc: 0.9893 - val_loss: 0.6649 - val_acc: 0.8369\n",
      "Epoch 88/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.1352 - acc: 0.9862 - val_loss: 0.6386 - val_acc: 0.8405\n",
      "Epoch 89/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1168 - acc: 0.9934 - val_loss: 0.6195 - val_acc: 0.8429\n",
      "Epoch 90/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.1254 - acc: 0.9893 - val_loss: 0.6126 - val_acc: 0.8393\n",
      "Epoch 91/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.1267 - acc: 0.9908 - val_loss: 0.6164 - val_acc: 0.8440\n",
      "Epoch 92/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.1180 - acc: 0.9908 - val_loss: 0.6232 - val_acc: 0.8429\n",
      "Epoch 93/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.1173 - acc: 0.9898 - val_loss: 0.6300 - val_acc: 0.8429\n",
      "Epoch 94/400\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.1091 - acc: 0.9908 - val_loss: 0.6290 - val_acc: 0.8393\n",
      "Epoch 95/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.1175 - acc: 0.9903 - val_loss: 0.6217 - val_acc: 0.8417\n",
      "Epoch 96/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.1052 - acc: 0.9939 - val_loss: 0.6032 - val_acc: 0.8464\n",
      "Epoch 97/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0955 - acc: 0.9959 - val_loss: 0.5838 - val_acc: 0.8583\n",
      "Epoch 98/400\n",
      "1960/1960 [==============================] - 0s 76us/step - loss: 0.0987 - acc: 0.9964 - val_loss: 0.5592 - val_acc: 0.8631\n",
      "Epoch 99/400\n",
      "1960/1960 [==============================] - 0s 73us/step - loss: 0.0940 - acc: 0.9913 - val_loss: 0.5652 - val_acc: 0.8643\n",
      "Epoch 100/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0963 - acc: 0.9949 - val_loss: 0.5870 - val_acc: 0.8619\n",
      "Epoch 101/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0924 - acc: 0.9939 - val_loss: 0.6012 - val_acc: 0.8548\n",
      "Epoch 102/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0915 - acc: 0.9934 - val_loss: 0.6188 - val_acc: 0.8524\n",
      "Epoch 103/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0915 - acc: 0.9954 - val_loss: 0.6165 - val_acc: 0.8405\n",
      "Epoch 104/400\n",
      "1960/1960 [==============================] - 0s 74us/step - loss: 0.0939 - acc: 0.9944 - val_loss: 0.6126 - val_acc: 0.8345\n",
      "Epoch 105/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0952 - acc: 0.9939 - val_loss: 0.6185 - val_acc: 0.8298\n",
      "Epoch 106/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0831 - acc: 0.9954 - val_loss: 0.6079 - val_acc: 0.8369\n",
      "Epoch 107/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0824 - acc: 0.9974 - val_loss: 0.5752 - val_acc: 0.8417\n",
      "Epoch 108/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0856 - acc: 0.9969 - val_loss: 0.5506 - val_acc: 0.8548\n",
      "Epoch 109/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0930 - acc: 0.9939 - val_loss: 0.5320 - val_acc: 0.8595\n",
      "Epoch 110/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0858 - acc: 0.9939 - val_loss: 0.5436 - val_acc: 0.8619\n",
      "Epoch 111/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0792 - acc: 0.9959 - val_loss: 0.5634 - val_acc: 0.8548\n",
      "Epoch 112/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0836 - acc: 0.9964 - val_loss: 0.6063 - val_acc: 0.8429\n",
      "Epoch 113/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0784 - acc: 0.9959 - val_loss: 0.6199 - val_acc: 0.8405\n",
      "Epoch 114/400\n",
      "1960/1960 [==============================] - 0s 73us/step - loss: 0.0776 - acc: 0.9964 - val_loss: 0.6021 - val_acc: 0.8452\n",
      "Epoch 115/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0755 - acc: 0.9974 - val_loss: 0.5756 - val_acc: 0.8571\n",
      "Epoch 116/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0757 - acc: 0.9974 - val_loss: 0.5624 - val_acc: 0.8560\n",
      "Epoch 117/400\n",
      "1960/1960 [==============================] - 0s 74us/step - loss: 0.0735 - acc: 0.9980 - val_loss: 0.5492 - val_acc: 0.8583\n",
      "Epoch 118/400\n",
      "1960/1960 [==============================] - 0s 75us/step - loss: 0.0766 - acc: 0.9974 - val_loss: 0.5771 - val_acc: 0.8548\n",
      "Epoch 119/400\n",
      "1960/1960 [==============================] - 0s 74us/step - loss: 0.0673 - acc: 0.9990 - val_loss: 0.6328 - val_acc: 0.8381\n",
      "Epoch 120/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0654 - acc: 0.9969 - val_loss: 0.6703 - val_acc: 0.8298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0690 - acc: 0.9964 - val_loss: 0.6605 - val_acc: 0.8310\n",
      "Epoch 122/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0652 - acc: 0.9954 - val_loss: 0.6393 - val_acc: 0.8405\n",
      "Epoch 123/400\n",
      "1960/1960 [==============================] - 0s 73us/step - loss: 0.0733 - acc: 0.9980 - val_loss: 0.5964 - val_acc: 0.8548\n",
      "Epoch 124/400\n",
      "1960/1960 [==============================] - 0s 74us/step - loss: 0.0697 - acc: 0.9959 - val_loss: 0.5726 - val_acc: 0.8548\n",
      "Epoch 125/400\n",
      "1960/1960 [==============================] - 0s 74us/step - loss: 0.0638 - acc: 0.9969 - val_loss: 0.5709 - val_acc: 0.8595\n",
      "Epoch 126/400\n",
      "1960/1960 [==============================] - 0s 75us/step - loss: 0.0617 - acc: 0.9969 - val_loss: 0.5776 - val_acc: 0.8560\n",
      "Epoch 127/400\n",
      "1960/1960 [==============================] - 0s 75us/step - loss: 0.0653 - acc: 0.9980 - val_loss: 0.5615 - val_acc: 0.8583\n",
      "Epoch 128/400\n",
      "1960/1960 [==============================] - 0s 74us/step - loss: 0.0656 - acc: 0.9974 - val_loss: 0.5468 - val_acc: 0.8667\n",
      "Epoch 129/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0584 - acc: 0.9995 - val_loss: 0.5273 - val_acc: 0.8702\n",
      "Epoch 130/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0592 - acc: 0.9964 - val_loss: 0.5149 - val_acc: 0.8774\n",
      "Epoch 131/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0580 - acc: 0.9985 - val_loss: 0.4940 - val_acc: 0.8762\n",
      "Epoch 132/400\n",
      "1960/1960 [==============================] - 0s 77us/step - loss: 0.0574 - acc: 0.9990 - val_loss: 0.4707 - val_acc: 0.8810\n",
      "Epoch 133/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0545 - acc: 0.9985 - val_loss: 0.4485 - val_acc: 0.8940\n",
      "Epoch 134/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0571 - acc: 0.9969 - val_loss: 0.4415 - val_acc: 0.8964\n",
      "Epoch 135/400\n",
      "1960/1960 [==============================] - 0s 73us/step - loss: 0.0551 - acc: 0.9980 - val_loss: 0.4441 - val_acc: 0.8988\n",
      "Epoch 136/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0599 - acc: 0.9964 - val_loss: 0.4484 - val_acc: 0.8988\n",
      "Epoch 137/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0567 - acc: 0.9974 - val_loss: 0.4459 - val_acc: 0.8964\n",
      "Epoch 138/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0543 - acc: 0.9980 - val_loss: 0.4528 - val_acc: 0.8917\n",
      "Epoch 139/400\n",
      "1960/1960 [==============================] - 0s 73us/step - loss: 0.0535 - acc: 0.9985 - val_loss: 0.4612 - val_acc: 0.8881\n",
      "Epoch 140/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0510 - acc: 0.9990 - val_loss: 0.4682 - val_acc: 0.8869\n",
      "Epoch 141/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0527 - acc: 0.9974 - val_loss: 0.4751 - val_acc: 0.8857\n",
      "Epoch 142/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0554 - acc: 0.9964 - val_loss: 0.4880 - val_acc: 0.8810\n",
      "Epoch 143/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0501 - acc: 0.9980 - val_loss: 0.5189 - val_acc: 0.8714\n",
      "Epoch 144/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0542 - acc: 0.9985 - val_loss: 0.5473 - val_acc: 0.8667\n",
      "Epoch 145/400\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0494 - acc: 0.9985 - val_loss: 0.5475 - val_acc: 0.8702\n",
      "Epoch 146/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0507 - acc: 0.9974 - val_loss: 0.5255 - val_acc: 0.8738\n",
      "Epoch 147/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0505 - acc: 0.9990 - val_loss: 0.5000 - val_acc: 0.8798\n",
      "Epoch 148/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0477 - acc: 0.9985 - val_loss: 0.4828 - val_acc: 0.8857\n",
      "Epoch 149/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0458 - acc: 0.9980 - val_loss: 0.4736 - val_acc: 0.8905\n",
      "Epoch 150/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0450 - acc: 0.9990 - val_loss: 0.4850 - val_acc: 0.8869\n",
      "Epoch 151/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0506 - acc: 0.9964 - val_loss: 0.5278 - val_acc: 0.8738\n",
      "Epoch 152/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0481 - acc: 0.9980 - val_loss: 0.5737 - val_acc: 0.8679\n",
      "Epoch 153/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0512 - acc: 0.9964 - val_loss: 0.5792 - val_acc: 0.8690\n",
      "Epoch 154/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0447 - acc: 0.9985 - val_loss: 0.5735 - val_acc: 0.8690\n",
      "Epoch 155/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0449 - acc: 0.9990 - val_loss: 0.5367 - val_acc: 0.8714\n",
      "Epoch 156/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0415 - acc: 0.9980 - val_loss: 0.4926 - val_acc: 0.8881\n",
      "Epoch 157/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0464 - acc: 0.9969 - val_loss: 0.4641 - val_acc: 0.8905\n",
      "Epoch 158/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0469 - acc: 0.9974 - val_loss: 0.4410 - val_acc: 0.8964\n",
      "Epoch 159/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0437 - acc: 0.9995 - val_loss: 0.4373 - val_acc: 0.9000\n",
      "Epoch 160/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0424 - acc: 0.9985 - val_loss: 0.4460 - val_acc: 0.9000\n",
      "Epoch 161/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0414 - acc: 0.9990 - val_loss: 0.4562 - val_acc: 0.8964\n",
      "Epoch 162/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0418 - acc: 0.9985 - val_loss: 0.4774 - val_acc: 0.8976\n",
      "Epoch 163/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0430 - acc: 0.9985 - val_loss: 0.5079 - val_acc: 0.8869\n",
      "Epoch 164/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0439 - acc: 0.9985 - val_loss: 0.5255 - val_acc: 0.8786\n",
      "Epoch 165/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0441 - acc: 0.9980 - val_loss: 0.5310 - val_acc: 0.8845\n",
      "Epoch 166/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0392 - acc: 0.9990 - val_loss: 0.5124 - val_acc: 0.8857\n",
      "Epoch 167/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0390 - acc: 0.9990 - val_loss: 0.4948 - val_acc: 0.8857\n",
      "Epoch 168/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0411 - acc: 0.9985 - val_loss: 0.4755 - val_acc: 0.8881\n",
      "Epoch 169/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0402 - acc: 0.9969 - val_loss: 0.4633 - val_acc: 0.8952\n",
      "Epoch 170/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0399 - acc: 0.9980 - val_loss: 0.4518 - val_acc: 0.8940\n",
      "Epoch 171/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0417 - acc: 0.9985 - val_loss: 0.4408 - val_acc: 0.8976\n",
      "Epoch 172/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0418 - acc: 0.9990 - val_loss: 0.4323 - val_acc: 0.9012\n",
      "Epoch 173/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0380 - acc: 0.9974 - val_loss: 0.4289 - val_acc: 0.9012\n",
      "Epoch 174/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0392 - acc: 0.9969 - val_loss: 0.4258 - val_acc: 0.9000\n",
      "Epoch 175/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0360 - acc: 1.0000 - val_loss: 0.4226 - val_acc: 0.9024\n",
      "Epoch 176/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0356 - acc: 0.9980 - val_loss: 0.4196 - val_acc: 0.9000\n",
      "Epoch 177/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0419 - acc: 0.9990 - val_loss: 0.4222 - val_acc: 0.9024\n",
      "Epoch 178/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0373 - acc: 0.9985 - val_loss: 0.4304 - val_acc: 0.8964\n",
      "Epoch 179/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0383 - acc: 0.9974 - val_loss: 0.4457 - val_acc: 0.8964\n",
      "Epoch 180/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0376 - acc: 0.9995 - val_loss: 0.4650 - val_acc: 0.8940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0355 - acc: 0.9995 - val_loss: 0.4801 - val_acc: 0.8917\n",
      "Epoch 182/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0407 - acc: 0.9985 - val_loss: 0.4813 - val_acc: 0.8929\n",
      "Epoch 183/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0356 - acc: 1.0000 - val_loss: 0.4775 - val_acc: 0.8929\n",
      "Epoch 184/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0327 - acc: 0.9985 - val_loss: 0.4651 - val_acc: 0.8976\n",
      "Epoch 185/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0334 - acc: 0.9995 - val_loss: 0.4600 - val_acc: 0.8988\n",
      "Epoch 186/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0343 - acc: 0.9995 - val_loss: 0.4547 - val_acc: 0.8976\n",
      "Epoch 187/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0382 - acc: 0.9980 - val_loss: 0.4552 - val_acc: 0.9000\n",
      "Epoch 188/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0354 - acc: 0.9985 - val_loss: 0.4510 - val_acc: 0.9036\n",
      "Epoch 189/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0361 - acc: 0.9985 - val_loss: 0.4479 - val_acc: 0.9024\n",
      "Epoch 190/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0331 - acc: 0.9995 - val_loss: 0.4488 - val_acc: 0.9012\n",
      "Epoch 191/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0348 - acc: 1.0000 - val_loss: 0.4510 - val_acc: 0.8976\n",
      "Epoch 192/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0360 - acc: 0.9974 - val_loss: 0.4540 - val_acc: 0.8952\n",
      "Epoch 193/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0319 - acc: 0.9980 - val_loss: 0.4538 - val_acc: 0.8964\n",
      "Epoch 194/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0380 - acc: 0.9985 - val_loss: 0.4525 - val_acc: 0.8976\n",
      "Epoch 195/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0368 - acc: 0.9990 - val_loss: 0.4520 - val_acc: 0.8976\n",
      "Epoch 196/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0347 - acc: 0.9990 - val_loss: 0.4518 - val_acc: 0.8988\n",
      "Epoch 197/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0372 - acc: 0.9990 - val_loss: 0.4518 - val_acc: 0.8988\n",
      "Epoch 198/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0317 - acc: 0.9995 - val_loss: 0.4517 - val_acc: 0.9000\n",
      "Epoch 199/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0319 - acc: 0.9995 - val_loss: 0.4520 - val_acc: 0.9024\n",
      "Epoch 200/400\n",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0355 - acc: 0.9985 - val_loss: 0.4516 - val_acc: 0.9036\n",
      "Epoch 201/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0328 - acc: 0.9980 - val_loss: 0.4500 - val_acc: 0.9036\n",
      "Epoch 202/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0323 - acc: 0.9974 - val_loss: 0.4503 - val_acc: 0.9048\n",
      "Epoch 203/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0336 - acc: 1.0000 - val_loss: 0.4515 - val_acc: 0.9036\n",
      "Epoch 204/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0333 - acc: 0.9985 - val_loss: 0.4511 - val_acc: 0.9036\n",
      "Epoch 205/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0346 - acc: 0.9980 - val_loss: 0.4497 - val_acc: 0.9036\n",
      "Epoch 206/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0320 - acc: 0.9995 - val_loss: 0.4480 - val_acc: 0.9048\n",
      "Epoch 207/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0348 - acc: 0.9990 - val_loss: 0.4465 - val_acc: 0.9048\n",
      "Epoch 208/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0340 - acc: 0.9985 - val_loss: 0.4453 - val_acc: 0.9048\n",
      "Epoch 209/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0330 - acc: 0.9995 - val_loss: 0.4437 - val_acc: 0.9048\n",
      "Epoch 210/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0343 - acc: 0.9990 - val_loss: 0.4419 - val_acc: 0.9048\n",
      "Epoch 211/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0321 - acc: 0.9985 - val_loss: 0.4402 - val_acc: 0.9060\n",
      "Epoch 212/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0319 - acc: 0.9995 - val_loss: 0.4384 - val_acc: 0.9060\n",
      "Epoch 213/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0354 - acc: 0.9990 - val_loss: 0.4377 - val_acc: 0.9071\n",
      "Epoch 214/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0287 - acc: 0.9995 - val_loss: 0.4370 - val_acc: 0.9071\n",
      "Epoch 215/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0337 - acc: 0.9990 - val_loss: 0.4366 - val_acc: 0.9083\n",
      "Epoch 216/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0340 - acc: 0.9990 - val_loss: 0.4364 - val_acc: 0.9083\n",
      "Epoch 217/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0347 - acc: 0.9985 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 218/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0296 - acc: 0.9995 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 219/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0335 - acc: 0.9985 - val_loss: 0.4357 - val_acc: 0.9071\n",
      "Epoch 220/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0309 - acc: 1.0000 - val_loss: 0.4353 - val_acc: 0.9071\n",
      "Epoch 221/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0331 - acc: 0.9995 - val_loss: 0.4351 - val_acc: 0.9071\n",
      "Epoch 222/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0344 - acc: 0.9995 - val_loss: 0.4351 - val_acc: 0.9060\n",
      "Epoch 223/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0300 - acc: 0.9995 - val_loss: 0.4352 - val_acc: 0.9060\n",
      "Epoch 224/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0319 - acc: 0.9990 - val_loss: 0.4350 - val_acc: 0.9060\n",
      "Epoch 225/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0300 - acc: 0.9995 - val_loss: 0.4350 - val_acc: 0.9060\n",
      "Epoch 226/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0347 - acc: 0.9985 - val_loss: 0.4349 - val_acc: 0.9060\n",
      "Epoch 227/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0347 - acc: 0.9995 - val_loss: 0.4348 - val_acc: 0.9060\n",
      "Epoch 228/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0325 - acc: 1.0000 - val_loss: 0.4346 - val_acc: 0.9060\n",
      "Epoch 229/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0343 - acc: 0.9980 - val_loss: 0.4345 - val_acc: 0.9060\n",
      "Epoch 230/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0305 - acc: 0.9985 - val_loss: 0.4345 - val_acc: 0.9060\n",
      "Epoch 231/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0315 - acc: 0.9985 - val_loss: 0.4345 - val_acc: 0.9060\n",
      "Epoch 232/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0352 - acc: 0.9990 - val_loss: 0.4345 - val_acc: 0.9048\n",
      "Epoch 233/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0314 - acc: 0.9995 - val_loss: 0.4344 - val_acc: 0.9048\n",
      "Epoch 234/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0324 - acc: 0.9995 - val_loss: 0.4344 - val_acc: 0.9048\n",
      "Epoch 235/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0310 - acc: 1.0000 - val_loss: 0.4343 - val_acc: 0.9048\n",
      "Epoch 236/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0346 - acc: 0.9980 - val_loss: 0.4343 - val_acc: 0.9048\n",
      "Epoch 237/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0321 - acc: 0.9995 - val_loss: 0.4343 - val_acc: 0.9048\n",
      "Epoch 238/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0302 - acc: 0.9995 - val_loss: 0.4342 - val_acc: 0.9048\n",
      "Epoch 239/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0338 - acc: 0.9985 - val_loss: 0.4342 - val_acc: 0.9048\n",
      "Epoch 240/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0315 - acc: 0.9995 - val_loss: 0.4342 - val_acc: 0.9048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0316 - acc: 1.0000 - val_loss: 0.4343 - val_acc: 0.9048\n",
      "Epoch 242/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0328 - acc: 0.9995 - val_loss: 0.4345 - val_acc: 0.9048\n",
      "Epoch 243/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0308 - acc: 0.9995 - val_loss: 0.4346 - val_acc: 0.9048\n",
      "Epoch 244/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0313 - acc: 0.9990 - val_loss: 0.4346 - val_acc: 0.9048\n",
      "Epoch 245/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0323 - acc: 0.9985 - val_loss: 0.4346 - val_acc: 0.9048\n",
      "Epoch 246/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0318 - acc: 0.9990 - val_loss: 0.4347 - val_acc: 0.9048\n",
      "Epoch 247/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0326 - acc: 0.9990 - val_loss: 0.4348 - val_acc: 0.9048\n",
      "Epoch 248/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0319 - acc: 0.9980 - val_loss: 0.4350 - val_acc: 0.9048\n",
      "Epoch 249/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0286 - acc: 1.0000 - val_loss: 0.4352 - val_acc: 0.9048\n",
      "Epoch 250/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0335 - acc: 0.9995 - val_loss: 0.4354 - val_acc: 0.9048\n",
      "Epoch 251/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0321 - acc: 0.9980 - val_loss: 0.4356 - val_acc: 0.9048\n",
      "Epoch 252/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0325 - acc: 1.0000 - val_loss: 0.4358 - val_acc: 0.9048\n",
      "Epoch 253/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0338 - acc: 0.9995 - val_loss: 0.4360 - val_acc: 0.9048\n",
      "Epoch 254/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0328 - acc: 1.0000 - val_loss: 0.4361 - val_acc: 0.9048\n",
      "Epoch 255/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0345 - acc: 0.9995 - val_loss: 0.4364 - val_acc: 0.9048\n",
      "Epoch 256/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0306 - acc: 0.9990 - val_loss: 0.4366 - val_acc: 0.9048\n",
      "Epoch 257/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0303 - acc: 0.9980 - val_loss: 0.4367 - val_acc: 0.9048\n",
      "Epoch 258/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0318 - acc: 0.9995 - val_loss: 0.4368 - val_acc: 0.9048\n",
      "Epoch 259/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0286 - acc: 0.9990 - val_loss: 0.4369 - val_acc: 0.9048\n",
      "Epoch 260/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0320 - acc: 0.9985 - val_loss: 0.4371 - val_acc: 0.9048\n",
      "Epoch 261/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0331 - acc: 0.9995 - val_loss: 0.4371 - val_acc: 0.9048\n",
      "Epoch 262/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0322 - acc: 0.9995 - val_loss: 0.4371 - val_acc: 0.9048\n",
      "Epoch 263/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0307 - acc: 1.0000 - val_loss: 0.4369 - val_acc: 0.9048\n",
      "Epoch 264/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0341 - acc: 0.9990 - val_loss: 0.4367 - val_acc: 0.9048\n",
      "Epoch 265/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0320 - acc: 0.9985 - val_loss: 0.4365 - val_acc: 0.9048\n",
      "Epoch 266/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0318 - acc: 0.9990 - val_loss: 0.4363 - val_acc: 0.9048\n",
      "Epoch 267/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0318 - acc: 0.9990 - val_loss: 0.4361 - val_acc: 0.9048\n",
      "Epoch 268/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0311 - acc: 0.9990 - val_loss: 0.4359 - val_acc: 0.9048\n",
      "Epoch 269/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0323 - acc: 0.9974 - val_loss: 0.4358 - val_acc: 0.9048\n",
      "Epoch 270/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0294 - acc: 1.0000 - val_loss: 0.4356 - val_acc: 0.9060\n",
      "Epoch 271/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0313 - acc: 0.9995 - val_loss: 0.4355 - val_acc: 0.9060\n",
      "Epoch 272/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0299 - acc: 1.0000 - val_loss: 0.4354 - val_acc: 0.9060\n",
      "Epoch 273/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0348 - acc: 0.9969 - val_loss: 0.4355 - val_acc: 0.9060\n",
      "Epoch 274/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0279 - acc: 0.9995 - val_loss: 0.4356 - val_acc: 0.9060\n",
      "Epoch 275/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0340 - acc: 0.9995 - val_loss: 0.4357 - val_acc: 0.9060\n",
      "Epoch 276/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0329 - acc: 0.9990 - val_loss: 0.4359 - val_acc: 0.9048\n",
      "Epoch 277/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0350 - acc: 0.9985 - val_loss: 0.4363 - val_acc: 0.9048\n",
      "Epoch 278/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0311 - acc: 0.9995 - val_loss: 0.4366 - val_acc: 0.9048\n",
      "Epoch 279/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0317 - acc: 0.9995 - val_loss: 0.4369 - val_acc: 0.9048\n",
      "Epoch 280/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0329 - acc: 1.0000 - val_loss: 0.4371 - val_acc: 0.9048\n",
      "Epoch 281/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0336 - acc: 0.9990 - val_loss: 0.4374 - val_acc: 0.9048\n",
      "Epoch 282/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0299 - acc: 0.9985 - val_loss: 0.4376 - val_acc: 0.9048\n",
      "Epoch 283/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0294 - acc: 0.9990 - val_loss: 0.4379 - val_acc: 0.9048\n",
      "Epoch 284/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0314 - acc: 0.9985 - val_loss: 0.4381 - val_acc: 0.9048\n",
      "Epoch 285/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0351 - acc: 0.9990 - val_loss: 0.4383 - val_acc: 0.9048\n",
      "Epoch 286/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0297 - acc: 0.9995 - val_loss: 0.4385 - val_acc: 0.9048\n",
      "Epoch 287/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0325 - acc: 0.9990 - val_loss: 0.4387 - val_acc: 0.9048\n",
      "Epoch 288/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0306 - acc: 0.9990 - val_loss: 0.4388 - val_acc: 0.9048\n",
      "Epoch 289/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0322 - acc: 0.9974 - val_loss: 0.4389 - val_acc: 0.9048\n",
      "Epoch 290/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0366 - acc: 0.9985 - val_loss: 0.4389 - val_acc: 0.9048\n",
      "Epoch 291/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0304 - acc: 0.9985 - val_loss: 0.4390 - val_acc: 0.9048\n",
      "Epoch 292/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0285 - acc: 0.9980 - val_loss: 0.4389 - val_acc: 0.9048\n",
      "Epoch 293/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0302 - acc: 0.9990 - val_loss: 0.4389 - val_acc: 0.9060\n",
      "Epoch 294/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0335 - acc: 0.9985 - val_loss: 0.4389 - val_acc: 0.9060\n",
      "Epoch 295/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0295 - acc: 0.9995 - val_loss: 0.4388 - val_acc: 0.9060\n",
      "Epoch 296/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0333 - acc: 0.9990 - val_loss: 0.4387 - val_acc: 0.9060\n",
      "Epoch 297/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0311 - acc: 0.9995 - val_loss: 0.4386 - val_acc: 0.9060\n",
      "Epoch 298/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0331 - acc: 1.0000 - val_loss: 0.4386 - val_acc: 0.9060\n",
      "Epoch 299/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0308 - acc: 1.0000 - val_loss: 0.4386 - val_acc: 0.9060\n",
      "Epoch 300/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0346 - acc: 0.9980 - val_loss: 0.4384 - val_acc: 0.9060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0333 - acc: 0.9995 - val_loss: 0.4381 - val_acc: 0.9060\n",
      "Epoch 302/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.4380 - val_acc: 0.9060\n",
      "Epoch 303/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0328 - acc: 0.9990 - val_loss: 0.4377 - val_acc: 0.9060\n",
      "Epoch 304/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0306 - acc: 1.0000 - val_loss: 0.4376 - val_acc: 0.9060\n",
      "Epoch 305/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0337 - acc: 0.9985 - val_loss: 0.4374 - val_acc: 0.9060\n",
      "Epoch 306/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0317 - acc: 0.9995 - val_loss: 0.4371 - val_acc: 0.9060\n",
      "Epoch 307/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0379 - acc: 0.9969 - val_loss: 0.4370 - val_acc: 0.9060\n",
      "Epoch 308/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0340 - acc: 0.9990 - val_loss: 0.4368 - val_acc: 0.9060\n",
      "Epoch 309/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0321 - acc: 0.9990 - val_loss: 0.4367 - val_acc: 0.9060\n",
      "Epoch 310/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.4365 - val_acc: 0.9060\n",
      "Epoch 311/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0333 - acc: 0.9990 - val_loss: 0.4364 - val_acc: 0.9060\n",
      "Epoch 312/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0342 - acc: 0.9974 - val_loss: 0.4363 - val_acc: 0.9060\n",
      "Epoch 313/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0288 - acc: 0.9990 - val_loss: 0.4362 - val_acc: 0.9071\n",
      "Epoch 314/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0284 - acc: 1.0000 - val_loss: 0.4361 - val_acc: 0.9071\n",
      "Epoch 315/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0333 - acc: 0.9985 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 316/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0327 - acc: 0.9990 - val_loss: 0.4359 - val_acc: 0.9071\n",
      "Epoch 317/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0300 - acc: 1.0000 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 318/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0307 - acc: 1.0000 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 319/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0356 - acc: 0.9969 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 320/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0311 - acc: 1.0000 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 321/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0317 - acc: 0.9995 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 322/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0276 - acc: 1.0000 - val_loss: 0.4358 - val_acc: 0.9071\n",
      "Epoch 323/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0296 - acc: 1.0000 - val_loss: 0.4357 - val_acc: 0.9071\n",
      "Epoch 324/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0301 - acc: 0.9995 - val_loss: 0.4356 - val_acc: 0.9071\n",
      "Epoch 325/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0324 - acc: 0.9990 - val_loss: 0.4358 - val_acc: 0.9071\n",
      "Epoch 326/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0300 - acc: 1.0000 - val_loss: 0.4359 - val_acc: 0.9071\n",
      "Epoch 327/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0310 - acc: 0.9995 - val_loss: 0.4360 - val_acc: 0.9060\n",
      "Epoch 328/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0288 - acc: 0.9995 - val_loss: 0.4360 - val_acc: 0.9060\n",
      "Epoch 329/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0341 - acc: 0.9985 - val_loss: 0.4362 - val_acc: 0.9060\n",
      "Epoch 330/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0325 - acc: 1.0000 - val_loss: 0.4360 - val_acc: 0.9060\n",
      "Epoch 331/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0312 - acc: 0.9990 - val_loss: 0.4360 - val_acc: 0.9060\n",
      "Epoch 332/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0354 - acc: 0.9969 - val_loss: 0.4360 - val_acc: 0.9060\n",
      "Epoch 333/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0348 - acc: 0.9985 - val_loss: 0.4359 - val_acc: 0.9060\n",
      "Epoch 334/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0312 - acc: 0.9995 - val_loss: 0.4359 - val_acc: 0.9060\n",
      "Epoch 335/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0338 - acc: 0.9990 - val_loss: 0.4359 - val_acc: 0.9071\n",
      "Epoch 336/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0348 - acc: 0.9995 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 337/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0335 - acc: 0.9995 - val_loss: 0.4360 - val_acc: 0.9071\n",
      "Epoch 338/400\n",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0343 - acc: 0.9980 - val_loss: 0.4360 - val_acc: 0.9083\n",
      "Epoch 339/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0310 - acc: 0.9980 - val_loss: 0.4361 - val_acc: 0.9083\n",
      "Epoch 340/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0297 - acc: 0.9980 - val_loss: 0.4363 - val_acc: 0.9095\n",
      "Epoch 341/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0306 - acc: 0.9995 - val_loss: 0.4363 - val_acc: 0.9095\n",
      "Epoch 342/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0290 - acc: 0.9995 - val_loss: 0.4363 - val_acc: 0.9095\n",
      "Epoch 343/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0312 - acc: 0.9995 - val_loss: 0.4364 - val_acc: 0.9095\n",
      "Epoch 344/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0289 - acc: 0.9995 - val_loss: 0.4364 - val_acc: 0.9095\n",
      "Epoch 345/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0339 - acc: 0.9985 - val_loss: 0.4365 - val_acc: 0.9095\n",
      "Epoch 346/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0295 - acc: 0.9995 - val_loss: 0.4367 - val_acc: 0.9095\n",
      "Epoch 347/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0301 - acc: 0.9980 - val_loss: 0.4367 - val_acc: 0.9107\n",
      "Epoch 348/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0306 - acc: 1.0000 - val_loss: 0.4368 - val_acc: 0.9107\n",
      "Epoch 349/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0322 - acc: 0.9990 - val_loss: 0.4370 - val_acc: 0.9107\n",
      "Epoch 350/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0307 - acc: 0.9995 - val_loss: 0.4371 - val_acc: 0.9107\n",
      "Epoch 351/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0306 - acc: 0.9990 - val_loss: 0.4372 - val_acc: 0.9107\n",
      "Epoch 352/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0336 - acc: 0.9985 - val_loss: 0.4371 - val_acc: 0.9107\n",
      "Epoch 353/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0306 - acc: 0.9990 - val_loss: 0.4371 - val_acc: 0.9107\n",
      "Epoch 354/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0296 - acc: 1.0000 - val_loss: 0.4371 - val_acc: 0.9107\n",
      "Epoch 355/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0348 - acc: 0.9985 - val_loss: 0.4372 - val_acc: 0.9107\n",
      "Epoch 356/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0266 - acc: 0.9985 - val_loss: 0.4373 - val_acc: 0.9107\n",
      "Epoch 357/400\n",
      "1960/1960 [==============================] - 0s 72us/step - loss: 0.0307 - acc: 1.0000 - val_loss: 0.4373 - val_acc: 0.9107\n",
      "Epoch 358/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0309 - acc: 0.9985 - val_loss: 0.4372 - val_acc: 0.9107\n",
      "Epoch 359/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0324 - acc: 0.9969 - val_loss: 0.4372 - val_acc: 0.9095\n",
      "Epoch 360/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0291 - acc: 0.9990 - val_loss: 0.4373 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0315 - acc: 0.9995 - val_loss: 0.4374 - val_acc: 0.9083\n",
      "Epoch 362/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0300 - acc: 0.9995 - val_loss: 0.4375 - val_acc: 0.9083\n",
      "Epoch 363/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0316 - acc: 0.9995 - val_loss: 0.4375 - val_acc: 0.9083\n",
      "Epoch 364/400\n",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0337 - acc: 0.9985 - val_loss: 0.4377 - val_acc: 0.9083\n",
      "Epoch 365/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0332 - acc: 0.9985 - val_loss: 0.4378 - val_acc: 0.9083\n",
      "Epoch 366/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0309 - acc: 1.0000 - val_loss: 0.4378 - val_acc: 0.9083\n",
      "Epoch 367/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0279 - acc: 1.0000 - val_loss: 0.4379 - val_acc: 0.9083\n",
      "Epoch 368/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0307 - acc: 0.9985 - val_loss: 0.4381 - val_acc: 0.9083\n",
      "Epoch 369/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0336 - acc: 0.9990 - val_loss: 0.4382 - val_acc: 0.9083\n",
      "Epoch 370/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0299 - acc: 0.9995 - val_loss: 0.4383 - val_acc: 0.9071\n",
      "Epoch 371/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0336 - acc: 0.9985 - val_loss: 0.4386 - val_acc: 0.9071\n",
      "Epoch 372/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0336 - acc: 0.9980 - val_loss: 0.4387 - val_acc: 0.9071\n",
      "Epoch 373/400\n",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0316 - acc: 0.9995 - val_loss: 0.4388 - val_acc: 0.9071\n",
      "Epoch 374/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0300 - acc: 1.0000 - val_loss: 0.4388 - val_acc: 0.9071\n",
      "Epoch 375/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0325 - acc: 0.9985 - val_loss: 0.4388 - val_acc: 0.9083\n",
      "Epoch 376/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0320 - acc: 1.0000 - val_loss: 0.4387 - val_acc: 0.9083\n",
      "Epoch 377/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0301 - acc: 0.9995 - val_loss: 0.4386 - val_acc: 0.9083\n",
      "Epoch 378/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0300 - acc: 0.9990 - val_loss: 0.4385 - val_acc: 0.9083\n",
      "Epoch 379/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0315 - acc: 0.9995 - val_loss: 0.4383 - val_acc: 0.9083\n",
      "Epoch 380/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0303 - acc: 1.0000 - val_loss: 0.4381 - val_acc: 0.9083\n",
      "Epoch 381/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0303 - acc: 0.9990 - val_loss: 0.4380 - val_acc: 0.9071\n",
      "Epoch 382/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0327 - acc: 0.9990 - val_loss: 0.4377 - val_acc: 0.9071\n",
      "Epoch 383/400\n",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0331 - acc: 0.9990 - val_loss: 0.4374 - val_acc: 0.9071\n",
      "Epoch 384/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0281 - acc: 1.0000 - val_loss: 0.4371 - val_acc: 0.9071\n",
      "Epoch 385/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0320 - acc: 0.9990 - val_loss: 0.4370 - val_acc: 0.9071\n",
      "Epoch 386/400\n",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0278 - acc: 1.0000 - val_loss: 0.4370 - val_acc: 0.9071\n",
      "Epoch 387/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0324 - acc: 0.9990 - val_loss: 0.4369 - val_acc: 0.9071\n",
      "Epoch 388/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0309 - acc: 0.9990 - val_loss: 0.4370 - val_acc: 0.9071\n",
      "Epoch 389/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0319 - acc: 0.9995 - val_loss: 0.4371 - val_acc: 0.9071\n",
      "Epoch 390/400\n",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0361 - acc: 0.9980 - val_loss: 0.4372 - val_acc: 0.9071\n",
      "Epoch 391/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0306 - acc: 0.9995 - val_loss: 0.4374 - val_acc: 0.9071\n",
      "Epoch 392/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0302 - acc: 0.9995 - val_loss: 0.4374 - val_acc: 0.9071\n",
      "Epoch 393/400\n",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0351 - acc: 0.9985 - val_loss: 0.4375 - val_acc: 0.9071\n",
      "Epoch 394/400\n",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0306 - acc: 0.9990 - val_loss: 0.4376 - val_acc: 0.9071\n",
      "Epoch 395/400\n",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0307 - acc: 0.9995 - val_loss: 0.4375 - val_acc: 0.9071\n",
      "Epoch 396/400\n",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0319 - acc: 0.9980 - val_loss: 0.4376 - val_acc: 0.9071\n",
      "Epoch 397/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0345 - acc: 0.9995 - val_loss: 0.4376 - val_acc: 0.9071\n",
      "Epoch 398/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0331 - acc: 0.9990 - val_loss: 0.4375 - val_acc: 0.9071\n",
      "Epoch 399/400\n",
      "1960/1960 [==============================] - 0s 71us/step - loss: 0.0288 - acc: 0.9990 - val_loss: 0.4374 - val_acc: 0.9071\n",
      "Epoch 400/400\n",
      "1960/1960 [==============================] - 0s 69us/step - loss: 0.0338 - acc: 0.9980 - val_loss: 0.4374 - val_acc: 0.9071\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "lr = 1e-3\n",
    "AR_single.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=2e-5)\n",
    "history=AR_single.fit([X_0,X_1],Y,\n",
    "        batch_size=len(Y),\n",
    "        epochs=400,\n",
    "        verbose=True,\n",
    "        shuffle=True,\n",
    "        callbacks=[lrScheduler],\n",
    "        validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AR_single.save_weights('weights/fine_heavy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlXXrWf6rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff300261a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_frame(p,C):\n",
    "    full_l = p.shape[0] # full length\n",
    "    if random.uniform(0,1)<0.5: # aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.9,1)*full_l)\n",
    "        s = random.randint(0, full_l-int(valid_l))\n",
    "        e = s+valid_l # sample end point\n",
    "        p = p[int(s):int(e),:,:]    \n",
    "    else: # without aligment sampling\n",
    "        valid_l = np.round(np.random.uniform(0.9,1)*full_l)\n",
    "        index = np.sort(np.random.choice(range(0,full_l),int(valid_l),replace=False))\n",
    "        p = p[index,:,:]\n",
    "    p = zoom(p,C.frame_l,C.joint_n,C.joint_d)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "AR_single.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23/1960 [00:00<00:08, 224.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/cv2/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n",
      "100%|██████████| 1960/1960 [00:05<00:00, 373.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "1960/1960 [==============================] - 4s 2ms/step - loss: 0.0544 - acc: 0.9934 - val_loss: 0.4370 - val_acc: 0.9060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 377.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0660 - acc: 0.9883 - val_loss: 0.4364 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 371.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0616 - acc: 0.9903 - val_loss: 0.4358 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 384.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 68us/step - loss: 0.0570 - acc: 0.9918 - val_loss: 0.4359 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 361.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0616 - acc: 0.9903 - val_loss: 0.4366 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 382.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0615 - acc: 0.9893 - val_loss: 0.4372 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 372.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0642 - acc: 0.9908 - val_loss: 0.4378 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 384.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0619 - acc: 0.9908 - val_loss: 0.4383 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 383.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0568 - acc: 0.9923 - val_loss: 0.4385 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 385.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0671 - acc: 0.9898 - val_loss: 0.4387 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 372.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0539 - acc: 0.9913 - val_loss: 0.4388 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/1960 [00:00<00:11, 167.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 367.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0539 - acc: 0.9939 - val_loss: 0.4393 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 371.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0534 - acc: 0.9939 - val_loss: 0.4399 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 382.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0607 - acc: 0.9893 - val_loss: 0.4402 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 376.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0669 - acc: 0.9893 - val_loss: 0.4409 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 372.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0641 - acc: 0.9918 - val_loss: 0.4411 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 383.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0649 - acc: 0.9918 - val_loss: 0.4410 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 372.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 58us/step - loss: 0.0525 - acc: 0.9939 - val_loss: 0.4414 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 365.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0689 - acc: 0.9913 - val_loss: 0.4419 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 382.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0587 - acc: 0.9918 - val_loss: 0.4428 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 383.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0679 - acc: 0.9893 - val_loss: 0.4436 - val_acc: 0.9071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/1960 [00:00<00:11, 166.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 372.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0514 - acc: 0.9954 - val_loss: 0.4444 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 384.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 58us/step - loss: 0.0573 - acc: 0.9929 - val_loss: 0.4448 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 385.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0653 - acc: 0.9903 - val_loss: 0.4448 - val_acc: 0.9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 382.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0515 - acc: 0.9939 - val_loss: 0.4446 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 384.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0526 - acc: 0.9944 - val_loss: 0.4446 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 371.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0533 - acc: 0.9913 - val_loss: 0.4449 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 385.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0467 - acc: 0.9964 - val_loss: 0.4451 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 371.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0680 - acc: 0.9918 - val_loss: 0.4452 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 383.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0572 - acc: 0.9888 - val_loss: 0.4451 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 372.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0511 - acc: 0.9954 - val_loss: 0.4448 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/1960 [00:00<00:11, 168.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 370.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0505 - acc: 0.9944 - val_loss: 0.4444 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 383.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0506 - acc: 0.9959 - val_loss: 0.4442 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 382.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0550 - acc: 0.9939 - val_loss: 0.4441 - val_acc: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 382.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0640 - acc: 0.9918 - val_loss: 0.4440 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 381.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0631 - acc: 0.9913 - val_loss: 0.4436 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 371.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0558 - acc: 0.9913 - val_loss: 0.4433 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 383.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0637 - acc: 0.9883 - val_loss: 0.4429 - val_acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 348.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 65us/step - loss: 0.0593 - acc: 0.9923 - val_loss: 0.4424 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 349.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 66us/step - loss: 0.0477 - acc: 0.9959 - val_loss: 0.4423 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 371.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 61us/step - loss: 0.0546 - acc: 0.9934 - val_loss: 0.4421 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/1960 [00:00<00:11, 172.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 361.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 70us/step - loss: 0.0503 - acc: 0.9949 - val_loss: 0.4423 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 361.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 62us/step - loss: 0.0561 - acc: 0.9918 - val_loss: 0.4423 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 383.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0560 - acc: 0.9929 - val_loss: 0.4423 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 335.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 63us/step - loss: 0.0463 - acc: 0.9964 - val_loss: 0.4422 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 385.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 60us/step - loss: 0.0505 - acc: 0.9949 - val_loss: 0.4420 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 370.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 67us/step - loss: 0.0577 - acc: 0.9929 - val_loss: 0.4415 - val_acc: 0.9131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 384.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 64us/step - loss: 0.0504 - acc: 0.9934 - val_loss: 0.4415 - val_acc: 0.9131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 383.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0536 - acc: 0.9949 - val_loss: 0.4417 - val_acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [00:05<00:00, 385.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/1\n",
      "\r",
      "1960/1960 [==============================] - 0s 59us/step - loss: 0.0538 - acc: 0.9944 - val_loss: 0.4416 - val_acc: 0.9107\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in tqdm(range(len(Train['pose']))): \n",
    "    #for i in range(len(Train['pose'])): \n",
    "    \n",
    "        label = np.zeros(C.clc_fine)\n",
    "        label[Train['fine_label'][i]-1] = 1 \n",
    "        \n",
    "        p = np.copy(Train['pose'][i]).reshape([-1,22,3])[:,C.joint_ind,:]\n",
    "        p = sampling_frame(p,C)\n",
    "        \n",
    "        #rotation\n",
    "        x_angle = np.random.uniform(-0.2,0.2)\n",
    "        y_angle = np.random.uniform(-0.2,0.2)\n",
    "        z_angle = np.random.uniform(-0.2,0.2)\n",
    "        R = euler2mat(x_angle, y_angle, z_angle, 'sxyz')\n",
    "        p = rotaion_one(p,R)\n",
    "        \n",
    "        p = normlize_range(p)\n",
    "        \n",
    "        p[:,:,0] = p[:,:,0]*random.uniform(0.9, 1.1)+p[:,:,0]*random.uniform(-0.1,0.1)\n",
    "        p[:,:,1] = p[:,:,1]*random.uniform(0.9, 1.1)+p[:,:,1]*random.uniform(-0.1,0.1)\n",
    "        p[:,:,2] = p[:,:,2]*random.uniform(0.9, 1.1)+p[:,:,2]*random.uniform(-0.1,0.1)\n",
    "        \n",
    "        \n",
    "        M = get_CG(p,C)\n",
    "        \n",
    "        X_0.append(M)\n",
    "        X_1.append(p)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    Y = np.stack(Y)\n",
    "   \n",
    "\n",
    "    AR_single.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )\n",
    "\n",
    "    if e%10==0:\n",
    "        AR_single.save_weights('weights/fine_heavy.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkUAAAJCCAYAAADOe7N5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlSgCAKjq/wL29XZ4dJDACwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ffee22e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred = AR_single.predict([X_test_0,X_test_1])\n",
    "cnf_matrix = confusion_matrix(np.argmax(Y_test,axis=1),np.argmax(Y_pred,axis=1))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cnf_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
